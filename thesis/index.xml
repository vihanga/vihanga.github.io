<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Reinforcement Learning and Virtual Human Animation on Vihanga Gamage&#39;s Corner of the World Wide Web</title>
    <link>https://vihanga.github.io/thesis/</link>
    <description>Recent content in Reinforcement Learning and Virtual Human Animation on Vihanga Gamage&#39;s Corner of the World Wide Web</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>&amp;copy; 2018</copyright>
    <lastBuildDate>Wed, 22 Jan 2025 00:00:00 +0000</lastBuildDate>
    
	<atom:link href="https://vihanga.github.io/thesis/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Chapter 1: Introduction</title>
      <link>https://vihanga.github.io/thesis/chapters/introduction/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://vihanga.github.io/thesis/chapters/introduction/</guid>
      <description>Introduction This thesis tackles a fundamental challenge in computer animation: how can we create virtual characters that move naturally and responsively without the prohibitive costs of motion capture?
The Problem Traditional character animation relies heavily on motion capture - an expensive, time-consuming process that requires recording human actors for every possible behavior variation. While this produces high-quality results, it&amp;rsquo;s simply not scalable for interactive applications where characters need to respond dynamically to user input or changing scenarios.</description>
    </item>
    
    <item>
      <title>Chapter 2: Virtual Character Animation and Perception</title>
      <link>https://vihanga.github.io/thesis/chapters/data-driven-animation/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://vihanga.github.io/thesis/chapters/data-driven-animation/</guid>
      <description>Virtual Character Animation and Perception This chapter establishes the foundation for understanding virtual character animation - both the technical challenges and the perceptual requirements that make animation &amp;ldquo;feel right&amp;rdquo; to human viewers.
The Animation Pipeline Creating believable virtual characters isn&amp;rsquo;t just about making them move - it&amp;rsquo;s a complex pipeline that starts with 3D modeling and ends with real-time rendering. I explore how animation data flows through this pipeline, from motion capture or procedural generation to the final pixels on screen.</description>
    </item>
    
    <item>
      <title>Chapter 3: Model-based Reinforcement Learning</title>
      <link>https://vihanga.github.io/thesis/chapters/model-based-rl/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://vihanga.github.io/thesis/chapters/model-based-rl/</guid>
      <description>Model-based Reinforcement Learning This chapter provides the theoretical foundation for understanding how model-based RL can revolutionize character animation. I explain why learning a model of the world enables more sample-efficient and generalizable agents compared to traditional approaches.
The Power of World Models Most RL systems are model-free - they learn through trial and error what actions lead to good outcomes. That works great if you have millions of attempts to practice.</description>
    </item>
    
    <item>
      <title>Chapter 4: Model-based Character Animation</title>
      <link>https://vihanga.github.io/thesis/chapters/model-based-animation/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://vihanga.github.io/thesis/chapters/model-based-animation/</guid>
      <description>Model-based Character Animation This chapter marks the beginning of my core contributions - demonstrating that model-based RL can work for character animation without physics simulation. It&amp;rsquo;s the proof-of-concept that sets the stage for everything that follows.
The Experiment That Started It All I started with a simple question: can an RL agent learn to animate a character using only a learned model of movement dynamics? No physics engine, no hand-crafted rewards - just data and learning.</description>
    </item>
    
    <item>
      <title>Chapter 5: RLAnimate - Data-driven RL for Character Animation</title>
      <link>https://vihanga.github.io/thesis/chapters/data-driven-rl/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://vihanga.github.io/thesis/chapters/data-driven-rl/</guid>
      <description>RLAnimate: Data-driven RL for Character Animation This is where everything comes together. Chapter 5 introduces RLAnimate - my core framework that revolutionizes how we think about RL for animation. Instead of rewards, we use motion capture data as objectives. It&amp;rsquo;s a simple idea with profound implications.
The Breakthrough: Objectives, Not Rewards Traditional RL for animation struggles with a fundamental problem: how do you write a reward function for &amp;ldquo;wave friendly&amp;rdquo; or &amp;ldquo;point naturally&amp;rdquo;?</description>
    </item>
    
    <item>
      <title>Chapter 6: Quaternions and Finger Animation</title>
      <link>https://vihanga.github.io/thesis/chapters/latent-dynamics/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://vihanga.github.io/thesis/chapters/latent-dynamics/</guid>
      <description>Quaternions and Finger Animation Chapter 6 tackles one of animation&amp;rsquo;s nastiest problems: fingers. With 30+ joints that move in complex, coordinated patterns, finger animation has long been the bane of procedural methods. My solution? Quaternions and a dedicated animation dynamics model.
Why Fingers Matter (And Why They&amp;rsquo;re Hard) Watch someone wave - their fingers aren&amp;rsquo;t static. They curl, extend, and move with subtle patterns that make the gesture feel alive.</description>
    </item>
    
    <item>
      <title>Chapter 7: Beat Gestures - The Ultimate Test</title>
      <link>https://vihanga.github.io/thesis/chapters/conversational-gestures/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://vihanga.github.io/thesis/chapters/conversational-gestures/</guid>
      <description>Beat Gestures: The Ultimate Test This chapter represents the culmination of everything - applying RLAnimate to conversational beat gestures. It&amp;rsquo;s the hardest test possible: can RL-generated animations be indistinguishable from real human gestures? The answer, validated through rigorous perceptual studies, is yes.
Why Beat Gestures Are Different Beat gestures - those rhythmic hand movements we make while speaking - are uniquely challenging: - They&amp;rsquo;re not goal-directed (unlike pointing or waving) - They must synchronize with speech prosody - They require subtle variety to feel natural - There&amp;rsquo;s no &amp;ldquo;correct&amp;rdquo; gesture - only human-like ones</description>
    </item>
    
    <item>
      <title>Chapter 8: Conclusions and Future Impact</title>
      <link>https://vihanga.github.io/thesis/chapters/conclusion/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://vihanga.github.io/thesis/chapters/conclusion/</guid>
      <description>Conclusions and Future Impact This thesis fundamentally changes how we think about character animation. By treating animation as a model-based RL problem with motion capture objectives, I&amp;rsquo;ve created a system that produces human-quality movement at real-time speeds. But this is just the beginning.
What We&amp;rsquo;ve Achieved RLAnimate represents several breakthrough achievements:
Human-quality animation from RL: For the first time, RL agents generate movement that humans can&amp;rsquo;t distinguish from motion capture.</description>
    </item>
    
    <item>
      <title>Chapter 4: Supplementary Material</title>
      <link>https://vihanga.github.io/thesis/supplementary/c4/</link>
      <pubDate>Wed, 22 Jan 2025 00:00:00 +0000</pubDate>
      
      <guid>https://vihanga.github.io/thesis/supplementary/c4/</guid>
      <description>Chapter 4: Model-Based Character Animation This page contains supplementary material referenced in Chapter 4 of the thesis, demonstrating the model-based reinforcement learning agents trained to portray gazing and pointing behaviors.
Video Demonstrations Model-Based Animation Agents (AMSTA 2021) The following video demonstrates the agents trained using learned dynamics models and online planning for target-driven behaviors:
  This video shows: - Gazing behavior with dynamic target tracking - Pointing behavior with both left and right arms - Combined gaze and point behaviors - Real-time responsiveness to changing targets - Comparison with inverse kinematics baseline</description>
    </item>
    
    <item>
      <title>Chapter 5 Supplementary Material - Part A: RLAnimate Output Sequences</title>
      <link>https://vihanga.github.io/thesis/supplementary/c5a/</link>
      <pubDate>Wed, 22 Jan 2025 00:00:00 +0000</pubDate>
      
      <guid>https://vihanga.github.io/thesis/supplementary/c5a/</guid>
      <description>Chapter 5 Part A: RLAnimate Output Sequences This page contains supplementary material for Chapter 5, demonstrating the RLAnimate A1 agents trained to portray waving and pointing behaviors.
Video Demonstrations RLAnimate Output Sequences and Comparison to Control Agents (ALA 2021) The following video shows RLAnimate agents performing waving and pointing behaviors, with comparisons to baseline control agents:
  This video demonstrates: - A1 agent performing waving behaviors with varying exaggeration levels - A1 agent performing pointing behaviors to different targets - Comparison with single dynamics baseline - Comparison with supervised learning baseline - Natural, human-like motion generation</description>
    </item>
    
    <item>
      <title>Chapter 5 Supplementary Material - Part B: Behavior Flexibility</title>
      <link>https://vihanga.github.io/thesis/supplementary/c5b/</link>
      <pubDate>Wed, 22 Jan 2025 00:00:00 +0000</pubDate>
      
      <guid>https://vihanga.github.io/thesis/supplementary/c5b/</guid>
      <description>Chapter 5 Part B: RLAnimate Behavior Portrayal Flexibility This page contains supplementary material for Chapter 5, demonstrating the flexibility and dynamic control capabilities of RLAnimate agents.
Video Demonstrations RLAnimate Behavior Portrayal Flexibility (ALA 2021) The following video demonstrates the dynamic flexibility of RLAnimate agents in portraying varied behaviors:
  This video shows: - Waving with continuously variable exaggeration levels - Pointing to dynamically changing targets - Smooth transitions between behaviors - Real-time responsiveness to parameter changes - Natural variation in movement execution</description>
    </item>
    
    <item>
      <title>Chapter 6 Supplementary Material: Quaternions and Finger Animation</title>
      <link>https://vihanga.github.io/thesis/supplementary/c6/</link>
      <pubDate>Wed, 22 Jan 2025 00:00:00 +0000</pubDate>
      
      <guid>https://vihanga.github.io/thesis/supplementary/c6/</guid>
      <description>Chapter 6: Latent Dynamic-augmented Animation Output This page contains supplementary material referenced in Chapter 6 of the thesis, demonstrating the A3 architecture with quaternion rotations and animation dynamics for finger animation.
A3 Architecture Results The A3 architecture introduced in Chapter 6 addresses the challenge of incorporating finger animation through:
 Quaternion Representations: Eliminating gimbal lock and improving computational efficiency Animation Dynamics Model: Dedicated latent dynamics for animation generation Hierarchical Learning: Better capture of joint dependencies  Performance Improvements  A2 + Quaternion baseline: 81.</description>
    </item>
    
    <item>
      <title>Chapter 7 Supplementary Material: Beat Gestures</title>
      <link>https://vihanga.github.io/thesis/supplementary/c7/</link>
      <pubDate>Wed, 22 Jan 2025 00:00:00 +0000</pubDate>
      
      <guid>https://vihanga.github.io/thesis/supplementary/c7/</guid>
      <description>Chapter 7: Portraying Conversational Gestures via Realism Regularisation This page contains supplementary material referenced in Chapter 7, including the video stimuli used in the perceptual evaluation study comparing RLAnimate A4, Gesticulator, and motion capture.
Perceptual Study Video Stimuli The following videos were used in the perceptual evaluation with 28 participants. Each stimulus set was rendered with three methods: RLAnimate A4, Gesticulator, and Motion Capture (ground truth).
Stimulus Set A  RLAnimate A4    Gesticulator    Motion Capture    Stimulus Set B  RLAnimate A4    Gesticulator    Motion Capture    Stimulus Set C  RLAnimate A4    Gesticulator    Motion Capture    Stimulus Set D  RLAnimate A4    Gesticulator    Motion Capture    Stimulus Set E  RLAnimate A4    Gesticulator    Motion Capture    Stimulus Set F  RLAnimate A4    Gesticulator    Motion Capture    Stimulus Set G  RLAnimate A4    Gesticulator    Motion Capture    Stimulus Set H  RLAnimate A4    Gesticulator    Motion Capture    Perceptual Study Results Statistical Results RLAnimate A4 vs Gesticulator (one-sided t-tests, α = 0.</description>
    </item>
    
    <item>
      <title>MIG&#39;18 Supplementary Material</title>
      <link>https://vihanga.github.io/thesis/publications/MIG18/</link>
      <pubDate>Wed, 22 Jan 2025 00:00:00 +0000</pubDate>
      
      <guid>https://vihanga.github.io/thesis/publications/MIG18/</guid>
      <description>Examining the effects of a virtual character on learning and engagement in serious games Authors: Vihanga Gamage, Cathy Ennis
Conference: MIG &amp;lsquo;18: Motion, Interaction and Games, November 8–10, 2018, Limassol, Cyprus
DOI: 10.1145&amp;frasl;3274247.3274499
Abstract Virtual characters have been employed for many purposes including interacting with players of serious games, with a purpose to increase engagement. These characters are often embodied conversational agents playing diverse roles, such as demonstrators, guides, teachers or interviewers.</description>
    </item>
    
  </channel>
</rss>