<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Model-based Reinforcement Learning for Animating Virtual Characters on Vihanga Gamage&#39;s Corner of the World Wide Web</title>
    <link>https://vihanga.github.io/thesis/</link>
    <description>Recent content in Model-based Reinforcement Learning for Animating Virtual Characters on Vihanga Gamage&#39;s Corner of the World Wide Web</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>&amp;copy; 2018</copyright>
    <lastBuildDate>Wed, 22 Jan 2025 00:00:00 +0000</lastBuildDate>
    
	<atom:link href="https://vihanga.github.io/thesis/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Chapter 1: Introduction</title>
      <link>https://vihanga.github.io/thesis/chapters/introduction/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://vihanga.github.io/thesis/chapters/introduction/</guid>
      <description>Introduction This thesis tackles a fundamental challenge in computer animation: how can we create virtual characters that move naturally and responsively without the prohibitive costs of motion capture?
The Problem Traditional character animation relies heavily on motion capture - an expensive, time-consuming process that requires recording human actors for every possible behavior variation. While this produces high-quality results, it&amp;rsquo;s simply not scalable for interactive applications where characters need to respond dynamically to user input or changing scenarios.</description>
    </item>
    
    <item>
      <title>Chapter 2: Virtual Character Animation and Perception</title>
      <link>https://vihanga.github.io/thesis/chapters/data-driven-animation/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://vihanga.github.io/thesis/chapters/data-driven-animation/</guid>
      <description>Virtual Character Animation and Perception This chapter establishes the foundation for understanding virtual character animation - both the technical challenges and the perceptual requirements that make animation &amp;ldquo;feel right&amp;rdquo; to human viewers.
The Animation Pipeline Creating believable virtual characters isn&amp;rsquo;t just about making them move - it&amp;rsquo;s a complex pipeline that starts with 3D modeling and ends with real-time rendering. I explore how animation data flows through this pipeline, from motion capture or procedural generation to the final pixels on screen.</description>
    </item>
    
    <item>
      <title>Chapter 3: Model-based Reinforcement Learning</title>
      <link>https://vihanga.github.io/thesis/chapters/model-based-rl/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://vihanga.github.io/thesis/chapters/model-based-rl/</guid>
      <description>Model-based Reinforcement Learning This chapter provides the theoretical foundation for understanding how model-based RL can revolutionize character animation. I explain why learning a model of the world enables more sample-efficient and generalizable agents compared to traditional approaches.
The Power of World Models Most RL systems are model-free - they learn through trial and error what actions lead to good outcomes. That works great if you have millions of attempts to practice.</description>
    </item>
    
    <item>
      <title>Chapter 4: Model-based Character Animation</title>
      <link>https://vihanga.github.io/thesis/chapters/model-based-animation/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://vihanga.github.io/thesis/chapters/model-based-animation/</guid>
      <description>Model-based Character Animation This chapter presents exploratory work establishing the feasibility of applying reinforcement learning to dynamic character animation tasks without reliance on physics simulation. The work addresses a fundamental research question: can reinforcement learning effectively control character animation for portraying social behaviours?
Motivation and Objectives Previous work in RL-based animation has relied heavily on physics signals for training, rendering such approaches unsuitable for social behaviours where physics feedback is absent or irrelevant.</description>
    </item>
    
    <item>
      <title>Chapter 5: RLAnimate - Data-driven RL for Character Animation</title>
      <link>https://vihanga.github.io/thesis/chapters/data-driven-rl/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://vihanga.github.io/thesis/chapters/data-driven-rl/</guid>
      <description>RLAnimate: Data-driven Reinforcement Learning for Character Animation This chapter introduces RLAnimate, a novel framework that fundamentally reconceptualises reinforcement learning for character animation. Rather than relying on hand-crafted reward functions, RLAnimate utilises motion capture data as learning objectives, addressing the challenge of articulating human-like movement mathematically.
Theoretical Foundation The central insight of RLAnimate lies in reformulating the animation problem: instead of defining rewards for desired outcomes, the framework learns to match the dynamics of human movement.</description>
    </item>
    
    <item>
      <title>Chapter 6: Quaternions and Finger Animation</title>
      <link>https://vihanga.github.io/thesis/chapters/latent-dynamics/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://vihanga.github.io/thesis/chapters/latent-dynamics/</guid>
      <description>Quaternions and Finger Animation This chapter addresses the challenges of extending RLAnimate to accommodate fine-grained articulations, specifically finger animation. The inclusion of an additional thirty joints for finger control revealed limitations in the original architecture, necessitating fundamental modifications to both the representation and dynamics modelling approaches.
Technical Challenges in Finger Animation The incorporation of finger movements presents unique challenges for animation synthesis:
 Increased dimensionality: Addition of 30+ joints substantially expands the state and action spaces Hierarchical dependencies: Finger movements operate within the broader kinematic structure of arm and torso motion Subtle articulations: Small inaccuracies in finger positions produce visually apparent artefacts Representational singularities: Euler angle representations suffer from gimbal lock, particularly problematic for the complex rotations required in hand animation  These challenges necessitated architectural innovations to maintain animation quality while managing the increased complexity.</description>
    </item>
    
    <item>
      <title>Chapter 7: Beat Gestures - The Ultimate Test</title>
      <link>https://vihanga.github.io/thesis/chapters/conversational-gestures/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://vihanga.github.io/thesis/chapters/conversational-gestures/</guid>
      <description>Conversational Beat Gesture Generation This chapter presents the application of RLAnimate to conversational beat gesture generation, representing the most challenging test of the framework&amp;rsquo;s capabilities. Beat gestures - rhythmic hand movements that accompany speech - lack goal-directed objectives and require precise synchronisation with speech prosody, making them unsuitable for physics-based or reward-engineered approaches.
The Challenge of Beat Gesture Synthesis Beat gestures present unique challenges that distinguish them from previously addressed behaviours:</description>
    </item>
    
    <item>
      <title>Chapter 8: Conclusions and Future Impact</title>
      <link>https://vihanga.github.io/thesis/chapters/conclusion/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://vihanga.github.io/thesis/chapters/conclusion/</guid>
      <description>Conclusions and Future Impact This thesis fundamentally changes how we think about character animation. By treating animation as a model-based RL problem with motion capture objectives, I&amp;rsquo;ve created a system that produces human-quality movement at real-time speeds. But this is just the beginning.
What We&amp;rsquo;ve Achieved RLAnimate represents several breakthrough achievements:
Human-quality animation from RL: For the first time, RL agents generate movement that humans can&amp;rsquo;t distinguish from motion capture.</description>
    </item>
    
    <item>
      <title>Chapter 4: Supplementary Material</title>
      <link>https://vihanga.github.io/thesis/supplementary/c4/</link>
      <pubDate>Wed, 22 Jan 2025 00:00:00 +0000</pubDate>
      
      <guid>https://vihanga.github.io/thesis/supplementary/c4/</guid>
      <description>Chapter 4: Model-Based Character Animation This page contains supplementary material referenced in Chapter 4 of the thesis, demonstrating the model-based reinforcement learning agents trained to portray gazing and pointing behaviors.
Video Demonstrations Model-Based Animation Agents (AMSTA 2021) The following video demonstrates the agents trained using learned dynamics models and online planning for target-driven behaviors:
  This video shows: - Gazing behavior with dynamic target tracking - Pointing behavior with both left and right arms - Combined gaze and point behaviors - Real-time responsiveness to changing targets - Comparison with inverse kinematics baseline</description>
    </item>
    
    <item>
      <title>Chapter 5 Supplementary Material - Part A: RLAnimate Output Sequences</title>
      <link>https://vihanga.github.io/thesis/supplementary/c5a/</link>
      <pubDate>Wed, 22 Jan 2025 00:00:00 +0000</pubDate>
      
      <guid>https://vihanga.github.io/thesis/supplementary/c5a/</guid>
      <description>Chapter 5 Part A: RLAnimate Output Sequences This page contains supplementary material for Chapter 5, demonstrating the RLAnimate A1 agents trained to portray waving and pointing behaviors.
Video Demonstrations RLAnimate Output Sequences and Comparison to Control Agents (ALA 2021) The following video shows RLAnimate agents performing waving and pointing behaviors, with comparisons to baseline control agents:
  This video demonstrates: - A1 agent performing waving behaviors with varying exaggeration levels - A1 agent performing pointing behaviors to different targets - Comparison with single dynamics baseline - Comparison with supervised learning baseline - Natural, human-like motion generation</description>
    </item>
    
    <item>
      <title>Chapter 5 Supplementary Material - Part B: Behavior Flexibility</title>
      <link>https://vihanga.github.io/thesis/supplementary/c5b/</link>
      <pubDate>Wed, 22 Jan 2025 00:00:00 +0000</pubDate>
      
      <guid>https://vihanga.github.io/thesis/supplementary/c5b/</guid>
      <description>Chapter 5 Part B: RLAnimate Behavior Portrayal Flexibility This page contains supplementary material for Chapter 5, demonstrating the flexibility and dynamic control capabilities of RLAnimate agents.
Video Demonstrations RLAnimate Behavior Portrayal Flexibility (ALA 2021) The following video demonstrates the dynamic flexibility of RLAnimate agents in portraying varied behaviors:
  This video shows: - Waving with continuously variable exaggeration levels - Pointing to dynamically changing targets - Smooth transitions between behaviors - Real-time responsiveness to parameter changes - Natural variation in movement execution</description>
    </item>
    
    <item>
      <title>Chapter 6 Supplementary Material: Quaternions and Finger Animation</title>
      <link>https://vihanga.github.io/thesis/supplementary/c6/</link>
      <pubDate>Wed, 22 Jan 2025 00:00:00 +0000</pubDate>
      
      <guid>https://vihanga.github.io/thesis/supplementary/c6/</guid>
      <description>Chapter 6: Latent Dynamic-augmented Animation Output This page contains supplementary material referenced in Chapter 6 of the thesis, demonstrating the A3 architecture with quaternion rotations and animation dynamics for finger animation.
A3 Architecture Results The A3 architecture introduced in Chapter 6 addresses the challenge of incorporating finger animation through:
 Quaternion Representations: Eliminating gimbal lock and improving computational efficiency Animation Dynamics Model: Dedicated latent dynamics for animation generation Hierarchical Learning: Better capture of joint dependencies  Performance Improvements  A2 + Quaternion baseline: 81.</description>
    </item>
    
    <item>
      <title>Chapter 7 Supplementary Material: Beat Gestures</title>
      <link>https://vihanga.github.io/thesis/supplementary/c7/</link>
      <pubDate>Wed, 22 Jan 2025 00:00:00 +0000</pubDate>
      
      <guid>https://vihanga.github.io/thesis/supplementary/c7/</guid>
      <description>Chapter 7: Portraying Conversational Gestures via Realism Regularisation This page contains supplementary material referenced in Chapter 7, including the video stimuli used in the perceptual evaluation study comparing RLAnimate A4, Gesticulator, and motion capture.
Perceptual Study Video Stimuli The following videos were used in the perceptual evaluation with 28 participants. Each stimulus set was rendered with three methods: RLAnimate A4, Gesticulator, and Motion Capture (ground truth).
 .video-set { margin-bottom: 50px; background: #f5f5f5; padding: 20px; border-radius: 8px; } .</description>
    </item>
    
    <item>
      <title>MIG&#39;18 Paper: Virtual Characters in Serious Games</title>
      <link>https://vihanga.github.io/thesis/publications/MIG18/</link>
      <pubDate>Wed, 22 Jan 2025 00:00:00 +0000</pubDate>
      
      <guid>https://vihanga.github.io/thesis/publications/MIG18/</guid>
      <description>Examining the effects of a virtual character on learning and engagement in serious games Authors: Vihanga Gamage, Cathy Ennis
Conference: MIG &amp;lsquo;18: Motion, Interaction and Games, November 8–10, 2018, Limassol, Cyprus
DOI: 10.1145&amp;frasl;3274247.3274499
Abstract Virtual characters have been employed for many purposes including interacting with players of serious games, with a purpose to increase engagement. These characters are often embodied conversational agents playing diverse roles, such as demonstrators, guides, teachers or interviewers.</description>
    </item>
    
  </channel>
</rss>