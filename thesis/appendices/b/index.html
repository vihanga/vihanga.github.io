<!DOCTYPE html>
<html lang="en-us">
<head>

  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="generator" content="Source Themes Academic 3.1.1">
  <meta name="generator" content="Hugo 0.55.4" />
  <meta name="author" content="Vihanga Gamage">

  
  
  
  
    
  
  <meta name="description" content="Appendix B: Exploring Model-Free RL This appendix provides detailed information about model-free reinforcement learning approaches explored in this thesis.
B.1 Theoretical Foundations B.1.1 Model-Free vs Model-Based RL  Key distinctions and trade-offs Computational complexity analysis Sample efficiency considerations Generalization capabilities  B.1.2 Core Algorithms Value-Based Methods  Q-Learning fundamentals Deep Q-Networks (DQN) Double DQN and variants Prioritized experience replay  Policy Gradient Methods  REINFORCE algorithm Actor-Critic methods Trust Region Policy Optimization (TRPO) Proximal Policy Optimization (PPO)  B.">

  
  <link rel="alternate" hreflang="en-us" href="https://vihanga.github.io/thesis/appendices/b/">

  


  

  

  

  
  
  
  <meta name="theme-color" content="#3f51b5">
  

  
  
  
  
    
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/4.1.3/css/bootstrap.min.css" integrity="sha256-eSi1q2PG6J7g7ib17yAaWMcrr5GrtohYChqibrV7PBE=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/academicons/1.8.6/css/academicons.min.css" integrity="sha256-uFVgMKfistnJAfoCUQigIl+JfUaP47GrRKjf6CTPVmw=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.4.1/css/all.css" integrity="sha384-5sAR7xN1Nv6T6+dT2mhtzEpVJvfS3NScPQTrOxhwjIuvcA67KV2R5Jz6kr4abQsz" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.2.5/jquery.fancybox.min.css" integrity="sha256-ygkqlh3CYSUri3LhQxzdcm0n1EQvH2Y+U5S2idbLtxs=" crossorigin="anonymous">

    
    
    
      
    
    
      
      
        
          <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/github.min.css" crossorigin="anonymous">
        
      
    

    

    

  

  
  
  <link rel="stylesheet" href="//fonts.googleapis.com/css?family=Lato:400,700|Merriweather|Roboto+Mono">
  

  <link rel="stylesheet" href="/styles.css">
  

  
  
  

  
  <link rel="alternate" href="https://vihanga.github.io/index.xml" type="application/rss+xml" title="Vihanga Gamage&#39;s Corner of the World Wide Web">
  <link rel="feed" href="https://vihanga.github.io/index.xml" type="application/rss+xml" title="Vihanga Gamage&#39;s Corner of the World Wide Web">
  

  <link rel="manifest" href="/site.webmanifest">
  <link rel="icon" type="image/png" href="/img/icon.png">
  <link rel="apple-touch-icon" type="image/png" href="/img/icon-192.png">

  <link rel="canonical" href="https://vihanga.github.io/thesis/appendices/b/">

  <meta property="twitter:card" content="summary_large_image">
  
  <meta property="twitter:site" content="@VihGamage">
  <meta property="twitter:creator" content="@VihGamage">
  
  <meta property="og:site_name" content="Vihanga Gamage&#39;s Corner of the World Wide Web">
  <meta property="og:url" content="https://vihanga.github.io/thesis/appendices/b/">
  <meta property="og:title" content="Appendix B: Exploring Model-Free RL | Vihanga Gamage&#39;s Corner of the World Wide Web">
  <meta property="og:description" content="Appendix B: Exploring Model-Free RL This appendix provides detailed information about model-free reinforcement learning approaches explored in this thesis.
B.1 Theoretical Foundations B.1.1 Model-Free vs Model-Based RL  Key distinctions and trade-offs Computational complexity analysis Sample efficiency considerations Generalization capabilities  B.1.2 Core Algorithms Value-Based Methods  Q-Learning fundamentals Deep Q-Networks (DQN) Double DQN and variants Prioritized experience replay  Policy Gradient Methods  REINFORCE algorithm Actor-Critic methods Trust Region Policy Optimization (TRPO) Proximal Policy Optimization (PPO)  B.">
  
  
    
  <meta property="og:image" content="https://vihanga.github.io/img/icon-192.png">
  <meta property="og:locale" content="en-us">
  
  <meta property="article:published_time" content="2025-01-22T00:00:00&#43;00:00">
  
  <meta property="article:modified_time" content="2025-01-22T00:00:00&#43;00:00">
  

  

  

  <title>Appendix B: Exploring Model-Free RL | Vihanga Gamage&#39;s Corner of the World Wide Web</title>

</head>
<body id="top" data-spy="scroll" data-target="#TableOfContents" data-offset="71" >

<nav class="navbar navbar-light fixed-top navbar-expand-lg py-0" id="navbar-main">
  <div class="container">

    
      <a class="navbar-brand" href="/">Vihanga Gamage&#39;s Corner of the World Wide Web</a>
      
      <button type="button" class="navbar-toggler" data-toggle="collapse"
              data-target="#navbar" aria-controls="navbar" aria-expanded="false" aria-label="Toggle navigation">
        <span><i class="fas fa-bars"></i></span>
      </button>
      

    
    <div class="collapse navbar-collapse" id="navbar">

      
      
      <ul class="navbar-nav ml-auto">
        

        

        
        
        
          
        

        <li class="nav-item">
          <a class="nav-link" href="/#about">
            
            <span>Home</span>
            
          </a>
        </li>

        
        

        

        
        
        
          
        

        <li class="nav-item">
          <a class="nav-link" href="/#publications">
            
            <span>Publications</span>
            
          </a>
        </li>

        
        

        

        
        
        
          
        

        <li class="nav-item">
          <a class="nav-link" href="/#research">
            
            <span>Research</span>
            
          </a>
        </li>

        
        

        

        
        
        
          
        

        <li class="nav-item">
          <a class="nav-link" href="/#blog">
            
            <span>Blog</span>
            
          </a>
        </li>

        
        

        

        
        
        
          
        

        <li class="nav-item">
          <a class="nav-link" href="/#chess">
            
            <span>Chess</span>
            
          </a>
        </li>

        
        

        

        
        
        
          
        

        <li class="nav-item">
          <a class="nav-link" href="/#contact">
            
            <span>Contact/Calendar</span>
            
          </a>
        </li>

        
        

        

        
        
        
          
        

        <li class="nav-item">
          <a class="nav-link" href="/thesis/">
            
            <span>Thesis</span>
            
          </a>
        </li>

        
        

      

        

        
      </ul>

    </div>
  </div>
</nav>


<link rel="stylesheet" href="/css/thesis-book.css">

<div class="thesis-container">
  
  <aside class="thesis-sidebar">
    <div class="thesis-sidebar-content">
      <h3 class="thesis-sidebar-title">Navigation</h3>
      
      <ul class="thesis-menu">
        <li><a href="/thesis/">Home</a></li>
        
        <li class="thesis-menu-section">
          <span class="thesis-menu-section-title">Chapters</span>
          <ul>
            <li><a href="/thesis/chapters/introduction/">1. Introduction</a></li>
            <li><a href="/thesis/chapters/data-driven-animation/">2. Data-driven Animation</a></li>
            <li><a href="/thesis/chapters/model-based-rl/">3. Model-based RL</a></li>
            <li><a href="/thesis/chapters/model-based-animation/">4. Model-Based Animation</a></li>
            <li><a href="/thesis/chapters/data-driven-rl/">5. Data-driven RL</a></li>
            <li><a href="/thesis/chapters/latent-dynamics/">6. Latent Dynamics</a></li>
            <li><a href="/thesis/chapters/conversational-gestures/">7. Conversational Gestures</a></li>
            <li><a href="/thesis/chapters/conclusion/">8. Conclusions</a></li>
          </ul>
        </li>
        
        <li class="thesis-menu-section">
          <span class="thesis-menu-section-title">Supplementary</span>
          <ul>
            <li><a href="/thesis/supplementary/c4/">Chapter 4</a></li>
            <li><a href="/thesis/supplementary/c5a/">Chapter 5 - Part A</a></li>
            <li><a href="/thesis/supplementary/c5b/">Chapter 5 - Part B</a></li>
            <li><a href="/thesis/supplementary/c6/">Chapter 6</a></li>
            <li><a href="/thesis/supplementary/c7/">Chapter 7</a></li>
          </ul>
        </li>
        
        <li class="thesis-menu-section">
          <span class="thesis-menu-section-title">Appendices</span>
          <ul>
            <li><a href="/thesis/appendices/a/">A: Serious Games</a></li>
            <li><a href="/thesis/appendices/b/">B: Model-Free RL</a></li>
            <li><a href="/thesis/appendices/c/">C: Supplementary Index</a></li>
            <li><a href="/thesis/appendices/d/">D: Perceptual Evaluation</a></li>
            <li><a href="/thesis/appendices/e/">E: Model-free Results</a></li>
            <li><a href="/thesis/appendices/f/">F: Star Jump Calculation</a></li>
            <li><a href="/thesis/appendices/g/">G: RLAnimate Directory</a></li>
          </ul>
        </li>
      </ul>
    </div>
  </aside>

  
  <main class="thesis-main">
    <article class="thesis-content">
      <h1>Appendix B: Exploring Model-Free RL</h1>
      
      
      

<h1 id="appendix-b-exploring-model-free-rl">Appendix B: Exploring Model-Free RL</h1>

<p>This appendix provides detailed information about model-free reinforcement learning approaches explored in this thesis.</p>

<h2 id="b-1-theoretical-foundations">B.1 Theoretical Foundations</h2>

<h3 id="b-1-1-model-free-vs-model-based-rl">B.1.1 Model-Free vs Model-Based RL</h3>

<ul>
<li>Key distinctions and trade-offs</li>
<li>Computational complexity analysis</li>
<li>Sample efficiency considerations</li>
<li>Generalization capabilities</li>
</ul>

<h3 id="b-1-2-core-algorithms">B.1.2 Core Algorithms</h3>

<h4 id="value-based-methods">Value-Based Methods</h4>

<ul>
<li>Q-Learning fundamentals</li>
<li>Deep Q-Networks (DQN)</li>
<li>Double DQN and variants</li>
<li>Prioritized experience replay</li>
</ul>

<h4 id="policy-gradient-methods">Policy Gradient Methods</h4>

<ul>
<li>REINFORCE algorithm</li>
<li>Actor-Critic methods</li>
<li>Trust Region Policy Optimization (TRPO)</li>
<li>Proximal Policy Optimization (PPO)</li>
</ul>

<h2 id="b-2-implementation-details">B.2 Implementation Details</h2>

<h3 id="b-2-1-network-architectures">B.2.1 Network Architectures</h3>

<ul>
<li>Convolutional layers for visual input</li>
<li>Recurrent components for temporal dependencies</li>
<li>Attention mechanisms</li>
<li>Architecture search strategies</li>
</ul>

<h3 id="b-2-2-training-procedures">B.2.2 Training Procedures</h3>

<ul>
<li>Hyperparameter configurations</li>
<li>Learning rate schedules</li>
<li>Batch size considerations</li>
<li>Regularization techniques</li>
</ul>

<h2 id="b-3-experimental-setup">B.3 Experimental Setup</h2>

<h3 id="b-3-1-environment-specifications">B.3.1 Environment Specifications</h3>

<ul>
<li>State space representations</li>
<li>Action space definitions</li>
<li>Reward function designs</li>
<li>Episode termination conditions</li>
</ul>

<h3 id="b-3-2-evaluation-metrics">B.3.2 Evaluation Metrics</h3>

<ul>
<li>Average episode return</li>
<li>Sample efficiency measures</li>
<li>Convergence analysis</li>
<li>Stability indicators</li>
</ul>

<h2 id="b-4-algorithm-comparisons">B.4 Algorithm Comparisons</h2>

<h3 id="b-4-1-performance-analysis">B.4.1 Performance Analysis</h3>

<ul>
<li>Learning curves across different algorithms</li>
<li>Final performance comparisons</li>
<li>Computational resource requirements</li>
<li>Training time analysis</li>
</ul>

<h3 id="b-4-2-ablation-studies">B.4.2 Ablation Studies</h3>

<ul>
<li>Impact of different components</li>
<li>Sensitivity to hyperparameters</li>
<li>Architecture variations</li>
<li>Exploration strategies</li>
</ul>

<h2 id="b-5-code-examples">B.5 Code Examples</h2>

<h3 id="b-5-1-basic-q-learning-implementation">B.5.1 Basic Q-Learning Implementation</h3>

<pre><code class="language-python"># Simplified Q-learning pseudocode
def q_learning(env, episodes, alpha, gamma, epsilon):
    Q = initialize_q_table()
    for episode in range(episodes):
        state = env.reset()
        while not done:
            action = epsilon_greedy(Q, state, epsilon)
            next_state, reward, done = env.step(action)
            Q[state, action] += alpha * (reward + gamma * max(Q[next_state]) - Q[state, action])
            state = next_state
    return Q
</code></pre>

<h3 id="b-5-2-ppo-update-step">B.5.2 PPO Update Step</h3>

<pre><code class="language-python"># Simplified PPO update pseudocode
def ppo_update(policy, value_function, trajectories, clip_epsilon):
    for trajectory in trajectories:
        advantages = compute_advantages(trajectory, value_function)
        old_log_probs = compute_log_probs(trajectory, policy)
        
        for epoch in range(ppo_epochs):
            new_log_probs = compute_log_probs(trajectory, policy)
            ratio = exp(new_log_probs - old_log_probs)
            clipped_ratio = clip(ratio, 1 - clip_epsilon, 1 + clip_epsilon)
            policy_loss = -min(ratio * advantages, clipped_ratio * advantages)
            optimize(policy_loss)
</code></pre>

      
      
      <nav class="thesis-chapter-nav">
        
        <a href="https://vihanga.github.io/thesis/chapters/data-driven-animation/">← Chapter 2: Data-driven Character Animation</a>
        
        
        
        <a href="https://vihanga.github.io/thesis/chapters/introduction/">Chapter 1: Introduction →</a>
        
      </nav>
    </article>
  </main>
</div>

<div class="container">
  <footer class="site-footer">
  

  <p class="powered-by">
    &copy; 2018 &middot; 

    Powered by the
    <a href="https://sourcethemes.com/academic/" target="_blank" rel="noopener">Academic theme</a> for
    <a href="https://gohugo.io" target="_blank" rel="noopener">Hugo</a>.

    
    <span class="float-right" aria-hidden="true">
      <a href="#" id="back_to_top">
        <span class="button_icon">
          <i class="fas fa-chevron-up fa-2x"></i>
        </span>
      </a>
    </span>
    
  </p>
</footer>

</div>


<div id="modal" class="modal fade" role="dialog">
  <div class="modal-dialog">
    <div class="modal-content">
      <div class="modal-header">
        <h5 class="modal-title">Cite</h5>
        <button type="button" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body">
        <pre><code class="tex hljs"></code></pre>
      </div>
      <div class="modal-footer">
        <a class="btn btn-outline-primary my-1 js-copy-cite" href="#" target="_blank">
          <i class="fas fa-copy"></i> Copy
        </a>
        <a class="btn btn-outline-primary my-1 js-download-cite" href="#" target="_blank">
          <i class="fas fa-download"></i> Download
        </a>
        <div id="modal-error"></div>
      </div>
    </div>
  </div>
</div>

    

    
    

    
    
    
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js" integrity="sha512-+NqPlbbtM1QqiK8ZAo4Yrj2c4lNQoGv8P79DPtKzj++l5jnN39rHA/xsqn8zE9l0uSoxaCdrOgFs6yjyfbBxSg==" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.imagesloaded/4.1.3/imagesloaded.pkgd.min.js" integrity="sha512-umsR78NN0D23AzgoZ11K7raBD+R6hqKojyBZs1w8WvYlsI+QuKRGBx3LFCwhatzBunCjDuJpDHwxD13sLMbpRA==" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/4.1.3/js/bootstrap.min.js" integrity="sha256-VsEqElsCHSGmnmHXGQzvoWjWwoznFSZc6hs7ARLRacQ=" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.isotope/3.0.4/isotope.pkgd.min.js" integrity="sha512-VDBOIlDbuC4VWxGJNmuFRQ0Li0SKkDpmGyuhAG5LTDLd/dJ/S0WMVxriR2Y+CyPL5gzjpN4f/6iqWVBJlht0tQ==" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.2.5/jquery.fancybox.min.js" integrity="sha256-X5PoE3KU5l+JcX+w09p/wHl9AzK333C4hJ2I9S5mD4M=" crossorigin="anonymous"></script>

      
        
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js" integrity="sha256-/BfiIkHlHoVihZdc6TFuj7MmJ0TWcWsMXkeDFwhi0zw=" crossorigin="anonymous"></script>
        
      

      
      
    

    
    

    
    
    

    
    
    <script>hljs.initHighlightingOnLoad();</script>
    

    
    
    <script>
      const search_index_filename = "/index.json";
      const i18n = {
        'placeholder': "Search...",
        'results': "Search Results",
        'no_results': "No results found"
      };
      const content_type = {
        'post': "Posts",
        'project': "Projects",
        'publication' : "Publications",
        'talk' : "Talks"
        };
    </script>
    

    
    

    
    
    <script id="search-hit-fuse-template" type="text/x-template">
      <div class="search-hit" id="summary-{{key}}">
      <div class="search-hit-content">
        <div class="search-hit-name">
          <a href="{{relpermalink}}">{{title}}</a>
          <div class="article-metadata search-hit-type">{{type}}</div>
          <p class="search-hit-description">{{snippet}}</p>
        </div>
      </div>
      </div>
    </script>
    

    
    
    <script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/3.2.1/fuse.min.js" integrity="sha256-VzgmKYmhsGNNN4Ph1kMW+BjoYJM2jV5i4IlFoeZA9XI=" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/jquery.mark.min.js" integrity="sha256-4HLtjeVgH0eIB3aZ9mLYF6E8oU5chNdjU6p6rrXpl9U=" crossorigin="anonymous"></script>
    

    
    

    
    
    
    
    
    
    
    
    <script src="/js/academic.min.2861db6bcf2db4b5eade32c795453e47.js"></script>

    

  </body>
</html>
