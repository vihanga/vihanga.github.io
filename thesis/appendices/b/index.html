<!DOCTYPE html>
<html lang="en-us">
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="generator" content="Hugo 0.55.4">
  
  <title>Appendix B: Exploring Model-Free RL | Vihanga Gamage&#39;s Corner of the World Wide Web</title>
  
  <link rel="stylesheet" href="/css/thesis-book.css">
  
  
  <link rel="icon" type="image/png" href="/img/icon.png">
  <link rel="apple-touch-icon" type="image/png" href="/img/icon-192.png">
</head>
<body>
  <div class="thesis-wrapper">
    
<div class="thesis-container">
  
  <aside class="thesis-sidebar">
    <div class="thesis-sidebar-content">
      <h3 class="thesis-sidebar-title">Thesis Navigation</h3>
      
      <ul class="thesis-menu">
        <li><a href="/thesis/">← Table of Contents</a></li>
        
        <li class="thesis-menu-section">
          <span class="thesis-menu-section-title">Chapters</span>
          <ul>
            <li><a href="/thesis/chapters/introduction/" >1. Introduction</a></li>
            <li><a href="/thesis/chapters/data-driven-animation/" >2. Data-driven Animation</a></li>
            <li><a href="/thesis/chapters/model-based-rl/" >3. Model-based RL</a></li>
            <li><a href="/thesis/chapters/model-based-animation/" >4. Model-Based Animation</a></li>
            <li><a href="/thesis/chapters/data-driven-rl/" >5. Data-driven RL</a></li>
            <li><a href="/thesis/chapters/latent-dynamics/" >6. Latent Dynamics</a></li>
            <li><a href="/thesis/chapters/conversational-gestures/" >7. Conversational Gestures</a></li>
            <li><a href="/thesis/chapters/conclusion/" >8. Conclusions</a></li>
          </ul>
        </li>
        
        <li class="thesis-menu-section">
          <span class="thesis-menu-section-title">Supplementary</span>
          <ul>
            <li><a href="/thesis/supplementary/c4/">Chapter 4</a></li>
            <li><a href="/thesis/supplementary/c5a/">Chapter 5 - Part A</a></li>
            <li><a href="/thesis/supplementary/c5b/">Chapter 5 - Part B</a></li>
            <li><a href="/thesis/supplementary/c6/">Chapter 6</a></li>
            <li><a href="/thesis/supplementary/c7/">Chapter 7</a></li>
          </ul>
        </li>
        
        <li class="thesis-menu-section">
          <span class="thesis-menu-section-title">Appendices</span>
          <ul>
            <li><a href="/thesis/appendices/a/">A: Serious Games</a></li>
            <li><a href="/thesis/appendices/b/">B: Model-Free RL</a></li>
            <li><a href="/thesis/appendices/c/">C: Supplementary Index</a></li>
            <li><a href="/thesis/appendices/d/">D: Perceptual Evaluation</a></li>
            <li><a href="/thesis/appendices/e/">E: Model-free Results</a></li>
            <li><a href="/thesis/appendices/f/">F: Star Jump Calculation</a></li>
            <li><a href="/thesis/appendices/g/">G: RLAnimate Directory</a></li>
          </ul>
        </li>
      </ul>
      
      <div class="thesis-sidebar-footer">
        <a href="/" class="back-to-main">← Back to Main Site</a>
      </div>
    </div>
  </aside>

  
  <main class="thesis-main">
    <article class="thesis-content">
      <h1>Appendix B: Exploring Model-Free RL</h1>
      
      
      

<h1 id="appendix-b-exploring-model-free-rl">Appendix B: Exploring Model-Free RL</h1>

<p>This appendix provides detailed information about model-free reinforcement learning approaches explored in this thesis.</p>

<h2 id="b-1-theoretical-foundations">B.1 Theoretical Foundations</h2>

<h3 id="b-1-1-model-free-vs-model-based-rl">B.1.1 Model-Free vs Model-Based RL</h3>

<ul>
<li>Key distinctions and trade-offs</li>
<li>Computational complexity analysis</li>
<li>Sample efficiency considerations</li>
<li>Generalization capabilities</li>
</ul>

<h3 id="b-1-2-core-algorithms">B.1.2 Core Algorithms</h3>

<h4 id="value-based-methods">Value-Based Methods</h4>

<ul>
<li>Q-Learning fundamentals</li>
<li>Deep Q-Networks (DQN)</li>
<li>Double DQN and variants</li>
<li>Prioritized experience replay</li>
</ul>

<h4 id="policy-gradient-methods">Policy Gradient Methods</h4>

<ul>
<li>REINFORCE algorithm</li>
<li>Actor-Critic methods</li>
<li>Trust Region Policy Optimization (TRPO)</li>
<li>Proximal Policy Optimization (PPO)</li>
</ul>

<h2 id="b-2-implementation-details">B.2 Implementation Details</h2>

<h3 id="b-2-1-network-architectures">B.2.1 Network Architectures</h3>

<ul>
<li>Convolutional layers for visual input</li>
<li>Recurrent components for temporal dependencies</li>
<li>Attention mechanisms</li>
<li>Architecture search strategies</li>
</ul>

<h3 id="b-2-2-training-procedures">B.2.2 Training Procedures</h3>

<ul>
<li>Hyperparameter configurations</li>
<li>Learning rate schedules</li>
<li>Batch size considerations</li>
<li>Regularization techniques</li>
</ul>

<h2 id="b-3-experimental-setup">B.3 Experimental Setup</h2>

<h3 id="b-3-1-environment-specifications">B.3.1 Environment Specifications</h3>

<ul>
<li>State space representations</li>
<li>Action space definitions</li>
<li>Reward function designs</li>
<li>Episode termination conditions</li>
</ul>

<h3 id="b-3-2-evaluation-metrics">B.3.2 Evaluation Metrics</h3>

<ul>
<li>Average episode return</li>
<li>Sample efficiency measures</li>
<li>Convergence analysis</li>
<li>Stability indicators</li>
</ul>

<h2 id="b-4-algorithm-comparisons">B.4 Algorithm Comparisons</h2>

<h3 id="b-4-1-performance-analysis">B.4.1 Performance Analysis</h3>

<ul>
<li>Learning curves across different algorithms</li>
<li>Final performance comparisons</li>
<li>Computational resource requirements</li>
<li>Training time analysis</li>
</ul>

<h3 id="b-4-2-ablation-studies">B.4.2 Ablation Studies</h3>

<ul>
<li>Impact of different components</li>
<li>Sensitivity to hyperparameters</li>
<li>Architecture variations</li>
<li>Exploration strategies</li>
</ul>

<h2 id="b-5-code-examples">B.5 Code Examples</h2>

<h3 id="b-5-1-basic-q-learning-implementation">B.5.1 Basic Q-Learning Implementation</h3>

<pre><code class="language-python"># Simplified Q-learning pseudocode
def q_learning(env, episodes, alpha, gamma, epsilon):
    Q = initialize_q_table()
    for episode in range(episodes):
        state = env.reset()
        while not done:
            action = epsilon_greedy(Q, state, epsilon)
            next_state, reward, done = env.step(action)
            Q[state, action] += alpha * (reward + gamma * max(Q[next_state]) - Q[state, action])
            state = next_state
    return Q
</code></pre>

<h3 id="b-5-2-ppo-update-step">B.5.2 PPO Update Step</h3>

<pre><code class="language-python"># Simplified PPO update pseudocode
def ppo_update(policy, value_function, trajectories, clip_epsilon):
    for trajectory in trajectories:
        advantages = compute_advantages(trajectory, value_function)
        old_log_probs = compute_log_probs(trajectory, policy)
        
        for epoch in range(ppo_epochs):
            new_log_probs = compute_log_probs(trajectory, policy)
            ratio = exp(new_log_probs - old_log_probs)
            clipped_ratio = clip(ratio, 1 - clip_epsilon, 1 + clip_epsilon)
            policy_loss = -min(ratio * advantages, clipped_ratio * advantages)
            optimize(policy_loss)
</code></pre>

      
      
      <nav class="thesis-chapter-nav">
        
        <a href="https://vihanga.github.io/thesis/chapters/data-driven-animation/">← Chapter 2: Data-driven Character Animation</a>
        
        
        
        <a href="https://vihanga.github.io/thesis/chapters/introduction/">Chapter 1: Introduction →</a>
        
      </nav>
    </article>
  </main>
</div>

  </div>
</body>
</html>