<!DOCTYPE html>
<html lang="en-us">
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="generator" content="Hugo 0.55.4">
  
  <title>Appendix G: RLAnimate Directory | Thesis</title>
  
  <link rel="stylesheet" href="/css/thesis-book.css">
  
  
  <link rel="icon" type="image/svg+xml" href="/img/dramatic_v_logo.svg">
  <link rel="apple-touch-icon" href="/img/dramatic_v_logo.svg">
  
  
  <meta property="og:image" content="/img/dramatic_v_logo.svg">
  <meta property="og:image:type" content="image/svg+xml">
  <meta name="twitter:image" content="/img/dramatic_v_logo.svg">
</head>
<body>
  <div class="thesis-wrapper">
    
<div class="thesis-container">
  
  <aside class="thesis-sidebar">
    <div class="thesis-sidebar-content">
      <div class="thesis-logo">
        <img src="/img/dramatic_v_logo.svg" alt="Thesis Logo">
      </div>
      <h3 class="thesis-sidebar-title">Thesis Navigation</h3>
      
      <ul class="thesis-menu">
        <li><a href="/thesis/">← Table of Contents</a></li>
        
        <li class="thesis-menu-section">
          <span class="thesis-menu-section-title">Chapters</span>
          <ul>
            <li><a href="/thesis/chapters/introduction/" >1. Introduction</a></li>
            <li><a href="/thesis/chapters/data-driven-animation/" >2. Data-driven Animation</a></li>
            <li><a href="/thesis/chapters/model-based-rl/" >3. Model-based RL</a></li>
            <li><a href="/thesis/chapters/model-based-animation/" >4. Model-Based Animation</a></li>
            <li><a href="/thesis/chapters/data-driven-rl/" >5. Data-driven RL</a></li>
            <li><a href="/thesis/chapters/latent-dynamics/" >6. Latent Dynamics</a></li>
            <li><a href="/thesis/chapters/conversational-gestures/" >7. Conversational Gestures</a></li>
            <li><a href="/thesis/chapters/conclusion/" >8. Conclusions</a></li>
          </ul>
        </li>
        
        <li class="thesis-menu-section">
          <span class="thesis-menu-section-title">Supplementary</span>
          <ul>
            <li><a href="/thesis/supplementary/c4/">Chapter 4</a></li>
            <li><a href="/thesis/supplementary/c5a/">Chapter 5 - Part A</a></li>
            <li><a href="/thesis/supplementary/c5b/">Chapter 5 - Part B</a></li>
            <li><a href="/thesis/supplementary/c6/">Chapter 6</a></li>
            <li><a href="/thesis/supplementary/c7/">Chapter 7</a></li>
          </ul>
        </li>
        
        <li class="thesis-menu-section">
          <span class="thesis-menu-section-title">Appendices</span>
          <ul>
            <li><a href="/thesis/appendices/a/">A: Serious Games</a></li>
            <li><a href="/thesis/appendices/b/">B: Model-Free RL</a></li>
            <li><a href="/thesis/appendices/c/">C: Supplementary Index</a></li>
            <li><a href="/thesis/appendices/d/">D: Perceptual Evaluation</a></li>
            <li><a href="/thesis/appendices/e/">E: Model-free Results</a></li>
            <li><a href="/thesis/appendices/f/">F: Star Jump Calculation</a></li>
            <li><a href="/thesis/appendices/g/">G: RLAnimate Directory</a></li>
          </ul>
        </li>
      </ul>
      
      <div class="thesis-sidebar-footer">
        <a href="/" class="back-to-main">← Back to Main Site</a>
      </div>
    </div>
  </aside>

  
  <main class="thesis-main">
    <article class="thesis-content">
      <h1>Appendix G: RLAnimate Directory</h1>
      
      
      

<h1 id="appendix-g-rlanimate-directory">Appendix G: RLAnimate Directory</h1>

<p>This appendix provides a comprehensive directory of the RLAnimate framework, including its architecture, components, API reference, and usage examples.</p>

<h2 id="g-1-system-architecture">G.1 System Architecture</h2>

<h3 id="g-1-1-overview">G.1.1 Overview</h3>

<p>RLAnimate is a modular framework for physics-based character animation using reinforcement learning. The system consists of:</p>

<pre><code>RLAnimate/
├── core/
│   ├── agents/
│   ├── environments/
│   ├── physics/
│   └── utils/
├── models/
│   ├── policies/
│   ├── value_functions/
│   └── networks/
├── training/
│   ├── algorithms/
│   ├── replay_buffers/
│   └── schedulers/
├── evaluation/
│   ├── metrics/
│   ├── visualization/
│   └── benchmarks/
└── examples/
    ├── tutorials/
    ├── experiments/
    └── demos/
</code></pre>

<h3 id="g-1-2-core-components">G.1.2 Core Components</h3>

<h4 id="physics-engine-interface">Physics Engine Interface</h4>

<pre><code class="language-python">class PhysicsInterface:
    &quot;&quot;&quot;Abstract interface for physics simulation backends&quot;&quot;&quot;
    
    def step(self, actions: np.ndarray) -&gt; Tuple[State, float, bool, dict]:
        &quot;&quot;&quot;Advance simulation by one timestep&quot;&quot;&quot;
        
    def reset(self) -&gt; State:
        &quot;&quot;&quot;Reset simulation to initial state&quot;&quot;&quot;
        
    def get_state(self) -&gt; State:
        &quot;&quot;&quot;Get current simulation state&quot;&quot;&quot;
        
    def set_state(self, state: State) -&gt; None:
        &quot;&quot;&quot;Set simulation state&quot;&quot;&quot;
</code></pre>

<h4 id="character-model">Character Model</h4>

<pre><code class="language-python">class Character:
    &quot;&quot;&quot;Articulated character representation&quot;&quot;&quot;
    
    properties = {
        'num_joints': 21,
        'num_dof': 63,
        'mass': 70.0,  # kg
        'height': 1.75  # m
    }
    
    joints = [
        'pelvis', 'spine', 'chest', 'neck', 'head',
        'left_shoulder', 'left_elbow', 'left_wrist',
        'right_shoulder', 'right_elbow', 'right_wrist',
        'left_hip', 'left_knee', 'left_ankle',
        'right_hip', 'right_knee', 'right_ankle'
    ]
</code></pre>

<h2 id="g-2-api-reference">G.2 API Reference</h2>

<h3 id="g-2-1-environment-api">G.2.1 Environment API</h3>

<pre><code class="language-python">class RLAnimateEnv(gym.Env):
    &quot;&quot;&quot;Base environment for character animation tasks&quot;&quot;&quot;
    
    def __init__(self, config: Dict[str, Any]):
        &quot;&quot;&quot;
        Args:
            config: Environment configuration dictionary
        &quot;&quot;&quot;
        
    def step(self, action: np.ndarray) -&gt; Tuple[np.ndarray, float, bool, dict]:
        &quot;&quot;&quot;
        Execute action and return step information
        
        Args:
            action: Joint torques or target positions
            
        Returns:
            observation: Current state observation
            reward: Step reward
            done: Episode termination flag
            info: Additional information
        &quot;&quot;&quot;
        
    def reset(self) -&gt; np.ndarray:
        &quot;&quot;&quot;Reset environment to initial state&quot;&quot;&quot;
        
    def render(self, mode: str = 'human') -&gt; Optional[np.ndarray]:
        &quot;&quot;&quot;Render current state&quot;&quot;&quot;
</code></pre>

<h3 id="g-2-2-policy-api">G.2.2 Policy API</h3>

<pre><code class="language-python">class Policy(nn.Module):
    &quot;&quot;&quot;Base policy network class&quot;&quot;&quot;
    
    def forward(self, obs: torch.Tensor) -&gt; Distribution:
        &quot;&quot;&quot;
        Compute action distribution
        
        Args:
            obs: Observation tensor
            
        Returns:
            Action distribution (Normal or Categorical)
        &quot;&quot;&quot;
        
    def get_action(self, obs: torch.Tensor, 
                   deterministic: bool = False) -&gt; torch.Tensor:
        &quot;&quot;&quot;Sample action from policy&quot;&quot;&quot;
        
    def evaluate_actions(self, obs: torch.Tensor, 
                        actions: torch.Tensor) -&gt; Tuple[torch.Tensor, torch.Tensor]:
        &quot;&quot;&quot;Evaluate log probability and entropy of actions&quot;&quot;&quot;
</code></pre>

<h3 id="g-2-3-training-api">G.2.3 Training API</h3>

<pre><code class="language-python">class Trainer:
    &quot;&quot;&quot;Main training orchestrator&quot;&quot;&quot;
    
    def __init__(self, 
                 env: RLAnimateEnv,
                 policy: Policy,
                 algorithm: str = 'ppo',
                 config: Dict[str, Any] = None):
        &quot;&quot;&quot;Initialize trainer with environment and policy&quot;&quot;&quot;
        
    def train(self, 
              total_timesteps: int,
              callback: Optional[Callable] = None) -&gt; Policy:
        &quot;&quot;&quot;
        Train policy for specified timesteps
        
        Returns:
            Trained policy
        &quot;&quot;&quot;
        
    def save(self, path: str) -&gt; None:
        &quot;&quot;&quot;Save trained model&quot;&quot;&quot;
        
    def load(self, path: str) -&gt; None:
        &quot;&quot;&quot;Load trained model&quot;&quot;&quot;
</code></pre>

<h2 id="g-3-configuration-system">G.3 Configuration System</h2>

<h3 id="g-3-1-environment-configuration">G.3.1 Environment Configuration</h3>

<pre><code class="language-yaml"># config/env/locomotion.yaml
env:
  name: &quot;Locomotion-v1&quot;
  physics_backend: &quot;mujoco&quot;
  timestep: 0.01
  max_episode_steps: 1000
  
  observation:
    include_velocities: true
    include_accelerations: false
    history_length: 3
    
  reward:
    weights:
      task: 1.0
      energy: 0.1
      smoothness: 0.05
      balance: 0.2
      
  termination:
    fall_threshold: 0.3
    max_joint_error: 45  # degrees
</code></pre>

<h3 id="g-3-2-training-configuration">G.3.2 Training Configuration</h3>

<pre><code class="language-yaml"># config/train/ppo_default.yaml
algorithm: &quot;ppo&quot;
policy:
  network_type: &quot;mlp&quot;
  hidden_sizes: [256, 256]
  activation: &quot;tanh&quot;
  
training:
  learning_rate: 3e-4
  n_steps: 2048
  batch_size: 64
  n_epochs: 10
  gamma: 0.99
  gae_lambda: 0.95
  clip_range: 0.2
  
  schedule:
    learning_rate: &quot;linear&quot;
    clip_range: &quot;constant&quot;
</code></pre>

<h2 id="g-4-usage-examples">G.4 Usage Examples</h2>

<h3 id="g-4-1-basic-training-script">G.4.1 Basic Training Script</h3>

<pre><code class="language-python">import rlanimate as rla

# Create environment
env = rla.make_env('Locomotion-v1')

# Create policy
policy = rla.Policy(
    observation_space=env.observation_space,
    action_space=env.action_space,
    hidden_sizes=[256, 256]
)

# Create trainer
trainer = rla.Trainer(
    env=env,
    policy=policy,
    algorithm='ppo',
    config={'learning_rate': 3e-4}
)

# Train
policy = trainer.train(total_timesteps=1_000_000)

# Save model
trainer.save('models/locomotion_ppo.zip')

# Evaluate
mean_reward = rla.evaluate(env, policy, n_episodes=100)
print(f&quot;Mean reward: {mean_reward}&quot;)
</code></pre>

<h3 id="g-4-2-custom-task-definition">G.4.2 Custom Task Definition</h3>

<pre><code class="language-python">class JumpingTask(rla.Task):
    &quot;&quot;&quot;Custom jumping task&quot;&quot;&quot;
    
    def __init__(self, target_height: float = 0.5):
        self.target_height = target_height
        
    def compute_reward(self, state: State, action: np.ndarray) -&gt; float:
        # Height reward
        height_reward = np.exp(-abs(state.com_height - self.target_height))
        
        # Posture reward
        posture_reward = self.compute_posture_reward(state)
        
        # Energy penalty
        energy_penalty = -0.001 * np.sum(action**2)
        
        return height_reward + 0.5 * posture_reward + energy_penalty
        
    def is_success(self, state: State) -&gt; bool:
        return state.max_com_height &gt;= self.target_height * 0.95
</code></pre>

<h3 id="g-4-3-visualization-script">G.4.3 Visualization Script</h3>

<pre><code class="language-python">import rlanimate.visualization as viz

# Load trained model
env = rla.make_env('Locomotion-v1')
policy = rla.load_policy('models/locomotion_ppo.zip')

# Create visualizer
visualizer = viz.Visualizer(env, policy)

# Interactive visualization
visualizer.run_interactive()

# Record video
visualizer.record_video(
    'output/locomotion_demo.mp4',
    n_episodes=5,
    fps=30
)

# Generate trajectory plots
trajectories = visualizer.collect_trajectories(n_episodes=10)
viz.plot_trajectories(trajectories, save_path='output/trajectories.png')
</code></pre>

<h2 id="g-5-extension-guide">G.5 Extension Guide</h2>

<h3 id="g-5-1-adding-new-environments">G.5.1 Adding New Environments</h3>

<pre><code class="language-python"># rlanimate/envs/custom_env.py
class CustomEnv(RLAnimateEnv):
    &quot;&quot;&quot;Template for custom environment&quot;&quot;&quot;
    
    def __init__(self, config):
        super().__init__(config)
        # Initialize custom components
        
    def _compute_reward(self, state, action):
        # Implement reward function
        pass
        
    def _get_observation(self, state):
        # Implement observation extraction
        pass
        
# Register environment
from gym.envs.registration import register
register(
    id='CustomEnv-v1',
    entry_point='rlanimate.envs:CustomEnv',
)
</code></pre>

<h3 id="g-5-2-adding-new-algorithms">G.5.2 Adding New Algorithms</h3>

<pre><code class="language-python"># rlanimate/algorithms/custom_algo.py
class CustomAlgorithm(BaseAlgorithm):
    &quot;&quot;&quot;Template for custom RL algorithm&quot;&quot;&quot;
    
    def train_step(self, batch):
        # Implement training step
        pass
        
    def update_policy(self, trajectories):
        # Implement policy update
        pass
</code></pre>

<h2 id="g-6-performance-benchmarks">G.6 Performance Benchmarks</h2>

<h3 id="g-6-1-training-speed">G.6.1 Training Speed</h3>

<table>
<thead>
<tr>
<th>Algorithm</th>
<th>Steps/Second</th>
<th>GPU Memory</th>
<th>Training Time (1M steps)</th>
</tr>
</thead>

<tbody>
<tr>
<td>PPO</td>
<td>15,000</td>
<td>2.1 GB</td>
<td>1.1 hours</td>
</tr>

<tr>
<td>SAC</td>
<td>12,000</td>
<td>2.8 GB</td>
<td>1.4 hours</td>
</tr>

<tr>
<td>TD3</td>
<td>13,500</td>
<td>2.5 GB</td>
<td>1.2 hours</td>
</tr>
</tbody>
</table>

<h3 id="g-6-2-task-performance">G.6.2 Task Performance</h3>

<table>
<thead>
<tr>
<th>Task</th>
<th>PPO</th>
<th>SAC</th>
<th>TD3</th>
<th>Human Demo</th>
</tr>
</thead>

<tbody>
<tr>
<td>Walk</td>
<td>0.92</td>
<td>0.94</td>
<td>0.91</td>
<td>0.98</td>
</tr>

<tr>
<td>Run</td>
<td>0.88</td>
<td>0.91</td>
<td>0.89</td>
<td>0.97</td>
</tr>

<tr>
<td>Jump</td>
<td>0.85</td>
<td>0.82</td>
<td>0.86</td>
<td>0.95</td>
</tr>
</tbody>
</table>

<h2 id="g-7-troubleshooting">G.7 Troubleshooting</h2>

<h3 id="g-7-1-common-issues">G.7.1 Common Issues</h3>

<ol>
<li><strong>ImportError</strong>: Ensure all dependencies are installed</li>
<li><strong>CUDA Error</strong>: Check GPU compatibility and drivers</li>
<li><strong>Memory Error</strong>: Reduce batch size or model size</li>
<li><strong>Convergence Issues</strong>: Adjust learning rate or exploration</li>
</ol>

<h3 id="g-7-2-debug-mode">G.7.2 Debug Mode</h3>

<pre><code class="language-python"># Enable debug logging
import logging
logging.basicConfig(level=logging.DEBUG)

# Run with debug flags
env = rla.make_env('Locomotion-v1', debug=True)
trainer = rla.Trainer(env, policy, debug=True)
</code></pre>

<h2 id="g-8-license-and-citation">G.8 License and Citation</h2>

<h3 id="g-8-1-license">G.8.1 License</h3>

<p>RLAnimate is released under the MIT License.</p>

<h3 id="g-8-2-citation">G.8.2 Citation</h3>

<pre><code class="language-bibtex">@software{rlanimate2024,
  title={RLAnimate: A Framework for Physics-based Character Animation},
  author={[Author Names]},
  year={2024},
  url={https://github.com/[username]/rlanimate}
}
</code></pre>

      
      
      <nav class="thesis-chapter-nav">
        
        <a href="https://vihanga.github.io/thesis/chapters/conversational-gestures/">← Chapter 7: Conversational Gesture Generation</a>
        
        
        
        <a href="https://vihanga.github.io/thesis/chapters/latent-dynamics/">Chapter 6: Latent Dynamics Models for Character Animation →</a>
        
      </nav>
    </article>
  </main>
</div>

  </div>
</body>
</html>