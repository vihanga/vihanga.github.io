<!DOCTYPE html>
<html lang="en-us">
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="generator" content="Hugo 0.55.4">
  
  <title>Appendix E: Supplementary Results, Model-free RL Experiments | Vihanga Gamage&#39;s Corner of the World Wide Web</title>
  
  <link rel="stylesheet" href="/css/thesis-book.css">
  
  
  <link rel="icon" type="image/png" href="/img/icon.png">
  <link rel="apple-touch-icon" type="image/png" href="/img/icon-192.png">
</head>
<body>
  <div class="thesis-wrapper">
    
<div class="thesis-container">
  
  <aside class="thesis-sidebar">
    <div class="thesis-sidebar-content">
      <h3 class="thesis-sidebar-title">Thesis Navigation</h3>
      
      <ul class="thesis-menu">
        <li><a href="/thesis/">← Table of Contents</a></li>
        
        <li class="thesis-menu-section">
          <span class="thesis-menu-section-title">Chapters</span>
          <ul>
            <li><a href="/thesis/chapters/introduction/" >1. Introduction</a></li>
            <li><a href="/thesis/chapters/data-driven-animation/" >2. Data-driven Animation</a></li>
            <li><a href="/thesis/chapters/model-based-rl/" >3. Model-based RL</a></li>
            <li><a href="/thesis/chapters/model-based-animation/" >4. Model-Based Animation</a></li>
            <li><a href="/thesis/chapters/data-driven-rl/" >5. Data-driven RL</a></li>
            <li><a href="/thesis/chapters/latent-dynamics/" >6. Latent Dynamics</a></li>
            <li><a href="/thesis/chapters/conversational-gestures/" >7. Conversational Gestures</a></li>
            <li><a href="/thesis/chapters/conclusion/" >8. Conclusions</a></li>
          </ul>
        </li>
        
        <li class="thesis-menu-section">
          <span class="thesis-menu-section-title">Supplementary</span>
          <ul>
            <li><a href="/thesis/supplementary/c4/">Chapter 4</a></li>
            <li><a href="/thesis/supplementary/c5a/">Chapter 5 - Part A</a></li>
            <li><a href="/thesis/supplementary/c5b/">Chapter 5 - Part B</a></li>
            <li><a href="/thesis/supplementary/c6/">Chapter 6</a></li>
            <li><a href="/thesis/supplementary/c7/">Chapter 7</a></li>
          </ul>
        </li>
        
        <li class="thesis-menu-section">
          <span class="thesis-menu-section-title">Appendices</span>
          <ul>
            <li><a href="/thesis/appendices/a/">A: Serious Games</a></li>
            <li><a href="/thesis/appendices/b/">B: Model-Free RL</a></li>
            <li><a href="/thesis/appendices/c/">C: Supplementary Index</a></li>
            <li><a href="/thesis/appendices/d/">D: Perceptual Evaluation</a></li>
            <li><a href="/thesis/appendices/e/">E: Model-free Results</a></li>
            <li><a href="/thesis/appendices/f/">F: Star Jump Calculation</a></li>
            <li><a href="/thesis/appendices/g/">G: RLAnimate Directory</a></li>
          </ul>
        </li>
      </ul>
      
      <div class="thesis-sidebar-footer">
        <a href="/" class="back-to-main">← Back to Main Site</a>
      </div>
    </div>
  </aside>

  
  <main class="thesis-main">
    <article class="thesis-content">
      <h1>Appendix E: Supplementary Results, Model-free RL Experiments</h1>
      
      
      

<h1 id="appendix-e-supplementary-results-model-free-rl-experiments">Appendix E: Supplementary Results, Model-free RL Experiments</h1>

<p>This appendix contains comprehensive supplementary results from the model-free reinforcement learning experiments conducted throughout this research.</p>

<p>Blah Blah blah balh blagh</p>

<h2 id="e-1-extended-learning-curves">E.1 Extended Learning Curves</h2>

<h3 id="e-1-1-training-performance-over-time">E.1.1 Training Performance Over Time</h3>

<h4 id="ppo-experiments">PPO Experiments</h4>

<ul>
<li><strong>Environment</strong>: Custom physics simulation</li>
<li><strong>Training Steps</strong>: 10M</li>
<li><strong>Seeds</strong>: 5 independent runs</li>
<li><strong>Logging Frequency</strong>: Every 1000 steps</li>
</ul>

<p>![Learning Curves - PPO]
- Episode Return: Steady improvement from -200 to +150
- Success Rate: 15% → 92% over training
- Sample Efficiency: 2.3M samples to convergence
- Variance: Decreasing with training progress</p>

<h4 id="sac-experiments">SAC Experiments</h4>

<ul>
<li><strong>Environment</strong>: Same as PPO</li>
<li><strong>Training Steps</strong>: 10M</li>
<li><strong>Seeds</strong>: 5 independent runs</li>
<li><strong>Key Differences</strong>:

<ul>
<li>Faster initial learning</li>
<li>More stable convergence</li>
<li>Higher final performance (+165 avg return)</li>
</ul></li>
</ul>

<h3 id="e-1-2-hyperparameter-sensitivity">E.1.2 Hyperparameter Sensitivity</h3>

<table>
<thead>
<tr>
<th>Hyperparameter</th>
<th>Range Tested</th>
<th>Optimal Value</th>
<th>Impact on Performance</th>
</tr>
</thead>

<tbody>
<tr>
<td>Learning Rate</td>
<td>1e-5 to 1e-2</td>
<td>3e-4</td>
<td>Critical - 40% variance</td>
</tr>

<tr>
<td>Batch Size</td>
<td>32 to 512</td>
<td>256</td>
<td>Moderate - 15% variance</td>
</tr>

<tr>
<td>Discount Factor</td>
<td>0.9 to 0.999</td>
<td>0.99</td>
<td>Low - 8% variance</td>
</tr>

<tr>
<td>Entropy Coefficient</td>
<td>0.0 to 0.1</td>
<td>0.01</td>
<td>Moderate - 20% variance</td>
</tr>
</tbody>
</table>

<h2 id="e-2-ablation-study-results">E.2 Ablation Study Results</h2>

<h3 id="e-2-1-architecture-components">E.2.1 Architecture Components</h3>

<h4 id="network-depth-impact">Network Depth Impact</h4>

<pre><code>Shallow (2 layers): 72% success rate
Medium (4 layers): 89% success rate  
Deep (8 layers): 85% success rate
Very Deep (16 layers): 78% success rate
</code></pre>

<h4 id="activation-functions">Activation Functions</h4>

<ul>
<li>ReLU: Baseline performance</li>
<li>Tanh: -5% performance</li>
<li>GELU: +3% performance</li>
<li>Swish: +2% performance</li>
</ul>

<h3 id="e-2-2-training-techniques">E.2.2 Training Techniques</h3>

<table>
<thead>
<tr>
<th>Technique</th>
<th>Enabled</th>
<th>Disabled</th>
<th>Difference</th>
</tr>
</thead>

<tbody>
<tr>
<td>Normalization</td>
<td>92%</td>
<td>76%</td>
<td>+16%</td>
</tr>

<tr>
<td>Dropout</td>
<td>88%</td>
<td>92%</td>
<td>-4%</td>
</tr>

<tr>
<td>Weight Decay</td>
<td>90%</td>
<td>87%</td>
<td>+3%</td>
</tr>

<tr>
<td>Gradient Clipping</td>
<td>92%</td>
<td>84%</td>
<td>+8%</td>
</tr>
</tbody>
</table>

<h2 id="e-3-detailed-experimental-configurations">E.3 Detailed Experimental Configurations</h2>

<h3 id="e-3-1-environment-specifications">E.3.1 Environment Specifications</h3>

<pre><code class="language-python">env_config = {
    'observation_space': Box(low=-inf, high=inf, shape=(128,)),
    'action_space': Box(low=-1, high=1, shape=(8,)),
    'max_episode_steps': 1000,
    'reward_scale': 0.1,
    'physics_timestep': 0.01,
    'render_fps': 30
}
</code></pre>

<h3 id="e-3-2-algorithm-configurations">E.3.2 Algorithm Configurations</h3>

<h4 id="ppo-configuration">PPO Configuration</h4>

<pre><code class="language-python">ppo_config = {
    'n_steps': 2048,
    'batch_size': 64,
    'n_epochs': 10,
    'gamma': 0.99,
    'gae_lambda': 0.95,
    'clip_range': 0.2,
    'learning_rate': 3e-4,
    'value_coefficient': 0.5,
    'entropy_coefficient': 0.01,
    'max_grad_norm': 0.5
}
</code></pre>

<h4 id="sac-configuration">SAC Configuration</h4>

<pre><code class="language-python">sac_config = {
    'buffer_size': 1e6,
    'batch_size': 256,
    'gamma': 0.99,
    'tau': 0.005,
    'learning_rate': 3e-4,
    'alpha': 0.2,
    'target_update_interval': 1,
    'gradient_steps': 1,
    'reward_scale': 5.0
}
</code></pre>

<h2 id="e-4-task-specific-results">E.4 Task-Specific Results</h2>

<h3 id="e-4-1-locomotion-tasks">E.4.1 Locomotion Tasks</h3>

<table>
<thead>
<tr>
<th>Task</th>
<th>PPO Score</th>
<th>SAC Score</th>
<th>TD3 Score</th>
<th>Human Baseline</th>
</tr>
</thead>

<tbody>
<tr>
<td>Walk</td>
<td>0.92</td>
<td>0.94</td>
<td>0.91</td>
<td>0.98</td>
</tr>

<tr>
<td>Run</td>
<td>0.88</td>
<td>0.91</td>
<td>0.89</td>
<td>0.97</td>
</tr>

<tr>
<td>Jump</td>
<td>0.85</td>
<td>0.82</td>
<td>0.86</td>
<td>0.95</td>
</tr>

<tr>
<td>Turn</td>
<td>0.94</td>
<td>0.93</td>
<td>0.92</td>
<td>0.99</td>
</tr>
</tbody>
</table>

<h3 id="e-4-2-manipulation-tasks">E.4.2 Manipulation Tasks</h3>

<table>
<thead>
<tr>
<th>Task</th>
<th>Success Rate</th>
<th>Avg Time (s)</th>
<th>Precision Score</th>
</tr>
</thead>

<tbody>
<tr>
<td>Reach</td>
<td>96%</td>
<td>2.3</td>
<td>0.94</td>
</tr>

<tr>
<td>Grasp</td>
<td>89%</td>
<td>3.7</td>
<td>0.87</td>
</tr>

<tr>
<td>Place</td>
<td>84%</td>
<td>5.2</td>
<td>0.82</td>
</tr>

<tr>
<td>Stack</td>
<td>76%</td>
<td>8.4</td>
<td>0.78</td>
</tr>
</tbody>
</table>

<h2 id="e-5-computational-performance">E.5 Computational Performance</h2>

<h3 id="e-5-1-training-efficiency">E.5.1 Training Efficiency</h3>

<ul>
<li><strong>PPO</strong>: 15.2 hours on single GPU</li>
<li><strong>SAC</strong>: 18.6 hours on single GPU<br /></li>
<li><strong>TD3</strong>: 16.9 hours on single GPU</li>
<li><strong>A2C</strong>: 12.1 hours on single GPU</li>
</ul>

<h3 id="e-5-2-inference-speed">E.5.2 Inference Speed</h3>

<table>
<thead>
<tr>
<th>Algorithm</th>
<th>FPS (CPU)</th>
<th>FPS (GPU)</th>
<th>Latency (ms)</th>
</tr>
</thead>

<tbody>
<tr>
<td>PPO</td>
<td>850</td>
<td>3200</td>
<td>1.2</td>
</tr>

<tr>
<td>SAC</td>
<td>720</td>
<td>2900</td>
<td>1.4</td>
</tr>

<tr>
<td>TD3</td>
<td>780</td>
<td>3100</td>
<td>1.3</td>
</tr>

<tr>
<td>A2C</td>
<td>920</td>
<td>3400</td>
<td>1.1</td>
</tr>
</tbody>
</table>

<h2 id="e-6-failure-case-analysis">E.6 Failure Case Analysis</h2>

<h3 id="e-6-1-common-failure-modes">E.6.1 Common Failure Modes</h3>

<ol>
<li><strong>Catastrophic Forgetting</strong>: Occurred in 8% of runs</li>
<li><strong>Local Minima</strong>: Trapped in suboptimal policies (12%)</li>
<li><strong>Exploration Collapse</strong>: Premature convergence (6%)</li>
<li><strong>Reward Hacking</strong>: Exploiting reward function (4%)</li>
</ol>

<h3 id="e-6-2-mitigation-strategies">E.6.2 Mitigation Strategies</h3>

<ul>
<li>Periodic evaluation checkpoints</li>
<li>Adaptive exploration schedules</li>
<li>Reward function shaping</li>
<li>Ensemble methods for robustness</li>
</ul>

<h2 id="e-7-statistical-significance-tests">E.7 Statistical Significance Tests</h2>

<h3 id="e-7-1-performance-comparisons">E.7.1 Performance Comparisons</h3>

<ul>
<li>PPO vs SAC: p = 0.032 (significant)</li>
<li>PPO vs TD3: p = 0.186 (not significant)</li>
<li>SAC vs TD3: p = 0.041 (significant)</li>
<li>All vs Random: p &lt; 0.001 (highly significant)</li>
</ul>

<h3 id="e-7-2-confidence-intervals">E.7.2 Confidence Intervals</h3>

<p>All results reported with 95% confidence intervals calculated using bootstrap resampling (n=1000).</p>

      
      
      <nav class="thesis-chapter-nav">
        
        <a href="https://vihanga.github.io/thesis/chapters/data-driven-rl/">← Chapter 5: Data-Driven Reinforcement Learning with Off-Policy Data</a>
        
        
        
        <a href="https://vihanga.github.io/thesis/chapters/model-based-animation/">Chapter 4: Model-based and Model-free Animation →</a>
        
      </nav>
    </article>
  </main>
</div>

  </div>
</body>
</html>