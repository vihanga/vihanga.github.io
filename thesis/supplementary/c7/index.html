<!DOCTYPE html>
<html lang="en-us">
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="generator" content="Hugo 0.55.4">
  
  <title>Chapter 7 Supplementary Material: Beat Gestures | Thesis</title>
  
  <link rel="stylesheet" href="/css/thesis-book.css">
  
  
  <link rel="icon" type="image/svg+xml" href="/img/dramatic_v_logo.svg">
  <link rel="apple-touch-icon" href="/img/dramatic_v_logo.svg">
  
  
  <meta property="og:image" content="/img/dramatic_v_logo.svg">
  <meta property="og:image:type" content="image/svg+xml">
  <meta name="twitter:image" content="/img/dramatic_v_logo.svg">
</head>
<body>
  <div class="thesis-wrapper">
    
<div class="thesis-container">
  
  <aside class="thesis-sidebar">
    <div class="thesis-sidebar-content">
      <div class="thesis-logo">
        <img src="/img/dramatic_v_logo.svg" alt="Thesis Logo">
      </div>
      <h3 class="thesis-sidebar-title">Thesis Navigation</h3>
      
      <ul class="thesis-menu">
        <li><a href="/thesis/">← Table of Contents</a></li>
        
        <li class="thesis-menu-section">
          <span class="thesis-menu-section-title">Chapters</span>
          <ul>
            <li><a href="/thesis/chapters/introduction/" >1. Introduction</a></li>
            <li><a href="/thesis/chapters/data-driven-animation/" >2. Data-driven Animation</a></li>
            <li><a href="/thesis/chapters/model-based-rl/" >3. Model-based RL</a></li>
            <li><a href="/thesis/chapters/model-based-animation/" >4. Model-Based Animation</a></li>
            <li><a href="/thesis/chapters/data-driven-rl/" >5. Data-driven RL</a></li>
            <li><a href="/thesis/chapters/latent-dynamics/" >6. Latent Dynamics</a></li>
            <li><a href="/thesis/chapters/conversational-gestures/" >7. Conversational Gestures</a></li>
            <li><a href="/thesis/chapters/conclusion/" >8. Conclusions</a></li>
          </ul>
        </li>
        
        <li class="thesis-menu-section">
          <span class="thesis-menu-section-title">Supplementary</span>
          <ul>
            <li><a href="/thesis/supplementary/c4/">Chapter 4</a></li>
            <li><a href="/thesis/supplementary/c5a/">Chapter 5 - Part A</a></li>
            <li><a href="/thesis/supplementary/c5b/">Chapter 5 - Part B</a></li>
            <li><a href="/thesis/supplementary/c6/">Chapter 6</a></li>
            <li><a href="/thesis/supplementary/c7/">Chapter 7</a></li>
          </ul>
        </li>
        
        <li class="thesis-menu-section">
          <span class="thesis-menu-section-title">Appendices</span>
          <ul>
            <li><a href="/thesis/appendices/a/">A: Serious Games</a></li>
            <li><a href="/thesis/appendices/b/">B: Model-Free RL</a></li>
            <li><a href="/thesis/appendices/c/">C: Supplementary Index</a></li>
            <li><a href="/thesis/appendices/d/">D: Perceptual Evaluation</a></li>
            <li><a href="/thesis/appendices/e/">E: Model-free Results</a></li>
            <li><a href="/thesis/appendices/f/">F: Star Jump Calculation</a></li>
            <li><a href="/thesis/appendices/g/">G: RLAnimate Directory</a></li>
          </ul>
        </li>
      </ul>
      
      <div class="thesis-sidebar-footer">
        <a href="/" class="back-to-main">← Back to Main Site</a>
      </div>
    </div>
  </aside>

  
  <main class="thesis-main">
    <article class="thesis-content">
      <h1>Chapter 7 Supplementary Material: Beat Gestures</h1>
      
      
      

<h1 id="chapter-7-supplementary-material">Chapter 7 Supplementary Material</h1>

<h2 id="beat-gesture-generation-results">Beat Gesture Generation Results</h2>

<p>This page contains supplementary material referenced in Chapter 7 of the thesis, including video demonstrations of RLAnimate A4 generating beat gestures from speech input.</p>

<h3 id="a3-agent-demonstrations">A3 Agent Demonstrations</h3>

<h4 id="quaternion-based-waving-and-pointing-with-fingers">Quaternion-based Waving and Pointing with Fingers</h4>

<p>[PLACEHOLDER: Video grid showing:
- A3 agent with quaternion rotations
- Waving behaviors with finger movement
- Pointing behaviors with finger articulation
- Comparison with A2 Euler-based approach]</p>

<h4 id="animation-dynamics-model-results">Animation Dynamics Model Results</h4>

<p>[PLACEHOLDER: Videos demonstrating:
- Improved stability with animation dynamics
- Reduced finger artifacts
- Smoother transitions
- Natural finger configurations]</p>

<h3 id="a4-agent-beat-gesture-generation">A4 Agent - Beat Gesture Generation</h3>

<h4 id="speech-driven-beat-gestures">Speech-driven Beat Gestures</h4>

<p>[PLACEHOLDER: Video grid showing:
- A4 agent generating beat gestures from various speech inputs
- Different speaker voices (male/female)
- Various speech tempos and styles
- Emotional speech variations]</p>

<h4 id="realism-regularization-components">Realism Regularization Components</h4>

<p>[PLACEHOLDER: Visualizations showing:
- Human-likeness regularization effects
- Physics-based constraints in action
- Smoothness enforcement results
- Adversarial training impact]</p>

<h3 id="comparison-with-baselines">Comparison with Baselines</h3>

<h4 id="rlanimate-a4-vs-gesticulator">RLAnimate A4 vs Gesticulator</h4>

<p>[PLACEHOLDER: Side-by-side videos showing:
- Same speech input, different outputs
- A4&rsquo;s superior naturalness
- Gesticulator&rsquo;s mechanical movements
- Synchronization quality comparison]</p>

<h4 id="rlanimate-a4-vs-motion-capture">RLAnimate A4 vs Motion Capture</h4>

<p>[PLACEHOLDER: Comparison videos showing:
- A4 output alongside ground truth motion capture
- Statistical similarity in movement patterns
- Variability and naturalness comparison
- Perceptual quality assessment]</p>

<h3 id="technical-evaluation-metrics">Technical Evaluation Metrics</h3>

<h4 id="acceleration-analysis">Acceleration Analysis</h4>

<p>[PLACEHOLDER: Plots and visualizations showing:
- Joint acceleration profiles
- Rate of change of acceleration
- Comparison across methods
- Statistical distributions]</p>

<h4 id="smoothness-metrics">Smoothness Metrics</h4>

<p>[PLACEHOLDER: Graphs displaying:
- Smoothness scores over time
- Per-joint smoothness analysis
- Comparison with motion capture baseline
- Impact of smoothness regularization]</p>

<h3 id="perceptual-evaluation-materials">Perceptual Evaluation Materials</h3>

<h4 id="study-setup">Study Setup</h4>

<p>[PLACEHOLDER: Information about:
- Video clips used in the study
- Randomization procedure
- Question presentation format
- Participant interface screenshots]</p>

<h4 id="detailed-results">Detailed Results</h4>

<p>[PLACEHOLDER: Extended results including:
- Full statistical analysis
- Per-clip ratings breakdown
- Participant comments
- Demographic correlations]</p>

<h3 id="ablation-studies">Ablation Studies</h3>

<h4 id="realism-regularization-components-1">Realism Regularization Components</h4>

<p>[PLACEHOLDER: Videos showing A4 performance:
- Without human-likeness (H only)
- Without physics (P only)
- Without smoothness (S only)
- Without adversarial (A only)
- Various combinations]</p>

<h4 id="architecture-ablations">Architecture Ablations</h4>

<p>[PLACEHOLDER: Comparisons of:
- A4 with different hidden dimensions
- Impact of recurrent vs feedforward realism model
- Effect of different loss weightings
- Training stability analysis]</p>

<h3 id="speech-feature-analysis">Speech Feature Analysis</h3>

<h4 id="hubert-representations">HuBERT Representations</h4>

<p>[PLACEHOLDER: Visualizations of:
- Raw HuBERT features
- Temporal patterns in speech
- Correlation with gesture timing
- Feature importance analysis]</p>

<h3 id="generalization-tests">Generalization Tests</h3>

<h4 id="novel-speech-inputs">Novel Speech Inputs</h4>

<p>[PLACEHOLDER: Videos showing A4 performance on:
- Different languages
- Non-speech vocalizations
- Singing and rhythmic speech
- Extreme speech rates]</p>

<h4 id="style-transfer">Style Transfer</h4>

<p>[PLACEHOLDER: Demonstrations of:
- Gesture style variations
- Character-specific adaptations
- Cultural gesture differences
- Personality-driven modifications]</p>

<h3 id="implementation-resources">Implementation Resources</h3>

<h4 id="code-architecture">Code Architecture</h4>

<p>[PLACEHOLDER: Diagrams and explanations of:
- Complete A4 architecture
- Training pipeline
- Inference optimization
- Real-time implementation details]</p>

<h4 id="training-data">Training Data</h4>

<p>[PLACEHOLDER: Information about:
- Motion capture dataset used
- Speech-gesture alignment process
- Data augmentation techniques
- Training/validation splits]</p>

<h2 id="navigation">Navigation</h2>

<ul>
<li><a href="../c6/">← Chapter 6 Supplementary Material</a></li>
<li><a href="https://vihanga.github.io/thesis/" target="_blank">← Back to thesis main page</a></li>
<li><a href="https://vihanga.github.io/thesis/chapters/conversational-gestures/" target="_blank">Chapter 7 in main thesis</a></li>
</ul>

      
      
      <nav class="thesis-chapter-nav">
        
        <a href="https://vihanga.github.io/thesis/publications/MIG18/">← MIG&#39;18 Supplementary Material</a>
        
        
        
        <a href="https://vihanga.github.io/thesis/supplementary/c6/">Chapter 6 Supplementary Material: RLAnimate Waving and Pointing →</a>
        
      </nav>
    </article>
  </main>
</div>

  </div>
</body>
</html>