<!DOCTYPE html>
<html lang="en-us">
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="generator" content="Hugo 0.55.4">
  
  <title>Chapter 6: Supplementary Material | Thesis</title>
  
  <link rel="stylesheet" href="/css/thesis-book.css">
  
  
  <link rel="icon" type="image/svg+xml" href="/img/dramatic_v_logo.svg">
  <link rel="apple-touch-icon" href="/img/dramatic_v_logo.svg">
  
  
  <meta property="og:image" content="/img/dramatic_v_logo.svg">
  <meta property="og:image:type" content="image/svg+xml">
  <meta name="twitter:image" content="/img/dramatic_v_logo.svg">
</head>
<body>
  <div class="thesis-wrapper">
    
<div class="thesis-container">
  
  <aside class="thesis-sidebar">
    <div class="thesis-sidebar-content">
      <div class="thesis-logo">
        <img src="/img/dramatic_v_logo.svg" alt="Thesis Logo">
      </div>
      <h3 class="thesis-sidebar-title">Thesis Navigation</h3>
      
      <ul class="thesis-menu">
        <li><a href="/thesis/">← Table of Contents</a></li>
        
        <li class="thesis-menu-section">
          <span class="thesis-menu-section-title">Chapters</span>
          <ul>
            <li><a href="/thesis/chapters/introduction/" >1. Introduction</a></li>
            <li><a href="/thesis/chapters/data-driven-animation/" >2. Data-driven Animation</a></li>
            <li><a href="/thesis/chapters/model-based-rl/" >3. Model-based RL</a></li>
            <li><a href="/thesis/chapters/model-based-animation/" >4. Model-Based Animation</a></li>
            <li><a href="/thesis/chapters/data-driven-rl/" >5. Data-driven RL</a></li>
            <li><a href="/thesis/chapters/latent-dynamics/" >6. Latent Dynamics</a></li>
            <li><a href="/thesis/chapters/conversational-gestures/" >7. Conversational Gestures</a></li>
            <li><a href="/thesis/chapters/conclusion/" >8. Conclusions</a></li>
          </ul>
        </li>
        
        <li class="thesis-menu-section">
          <span class="thesis-menu-section-title">Supplementary</span>
          <ul>
            <li><a href="/thesis/supplementary/c4/">Chapter 4</a></li>
            <li><a href="/thesis/supplementary/c5a/">Chapter 5 - Part A</a></li>
            <li><a href="/thesis/supplementary/c5b/">Chapter 5 - Part B</a></li>
            <li><a href="/thesis/supplementary/c6/">Chapter 6</a></li>
            <li><a href="/thesis/supplementary/c7/">Chapter 7</a></li>
          </ul>
        </li>
        
        <li class="thesis-menu-section">
          <span class="thesis-menu-section-title">Appendices</span>
          <ul>
            <li><a href="/thesis/appendices/a/">A: Serious Games</a></li>
            <li><a href="/thesis/appendices/b/">B: Model-Free RL</a></li>
            <li><a href="/thesis/appendices/c/">C: Supplementary Index</a></li>
            <li><a href="/thesis/appendices/d/">D: Perceptual Evaluation</a></li>
            <li><a href="/thesis/appendices/e/">E: Model-free Results</a></li>
            <li><a href="/thesis/appendices/f/">F: Star Jump Calculation</a></li>
            <li><a href="/thesis/appendices/g/">G: RLAnimate Directory</a></li>
          </ul>
        </li>
      </ul>
      
      <div class="thesis-sidebar-footer">
        <a href="/" class="back-to-main">← Back to Main Site</a>
      </div>
    </div>
  </aside>

  
  <main class="thesis-main">
    <article class="thesis-content">
      <h1>Chapter 6: Supplementary Material</h1>
      
      
      

<h2 id="video-demonstrations">Video Demonstrations</h2>

<h3 id="latent-space-visualization">Latent Space Visualization</h3>

<ul>
<li><strong>Motion Embeddings</strong>: t-SNE and PCA visualizations of learned representations</li>
<li><strong>Interpolation Results</strong>: Smooth transitions between different motion styles</li>
<li><strong>Latent Dynamics</strong>: Evolution of latent codes during motion sequences</li>
<li><strong>Disentanglement</strong>: Independent control of motion factors</li>
</ul>

<h3 id="generative-results">Generative Results</h3>

<ul>
<li>Novel motion synthesis from latent codes</li>
<li>Style transfer between characters</li>
<li>Motion completion and in-betweening</li>
<li>Multi-modal motion generation</li>
</ul>

<h2 id="additional-results">Additional Results</h2>

<h3 id="quantitative-evaluation">Quantitative Evaluation</h3>

<table>
<thead>
<tr>
<th>Model Variant</th>
<th>Reconstruction Error</th>
<th>Latent Dim</th>
<th>Disentanglement Score</th>
<th>FID Score</th>
</tr>
</thead>

<tbody>
<tr>
<td>VAE</td>
<td>0.082</td>
<td>128</td>
<td>0.72</td>
<td>45.3</td>
</tr>

<tr>
<td>β-VAE</td>
<td>0.091</td>
<td>128</td>
<td>0.85</td>
<td>48.1</td>
</tr>

<tr>
<td>VQ-VAE</td>
<td>0.075</td>
<td>512 codes</td>
<td>0.68</td>
<td>42.7</td>
</tr>

<tr>
<td>Proposed</td>
<td>0.071</td>
<td>64</td>
<td>0.89</td>
<td>38.2</td>
</tr>
</tbody>
</table>

<h3 id="ablation-studies">Ablation Studies</h3>

<ul>
<li>Impact of latent dimensionality</li>
<li>Different regularization strategies</li>
<li>Architecture choices for encoder/decoder</li>
<li>Training objective variations</li>
</ul>

<h3 id="generalization-analysis">Generalization Analysis</h3>

<ul>
<li>Cross-dataset performance</li>
<li>Different skeleton configurations</li>
<li>Unseen motion categories</li>
<li>Temporal extrapolation</li>
</ul>

<h2 id="code-examples">Code Examples</h2>

<h3 id="variational-autoencoder-architecture">Variational Autoencoder Architecture</h3>

<pre><code class="language-python">class MotionVAE(nn.Module):
    def __init__(self, input_dim, latent_dim, hidden_dims=[512, 256]):
        super().__init__()
        self.latent_dim = latent_dim
        
        # Encoder
        encoder_layers = []
        in_dim = input_dim
        for h_dim in hidden_dims:
            encoder_layers.extend([
                nn.Linear(in_dim, h_dim),
                nn.ReLU(),
                nn.BatchNorm1d(h_dim)
            ])
            in_dim = h_dim
        
        self.encoder = nn.Sequential(*encoder_layers)
        self.fc_mu = nn.Linear(hidden_dims[-1], latent_dim)
        self.fc_var = nn.Linear(hidden_dims[-1], latent_dim)
        
        # Decoder
        decoder_layers = []
        hidden_dims.reverse()
        in_dim = latent_dim
        for h_dim in hidden_dims:
            decoder_layers.extend([
                nn.Linear(in_dim, h_dim),
                nn.ReLU(),
                nn.BatchNorm1d(h_dim)
            ])
            in_dim = h_dim
        
        self.decoder = nn.Sequential(*decoder_layers)
        self.final_layer = nn.Linear(hidden_dims[-1], input_dim)
        
    def encode(self, x):
        h = self.encoder(x)
        return self.fc_mu(h), self.fc_var(h)
    
    def reparameterize(self, mu, logvar):
        std = torch.exp(0.5 * logvar)
        eps = torch.randn_like(std)
        return mu + eps * std
    
    def decode(self, z):
        h = self.decoder(z)
        return self.final_layer(h)
    
    def forward(self, x):
        mu, logvar = self.encode(x)
        z = self.reparameterize(mu, logvar)
        return self.decode(z), mu, logvar
</code></pre>

<h3 id="latent-dynamics-model">Latent Dynamics Model</h3>

<pre><code class="language-python">class LatentDynamics(nn.Module):
    def __init__(self, latent_dim, action_dim, hidden_dim=256):
        super().__init__()
        self.dynamics_net = nn.Sequential(
            nn.Linear(latent_dim + action_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, latent_dim)
        )
        
        # Learned prior for regularization
        self.prior_net = nn.Sequential(
            nn.Linear(latent_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, latent_dim * 2)  # mu and logvar
        )
    
    def forward(self, z_t, a_t):
        &quot;&quot;&quot;Predict next latent state given current state and action&quot;&quot;&quot;
        z_next = self.dynamics_net(torch.cat([z_t, a_t], dim=-1))
        return z_t + z_next  # Residual connection
    
    def get_prior(self, z_t):
        &quot;&quot;&quot;Compute prior distribution for next state&quot;&quot;&quot;
        prior_params = self.prior_net(z_t)
        mu, logvar = torch.chunk(prior_params, 2, dim=-1)
        return mu, logvar
</code></pre>

<h3 id="training-pipeline">Training Pipeline</h3>

<pre><code class="language-python">def train_latent_model(model, dynamics_model, dataloader, epochs=100):
    optimizer = torch.optim.Adam(
        list(model.parameters()) + list(dynamics_model.parameters()),
        lr=1e-3
    )
    
    for epoch in range(epochs):
        for batch in dataloader:
            motion_sequence = batch['motion']  # Shape: (B, T, D)
            
            # Encode entire sequence
            latent_sequence = []
            for t in range(motion_sequence.size(1)):
                mu, logvar = model.encode(motion_sequence[:, t])
                z = model.reparameterize(mu, logvar)
                latent_sequence.append(z)
            
            latent_sequence = torch.stack(latent_sequence, dim=1)
            
            # Reconstruction loss
            recon_loss = 0
            for t in range(motion_sequence.size(1)):
                recon = model.decode(latent_sequence[:, t])
                recon_loss += F.mse_loss(recon, motion_sequence[:, t])
            
            # Dynamics loss
            dynamics_loss = 0
            for t in range(motion_sequence.size(1) - 1):
                z_pred = dynamics_model(latent_sequence[:, t], batch['actions'][:, t])
                dynamics_loss += F.mse_loss(z_pred, latent_sequence[:, t + 1])
            
            # KL divergence for VAE
            kl_loss = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())
            
            # Total loss
            loss = recon_loss + 0.1 * dynamics_loss + 0.001 * kl_loss
            
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()
</code></pre>

<h2 id="dataset-information">Dataset Information</h2>

<h3 id="motion-sequences-used">Motion Sequences Used</h3>

<ul>
<li><strong>CMU MoCap</strong>: 2,500 sequences covering 100+ motion types</li>
<li><strong>Human3.6M</strong>: 3.6 million frames of daily activities</li>
<li><strong>Custom Dance Dataset</strong>: 500 professional dance sequences</li>
<li><strong>Sports Motion Library</strong>: 1,000 athletic movement clips</li>
</ul>

<h3 id="data-preprocessing">Data Preprocessing</h3>

<pre><code class="language-python"># Motion data normalization pipeline
def preprocess_motion_sequences(raw_data):
    # Global position normalization
    normalized = normalize_root_position(raw_data)
    
    # Velocity and acceleration features
    velocities = compute_finite_differences(normalized, order=1)
    accelerations = compute_finite_differences(normalized, order=2)
    
    # Joint angle representation
    joint_angles = forward_kinematics_to_angles(normalized)
    
    # Combine features
    features = np.concatenate([
        normalized,
        velocities * 0.1,  # Scale velocities
        accelerations * 0.01,  # Scale accelerations
        joint_angles
    ], axis=-1)
    
    return features
</code></pre>

<h3 id="evaluation-protocols">Evaluation Protocols</h3>

<ol>
<li><p><strong>Reconstruction Quality</strong></p>

<ul>
<li>Per-joint position error</li>
<li>Velocity matching</li>
<li>Foot sliding artifacts</li>
<li>Motion smoothness</li>
</ul></li>

<li><p><strong>Latent Space Quality</strong></p>

<ul>
<li>Interpolation smoothness</li>
<li>Disentanglement metrics</li>
<li>Coverage and diversity</li>
<li>Clustering quality</li>
</ul></li>
</ol>

<h2 id="computational-resources">Computational Resources</h2>

<h3 id="training-infrastructure">Training Infrastructure</h3>

<ul>
<li><strong>GPU</strong>: 4x NVIDIA A100 (40GB)</li>
<li><strong>Training Time</strong>: 72 hours for full model</li>
<li><strong>Batch Size</strong>: 256 sequences</li>
<li><strong>Memory Usage</strong>: ~35GB GPU memory</li>
</ul>

<h3 id="inference-performance">Inference Performance</h3>

<table>
<thead>
<tr>
<th>Operation</th>
<th>Time (ms)</th>
<th>Memory (MB)</th>
</tr>
</thead>

<tbody>
<tr>
<td>Encode</td>
<td>0.8</td>
<td>45</td>
</tr>

<tr>
<td>Decode</td>
<td>1.2</td>
<td>52</td>
</tr>

<tr>
<td>Dynamics Step</td>
<td>0.3</td>
<td>18</td>
</tr>

<tr>
<td>Full Pipeline</td>
<td>2.5</td>
<td>120</td>
</tr>
</tbody>
</table>

<h2 id="supplementary-figures">Supplementary Figures</h2>

<h3 id="latent-space-analysis">Latent Space Analysis</h3>

<ul>
<li>Distribution of learned codes</li>
<li>Principal components visualization</li>
<li>Trajectory analysis in latent space</li>
<li>Correlation with semantic attributes</li>
</ul>

<h3 id="architecture-variants">Architecture Variants</h3>

<ul>
<li>Different encoder/decoder architectures</li>
<li>Skip connections impact</li>
<li>Attention mechanisms</li>
<li>Temporal modeling choices</li>
</ul>

<h2 id="advanced-applications">Advanced Applications</h2>

<h3 id="motion-editing-interface">Motion Editing Interface</h3>

<pre><code class="language-python">class LatentMotionEditor:
    def __init__(self, vae_model):
        self.model = vae_model
        self.attribute_directions = self.learn_attribute_directions()
    
    def edit_motion(self, motion, attribute, strength):
        &quot;&quot;&quot;Edit motion by manipulating latent codes&quot;&quot;&quot;
        # Encode to latent space
        z, _ = self.model.encode(motion)
        
        # Apply attribute direction
        direction = self.attribute_directions[attribute]
        z_edited = z + strength * direction
        
        # Decode back to motion
        edited_motion = self.model.decode(z_edited)
        
        return edited_motion
    
    def interpolate(self, motion1, motion2, alpha):
        &quot;&quot;&quot;Smooth interpolation between motions&quot;&quot;&quot;
        z1, _ = self.model.encode(motion1)
        z2, _ = self.model.encode(motion2)
        
        # Spherical linear interpolation
        z_interp = slerp(z1, z2, alpha)
        
        return self.model.decode(z_interp)
</code></pre>

<h3 id="real-time-applications">Real-time Applications</h3>

<ul>
<li>Interactive motion synthesis</li>
<li>Online motion compression</li>
<li>Latent space control interfaces</li>
<li>Motion prediction and completion</li>
</ul>

<hr />

<p><em>Code and trained models available at the project repository. Contact for dataset access.</em></p>

      
      
      <nav class="thesis-chapter-nav">
        
        <a href="https://vihanga.github.io/thesis/supplementary/c7/">← Chapter 7: Supplementary Material</a>
        
        
        
        <a href="https://vihanga.github.io/thesis/supplementary/c5b/">Chapter 5b: Supplementary Material →</a>
        
      </nav>
    </article>
  </main>
</div>

  </div>
</body>
</html>