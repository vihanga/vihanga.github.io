[{"authors":null,"categories":null,"content":" Appendix A: Exploring Engagement and Efficiency in Serious Games This appendix provides supplementary material related to the exploration of engagement and efficiency in serious games, as discussed in the main thesis.\nA.1 Game Design Principles A.1.1 Engagement Metrics  Player retention rates Time-on-task measurements User satisfaction surveys Learning outcome assessments  A.1.2 Efficiency Considerations  Computational resource usage Response time optimization Scalability factors Performance benchmarks  A.2 Case Studies A.2.1 Educational Games Analysis of engagement patterns in educational serious games, including: - Mathematics learning applications - Language acquisition platforms - Science simulation environments\nA.2.2 Training Simulations Efficiency metrics from professional training applications: - Medical procedure simulators - Military training systems - Industrial safety programs\nA.3 Experimental Data A.3.1 User Study Results Detailed results from user studies examining: - Engagement levels across different game mechanics - Efficiency trade-offs in game design decisions - Comparative analysis of different approaches\nA.3.2 Performance Metrics  Frame rate analysis Memory usage patterns Network latency measurements User interface responsiveness  A.4 Design Guidelines A.4.1 Balancing Engagement and Efficiency Best practices for optimizing both user engagement and system efficiency: - Progressive complexity scaling - Adaptive difficulty systems - Resource-aware design patterns\nA.4.2 Implementation Recommendations  Architecture considerations Technology stack selections Optimization strategies  ","date":1737504000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1737504000,"objectID":"5dc0883ca3b72b4553732b68c215c9ce","permalink":"https://vihanga.github.io/thesis/appendices/a/","publishdate":"2025-01-22T00:00:00Z","relpermalink":"/thesis/appendices/a/","section":"thesis","summary":"Appendix A: Exploring Engagement and Efficiency in Serious Games This appendix provides supplementary material related to the exploration of engagement and efficiency in serious games, as discussed in the main thesis.\nA.1 Game Design Principles A.1.1 Engagement Metrics  Player retention rates Time-on-task measurements User satisfaction surveys Learning outcome assessments  A.1.2 Efficiency Considerations  Computational resource usage Response time optimization Scalability factors Performance benchmarks  A.2 Case Studies A.","tags":null,"title":"Appendix A: Exploring Engagement and Efficiency in Serious Games","type":"thesis"},{"authors":null,"categories":null,"content":" 1.1 Engaging Virtual Characters [Placeholder for section on engaging virtual characters]\n1.1.1 Evolution of Virtual Characters in Popular Culture [Placeholder for evolution of virtual characters]\n1.1.2 Applications of Virtual Characters [Placeholder for applications]\n1.1.3 Portraying Natural Human-like Behaviour and Scalability [Placeholder for natural behavior and scalability]\n1.2 Research Questions [Placeholder for research questions]\n1.3 Research Approach [Placeholder for research approach]\n1.4 Summary of Contributions 1.4.1 Data-driven Character Animation [Placeholder for data-driven contributions]\n1.4.2 Model-based Reinforcement Learning [Placeholder for model-based RL contributions]\n1.5 Thesis Organisation [Placeholder for thesis organization]\n","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"981946ce109800ca64235ec2433b470c","permalink":"https://vihanga.github.io/thesis/chapters/introduction/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/thesis/chapters/introduction/","section":"thesis","summary":"1.1 Engaging Virtual Characters [Placeholder for section on engaging virtual characters]\n1.1.1 Evolution of Virtual Characters in Popular Culture [Placeholder for evolution of virtual characters]\n1.1.2 Applications of Virtual Characters [Placeholder for applications]\n1.1.3 Portraying Natural Human-like Behaviour and Scalability [Placeholder for natural behavior and scalability]\n1.2 Research Questions [Placeholder for research questions]\n1.3 Research Approach [Placeholder for research approach]\n1.4 Summary of Contributions 1.4.1 Data-driven Character Animation [Placeholder for data-driven contributions]","tags":null,"title":"Chapter 1: Introduction","type":"thesis"},{"authors":null,"categories":null,"content":" Appendix B: Exploring Model-Free RL This appendix provides detailed information about model-free reinforcement learning approaches explored in this thesis.\nB.1 Theoretical Foundations B.1.1 Model-Free vs Model-Based RL  Key distinctions and trade-offs Computational complexity analysis Sample efficiency considerations Generalization capabilities  B.1.2 Core Algorithms Value-Based Methods  Q-Learning fundamentals Deep Q-Networks (DQN) Double DQN and variants Prioritized experience replay  Policy Gradient Methods  REINFORCE algorithm Actor-Critic methods Trust Region Policy Optimization (TRPO) Proximal Policy Optimization (PPO)  B.2 Implementation Details B.2.1 Network Architectures  Convolutional layers for visual input Recurrent components for temporal dependencies Attention mechanisms Architecture search strategies  B.2.2 Training Procedures  Hyperparameter configurations Learning rate schedules Batch size considerations Regularization techniques  B.3 Experimental Setup B.3.1 Environment Specifications  State space representations Action space definitions Reward function designs Episode termination conditions  B.3.2 Evaluation Metrics  Average episode return Sample efficiency measures Convergence analysis Stability indicators  B.4 Algorithm Comparisons B.4.1 Performance Analysis  Learning curves across different algorithms Final performance comparisons Computational resource requirements Training time analysis  B.4.2 Ablation Studies  Impact of different components Sensitivity to hyperparameters Architecture variations Exploration strategies  B.5 Code Examples B.5.1 Basic Q-Learning Implementation # Simplified Q-learning pseudocode def q_learning(env, episodes, alpha, gamma, epsilon): Q = initialize_q_table() for episode in range(episodes): state = env.reset() while not done: action = epsilon_greedy(Q, state, epsilon) next_state, reward, done = env.step(action) Q[state, action] += alpha * (reward + gamma * max(Q[next_state]) - Q[state, action]) state = next_state return Q  B.5.2 PPO Update Step # Simplified PPO update pseudocode def ppo_update(policy, value_function, trajectories, clip_epsilon): for trajectory in trajectories: advantages = compute_advantages(trajectory, value_function) old_log_probs = compute_log_probs(trajectory, policy) for epoch in range(ppo_epochs): new_log_probs = compute_log_probs(trajectory, policy) ratio = exp(new_log_probs - old_log_probs) clipped_ratio = clip(ratio, 1 - clip_epsilon, 1 + clip_epsilon) policy_loss = -min(ratio * advantages, clipped_ratio * advantages) optimize(policy_loss)  ","date":1737504000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1737504000,"objectID":"6da15b22555622272b8139e1ab771943","permalink":"https://vihanga.github.io/thesis/appendices/b/","publishdate":"2025-01-22T00:00:00Z","relpermalink":"/thesis/appendices/b/","section":"thesis","summary":"Appendix B: Exploring Model-Free RL This appendix provides detailed information about model-free reinforcement learning approaches explored in this thesis.\nB.1 Theoretical Foundations B.1.1 Model-Free vs Model-Based RL  Key distinctions and trade-offs Computational complexity analysis Sample efficiency considerations Generalization capabilities  B.1.2 Core Algorithms Value-Based Methods  Q-Learning fundamentals Deep Q-Networks (DQN) Double DQN and variants Prioritized experience replay  Policy Gradient Methods  REINFORCE algorithm Actor-Critic methods Trust Region Policy Optimization (TRPO) Proximal Policy Optimization (PPO)  B.","tags":null,"title":"Appendix B: Exploring Model-Free RL","type":"thesis"},{"authors":null,"categories":null,"content":" 2.1 Creation and Animation of Virtual Characters 2.1.1 Creating Three-Dimensional Graphical Representations [Placeholder for 3D graphical representations]\n2.1.2 Applying Animation Data to 3D Character Models [Placeholder for animation data application]\n2.1.3 Rendering Characters [Placeholder for character rendering]\n2.2 Perception and Application of Virtual Characters 2.2.1 Perception of Virtual Characters [Placeholder for perception of virtual characters]\n2.2.2 Using Virtual Characters in Applications [Placeholder for applications]\n2.3 Procedural Character Animation 2.3.1 Neural network-based Methods [Placeholder for neural network methods]\n2.3.2 Physics-based Reinforcement Learning Approaches [Placeholder for physics-based RL]\n2.3.3 Contemporary Architectural Approaches [Placeholder for contemporary approaches]\n2.4 Chapter Summary [Placeholder for chapter summary]\n","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"9358d9b2e86d81f55a8f6a890e4d78f6","permalink":"https://vihanga.github.io/thesis/chapters/data-driven-animation/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/thesis/chapters/data-driven-animation/","section":"thesis","summary":"2.1 Creation and Animation of Virtual Characters 2.1.1 Creating Three-Dimensional Graphical Representations [Placeholder for 3D graphical representations]\n2.1.2 Applying Animation Data to 3D Character Models [Placeholder for animation data application]\n2.1.3 Rendering Characters [Placeholder for character rendering]\n2.2 Perception and Application of Virtual Characters 2.2.1 Perception of Virtual Characters [Placeholder for perception of virtual characters]\n2.2.2 Using Virtual Characters in Applications [Placeholder for applications]\n2.3 Procedural Character Animation 2.3.1 Neural network-based Methods [Placeholder for neural network methods]","tags":null,"title":"Chapter 2: Data-driven Character Animation","type":"thesis"},{"authors":null,"categories":null,"content":" Appendix C: Supplementary Material Index This appendix provides a comprehensive index of all supplementary materials associated with this thesis, including datasets, code repositories, multimedia content, and additional documentation.\nC.1 Code Repositories C.1.1 Main Research Code  Repository URL: [GitHub/research-code] License: MIT License Languages: Python, C++, MATLAB Key Components:  RL algorithm implementations Data processing pipelines Visualization tools Experiment runners   C.1.2 Evaluation Framework  Repository URL: [GitHub/evaluation-framework] Documentation: Available in /docs directory Dependencies: Listed in requirements.txt Installation Guide: See README.md  C.2 Datasets C.2.1 Motion Capture Data  Format: BVH, FBX, and custom JSON Size: ~12GB compressed Access: Available upon request Contents:  500+ motion sequences Multiple actor performances Various activity types Annotation metadata   C.2.2 Training Data  Format: HDF5 files Organization: By experiment type Preprocessing: Scripts included Statistics:  Training samples: 1M+ Validation samples: 200K Test samples: 100K   C.3 Multimedia Content C.3.1 Video Demonstrations  Location: [Project website/videos] Format: MP4 (H.264) Contents:  Algorithm comparisons Real-time demonstrations User study recordings System walkthroughs   C.3.2 Interactive Demos  Platform: WebGL builds Requirements: Modern web browser Features:  Real-time parameter adjustment Multiple scenario selection Performance visualization   C.4 Additional Documentation C.4.1 Extended Results  Filename: extended_results.pdf Pages: 150+ Contents:  Full experimental data Additional ablation studies Statistical analyses Error analysis   C.4.2 Implementation Notes  Filename: implementation_guide.pdf Topics Covered:  System architecture Algorithm optimizations Platform-specific considerations Troubleshooting guide   C.5 External Resources C.5.1 Third-Party Libraries  TensorFlow: v2.x PyTorch: v1.x OpenAI Gym: v0.x Unity ML-Agents: v2.x  C.5.2 Reference Implementations  Links to baseline implementations Comparison benchmarks Integration examples  C.6 Access Information C.6.1 Public Resources All public resources can be accessed at: - Project website: [URL] - DOI: [10.xxxx/xxxxx]\nC.6.2 Restricted Materials For access to restricted materials: - Contact: [email] - Affiliation requirements - Usage agreements\nC.7 Version Control C.7.1 Release History  v1.0: Initial release v1.1: Bug fixes and documentation updates v1.2: Additional experiments added v2.0: Major algorithm improvements  C.7.2 Update Notifications Subscribe to updates at: [project-updates-list]\n","date":1737504000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1737504000,"objectID":"1eaec400db514e2cedf9d7c41a8f51ed","permalink":"https://vihanga.github.io/thesis/appendices/c5a/","publishdate":"2025-01-22T00:00:00Z","relpermalink":"/thesis/appendices/c5a/","section":"thesis","summary":"Appendix C: Supplementary Material Index This appendix provides a comprehensive index of all supplementary materials associated with this thesis, including datasets, code repositories, multimedia content, and additional documentation.\nC.1 Code Repositories C.1.1 Main Research Code  Repository URL: [GitHub/research-code] License: MIT License Languages: Python, C++, MATLAB Key Components:  RL algorithm implementations Data processing pipelines Visualization tools Experiment runners   C.1.2 Evaluation Framework  Repository URL: [GitHub/evaluation-framework] Documentation: Available in /docs directory Dependencies: Listed in requirements.","tags":null,"title":"Appendix C: Supplementary Material Index","type":"thesis"},{"authors":null,"categories":null,"content":" 3.1 Introduction to Reinforcement Learning 3.1.1 Markov Decision Processes [Placeholder for MDP content]\n3.1.2 Model-free vs model-based Reinforcement Learning [Placeholder for model-free vs model-based comparison]\n3.1.3 Reward Functions [Placeholder for reward functions]\n3.2 Model-based Reinforcement Learning 3.2.1 Latent Dynamics Models [Placeholder for latent dynamics models]\n3.2.2 Perspectives on Model-based Reinforcement Learning [Placeholder for perspectives]\n3.3 Representations and Adversaries 3.3.1 Representation Learning [Placeholder for representation learning]\n3.3.2 Adversarial Learning for Regularisation [Placeholder for adversarial learning]\n3.4 Discussion [Placeholder for discussion]\n3.5 Chapter Summary [Placeholder for chapter summary]\n","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"752bb61cd070490e6fd7dd8b8fad4585","permalink":"https://vihanga.github.io/thesis/chapters/model-based-rl/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/thesis/chapters/model-based-rl/","section":"thesis","summary":"3.1 Introduction to Reinforcement Learning 3.1.1 Markov Decision Processes [Placeholder for MDP content]\n3.1.2 Model-free vs model-based Reinforcement Learning [Placeholder for model-free vs model-based comparison]\n3.1.3 Reward Functions [Placeholder for reward functions]\n3.2 Model-based Reinforcement Learning 3.2.1 Latent Dynamics Models [Placeholder for latent dynamics models]\n3.2.2 Perspectives on Model-based Reinforcement Learning [Placeholder for perspectives]\n3.3 Representations and Adversaries 3.3.1 Representation Learning [Placeholder for representation learning]\n3.3.2 Adversarial Learning for Regularisation [Placeholder for adversarial learning]","tags":null,"title":"Chapter 3: Model-based Reinforcement Learning","type":"thesis"},{"authors":null,"categories":null,"content":" Appendix D: Perceptual Evaluation This appendix presents detailed information about the perceptual evaluation studies conducted to assess the quality and realism of generated animations and interactive systems.\nD.1 Study Design D.1.1 Methodology Overview  Study Type: Mixed-methods approach Duration: 6 weeks Participants: N=120 Sessions: 3 per participant IRB Approval: #2024-XXX  D.1.2 Participant Demographics  Age Range: 18-65 years Gender Distribution: 48% female, 52% male Experience Levels:  Novice users: 40% Intermediate: 35% Expert: 25%  Background:  Computer graphics professionals: 20% Gamers: 30% General users: 50%   D.2 Evaluation Metrics D.2.1 Quantitative Measures Visual Realism  Likert Scale: 1-7 rating system Comparison Rankings: Forced choice paradigm Response Times: Measured in milliseconds Eye Tracking Data: Fixation patterns and saccades  Motion Quality  Naturalness Ratings: Continuous scale 0-100 Jerkiness Detection: Binary classification Timing Accuracy: Deviation from reference Smoothness Metrics: Objective calculations  D.2.2 Qualitative Measures  Open-ended Feedback: Thematic analysis Interview Responses: Semi-structured format Think-aloud Protocols: During interaction Post-study Questionnaires: Comprehensive feedback  D.3 Experimental Conditions D.3.1 Stimulus Presentation Display Setup  Monitor: 27\u0026rdquo; 4K display (3840x2160) Refresh Rate: 144Hz Viewing Distance: 60cm Lighting: Controlled ambient conditions  Stimulus Types  Static Comparisons: Side-by-side presentations Dynamic Sequences: 10-30 second clips Interactive Scenarios: Real-time manipulation A/B Testing: Randomized presentation order  D.3.2 Control Conditions  Ground Truth: Motion capture reference Baseline Methods: State-of-the-art comparisons Ablation Variants: Component-wise evaluation Random Conditions: Sanity checks  D.4 Results Analysis D.4.1 Statistical Methods  ANOVA: Between-subjects effects Post-hoc Tests: Bonferroni corrections Effect Sizes: Cohen\u0026rsquo;s d calculations Inter-rater Reliability: Krippendorff\u0026rsquo;s alpha  D.4.2 Key Findings Realism Ratings  Our method: M=5.8, SD=0.9 Baseline A: M=4.2, SD=1.2 Baseline B: M=4.6, SD=1.1 Ground truth: M=6.5, SD=0.6  User Preferences  72% preferred our method over baselines 85% rated as \u0026ldquo;realistic\u0026rdquo; or \u0026ldquo;very realistic\u0026rdquo; 91% found interactions intuitive 78% would use in production  D.5 Detailed Results Tables D.5.1 Condition Comparisons    Method Realism Smoothness Preference Response Time     Ours 5.8±0.9 6.1±0.7 72% 1.2s   Baseline A 4.2±1.2 4.8±1.1 12% 1.8s   Baseline B 4.6±1.1 5.2±0.9 16% 1.5s    D.5.2 Task-Specific Performance    Task Type Success Rate Completion Time Satisfaction     Navigation 94% 12.3s 5.9\u0026frasl;7   Manipulation 88% 18.7s 5.6\u0026frasl;7   Creation 82% 45.2s 6.2\u0026frasl;7    D.6 User Feedback Themes D.6.1 Positive Aspects  Natural Motion: \u0026ldquo;Movements felt very lifelike\u0026rdquo; Responsive Control: \u0026ldquo;System reacted instantly\u0026rdquo; Visual Quality: \u0026ldquo;Graphics were impressive\u0026rdquo; Ease of Use: \u0026ldquo;Intuitive interface\u0026rdquo;  D.6.2 Areas for Improvement  Edge Cases: \u0026ldquo;Some extreme poses looked odd\u0026rdquo; Learning Curve: \u0026ldquo;Advanced features need tutorials\u0026rdquo; Performance: \u0026ldquo;Occasional lag with complex scenes\u0026rdquo; Customization: \u0026ldquo;Want more control options\u0026rdquo;  D.7 Study Materials D.7.1 Questionnaires  Pre-study survey Post-task evaluations Final assessment form NASA-TLX workload assessment  D.7.2 Instructions  Participant information sheet Task descriptions Training materials Debriefing script  ","date":1737504000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1737504000,"objectID":"57686c6187a1d60d74921db76e75ca0c","permalink":"https://vihanga.github.io/thesis/appendices/d/","publishdate":"2025-01-22T00:00:00Z","relpermalink":"/thesis/appendices/d/","section":"thesis","summary":"Appendix D: Perceptual Evaluation This appendix presents detailed information about the perceptual evaluation studies conducted to assess the quality and realism of generated animations and interactive systems.\nD.1 Study Design D.1.1 Methodology Overview  Study Type: Mixed-methods approach Duration: 6 weeks Participants: N=120 Sessions: 3 per participant IRB Approval: #2024-XXX  D.1.2 Participant Demographics  Age Range: 18-65 years Gender Distribution: 48% female, 52% male Experience Levels:  Novice users: 40% Intermediate: 35% Expert: 25%  Background:  Computer graphics professionals: 20% Gamers: 30% General users: 50%   D.","tags":null,"title":"Appendix D: Perceptual Evaluation","type":"thesis"},{"authors":null,"categories":null,"content":" Chapter 4: Model-based and Model-free Animation This chapter explores both model-free and model-based approaches to animation control, presenting methods for creating intelligent animation agents that can learn from experience and plan ahead.\n4.1 Model-free RL for Animation Control This section introduces model-free reinforcement learning techniques for animation control, where agents learn policies directly from interaction without explicitly modeling the environment dynamics.\n4.1.1 Problem Formulation [Content placeholder: Define the animation control problem as a Markov Decision Process (MDP), including state and action spaces, reward functions, and learning objectives]\n4.1.2 Policy Learning Methods [Content placeholder: Discuss various model-free RL algorithms applicable to animation control, including policy gradient methods, actor-critic architectures, and their specific adaptations for character animation]\n4.1.3 Experimental Results [Content placeholder: Present experimental results demonstrating the effectiveness of model-free RL for various animation tasks, including locomotion, object manipulation, and athletic movements]\n4.2 Learned dynamics models and online planning for model-based animation agents This section presents model-based approaches where agents learn dynamics models of the environment and use them for planning and control.\n4.2.1 Dynamics Model Learning [Content placeholder: Describe methods for learning forward dynamics models from interaction data, including neural network architectures and training procedures]\n4.2.2 Online Planning Algorithms [Content placeholder: Present online planning algorithms that leverage learned dynamics models, including model predictive control (MPC) and sampling-based planning methods]\n4.2.3 Integration with Model-free Methods [Content placeholder: Discuss hybrid approaches that combine model-based planning with model-free learning, leveraging the strengths of both paradigms]\n4.2.4 Comparative Analysis [Content placeholder: Compare model-based and model-free approaches in terms of sample efficiency, computational requirements, and animation quality]\n4.3 Chapter Summary This chapter has presented a comprehensive exploration of both model-free and model-based approaches to animation control. We demonstrated that model-free methods can produce high-quality animations through direct policy learning, while model-based methods offer improved sample efficiency and planning capabilities. The combination of both approaches provides a powerful framework for creating intelligent animation agents capable of complex behaviors.\nKey contributions of this chapter include: - A systematic comparison of model-free and model-based approaches for animation control - Novel algorithms for learning dynamics models suitable for character animation - Demonstration of online planning methods that produce natural-looking motions - Insights into the trade-offs between different approaches and their appropriate use cases\nThe methods presented in this chapter lay the foundation for more advanced animation systems that can adapt to new tasks, generalize across different characters, and produce increasingly sophisticated behaviors.\n","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"d9348a507e32dce7a3031f613bbe5fa6","permalink":"https://vihanga.github.io/thesis/chapters/model-based-animation/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/thesis/chapters/model-based-animation/","section":"thesis","summary":"Chapter 4: Model-based and Model-free Animation This chapter explores both model-free and model-based approaches to animation control, presenting methods for creating intelligent animation agents that can learn from experience and plan ahead.\n4.1 Model-free RL for Animation Control This section introduces model-free reinforcement learning techniques for animation control, where agents learn policies directly from interaction without explicitly modeling the environment dynamics.\n4.1.1 Problem Formulation [Content placeholder: Define the animation control problem as a Markov Decision Process (MDP), including state and action spaces, reward functions, and learning objectives]","tags":null,"title":"Chapter 4: Model-based and Model-free Animation","type":"thesis"},{"authors":null,"categories":null,"content":" Appendix E: Supplementary Results, Model-free RL Experiments This appendix contains comprehensive supplementary results from the model-free reinforcement learning experiments conducted throughout this research.\nE.1 Extended Learning Curves E.1.1 Training Performance Over Time PPO Experiments  Environment: Custom physics simulation Training Steps: 10M Seeds: 5 independent runs Logging Frequency: Every 1000 steps  ![Learning Curves - PPO] - Episode Return: Steady improvement from -200 to +150 - Success Rate: 15% → 92% over training - Sample Efficiency: 2.3M samples to convergence - Variance: Decreasing with training progress\nSAC Experiments  Environment: Same as PPO Training Steps: 10M Seeds: 5 independent runs Key Differences:  Faster initial learning More stable convergence Higher final performance (+165 avg return)   E.1.2 Hyperparameter Sensitivity    Hyperparameter Range Tested Optimal Value Impact on Performance     Learning Rate 1e-5 to 1e-2 3e-4 Critical - 40% variance   Batch Size 32 to 512 256 Moderate - 15% variance   Discount Factor 0.9 to 0.999 0.99 Low - 8% variance   Entropy Coefficient 0.0 to 0.1 0.01 Moderate - 20% variance    E.2 Ablation Study Results E.2.1 Architecture Components Network Depth Impact Shallow (2 layers): 72% success rate Medium (4 layers): 89% success rate Deep (8 layers): 85% success rate Very Deep (16 layers): 78% success rate  Activation Functions  ReLU: Baseline performance Tanh: -5% performance GELU: +3% performance Swish: +2% performance  E.2.2 Training Techniques    Technique Enabled Disabled Difference     Normalization 92% 76% +16%   Dropout 88% 92% -4%   Weight Decay 90% 87% +3%   Gradient Clipping 92% 84% +8%    E.3 Detailed Experimental Configurations E.3.1 Environment Specifications env_config = { 'observation_space': Box(low=-inf, high=inf, shape=(128,)), 'action_space': Box(low=-1, high=1, shape=(8,)), 'max_episode_steps': 1000, 'reward_scale': 0.1, 'physics_timestep': 0.01, 'render_fps': 30 }  E.3.2 Algorithm Configurations PPO Configuration ppo_config = { 'n_steps': 2048, 'batch_size': 64, 'n_epochs': 10, 'gamma': 0.99, 'gae_lambda': 0.95, 'clip_range': 0.2, 'learning_rate': 3e-4, 'value_coefficient': 0.5, 'entropy_coefficient': 0.01, 'max_grad_norm': 0.5 }  SAC Configuration sac_config = { 'buffer_size': 1e6, 'batch_size': 256, 'gamma': 0.99, 'tau': 0.005, 'learning_rate': 3e-4, 'alpha': 0.2, 'target_update_interval': 1, 'gradient_steps': 1, 'reward_scale': 5.0 }  E.4 Task-Specific Results E.4.1 Locomotion Tasks    Task PPO Score SAC Score TD3 Score Human Baseline     Walk 0.92 0.94 0.91 0.98   Run 0.88 0.91 0.89 0.97   Jump 0.85 0.82 0.86 0.95   Turn 0.94 0.93 0.92 0.99    E.4.2 Manipulation Tasks    Task Success Rate Avg Time (s) Precision Score     Reach 96% 2.3 0.94   Grasp 89% 3.7 0.87   Place 84% 5.2 0.82   Stack 76% 8.4 0.78    E.5 Computational Performance E.5.1 Training Efficiency  PPO: 15.2 hours on single GPU SAC: 18.6 hours on single GPU\n TD3: 16.9 hours on single GPU A2C: 12.1 hours on single GPU  E.5.2 Inference Speed    Algorithm FPS (CPU) FPS (GPU) Latency (ms)     PPO 850 3200 1.2   SAC 720 2900 1.4   TD3 780 3100 1.3   A2C 920 3400 1.1    E.6 Failure Case Analysis E.6.1 Common Failure Modes  Catastrophic Forgetting: Occurred in 8% of runs Local Minima: Trapped in suboptimal policies (12%) Exploration Collapse: Premature convergence (6%) Reward Hacking: Exploiting reward function (4%)  E.6.2 Mitigation Strategies  Periodic evaluation checkpoints Adaptive exploration schedules Reward function shaping Ensemble methods for robustness  E.7 Statistical Significance Tests E.7.1 Performance Comparisons  PPO vs SAC: p = 0.032 (significant) PPO vs TD3: p = 0.186 (not significant) SAC vs TD3: p = 0.041 (significant) All vs Random: p \u0026lt; 0.001 (highly significant)  E.7.2 Confidence Intervals All results reported with 95% confidence intervals calculated using bootstrap resampling (n=1000).\n","date":1737504000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1737504000,"objectID":"b9c360ef5d46fbccbcb412c8afadbc30","permalink":"https://vihanga.github.io/thesis/appendices/e/","publishdate":"2025-01-22T00:00:00Z","relpermalink":"/thesis/appendices/e/","section":"thesis","summary":"Appendix E: Supplementary Results, Model-free RL Experiments This appendix contains comprehensive supplementary results from the model-free reinforcement learning experiments conducted throughout this research.\nE.1 Extended Learning Curves E.1.1 Training Performance Over Time PPO Experiments  Environment: Custom physics simulation Training Steps: 10M Seeds: 5 independent runs Logging Frequency: Every 1000 steps  ![Learning Curves - PPO] - Episode Return: Steady improvement from -200 to +150 - Success Rate: 15% → 92% over training - Sample Efficiency: 2.","tags":null,"title":"Appendix E: Supplementary Results, Model-free RL Experiments","type":"thesis"},{"authors":null,"categories":null,"content":" Chapter 5: Data-Driven Reinforcement Learning with Off-Policy Data This chapter addresses the challenge of leveraging existing animation data within reinforcement learning frameworks, presenting methods for combining data-driven approaches with RL to create more efficient and higher-quality animation systems.\n5.1 Introduction [Content placeholder: Introduce the motivation for combining data-driven methods with RL, discussing the abundance of motion capture data and the limitations of pure RL approaches in animation]\n5.2 Off-Policy Learning from Motion Data This section presents methods for learning from pre-recorded motion data within an RL framework, enabling agents to benefit from expert demonstrations while maintaining the flexibility to explore and improve.\n5.2.1 Problem Formulation [Content placeholder: Define the off-policy learning problem in the context of character animation, including notation and key challenges]\n5.2.2 Data Collection and Processing [Content placeholder: Describe methods for collecting and processing motion capture data for use in off-policy RL, including data augmentation and quality assessment]\n5.2.3 Off-Policy Algorithms [Content placeholder: Present specific off-policy algorithms adapted for animation, including importance sampling corrections and replay buffer management]\n5.3 Combining Imitation and Reinforcement Learning This section explores hybrid approaches that combine imitation learning from motion data with reinforcement learning for task achievement.\n5.3.1 Behavioral Cloning with RL Fine-tuning [Content placeholder: Describe methods that initialize policies through behavioral cloning and then refine them using RL]\n5.3.2 Reward Shaping with Motion Priors [Content placeholder: Present approaches for incorporating motion data as priors in reward functions to guide RL exploration]\n5.3.3 Adversarial Motion Learning [Content placeholder: Discuss adversarial methods that learn to distinguish between generated and reference motions, encouraging natural movement patterns]\n5.4 Experimental Evaluation 5.4.1 Benchmark Tasks [Content placeholder: Define a set of benchmark tasks for evaluating data-driven RL methods in animation]\n5.4.2 Quantitative Results [Content placeholder: Present quantitative comparisons of different approaches, including learning curves, final performance, and motion quality metrics]\n5.4.3 Qualitative Analysis [Content placeholder: Provide qualitative analysis of generated animations, including visual comparisons and expert evaluations]\n5.5 Applications 5.5.1 Style Transfer and Motion Editing [Content placeholder: Demonstrate applications to style transfer between different motion styles and interactive motion editing]\n5.5.2 Multi-skill Learning [Content placeholder: Show how data-driven RL can enable learning of multiple skills from diverse motion datasets]\n5.5.3 Adaptive Character Control [Content placeholder: Present applications to adaptive character control that can handle varying environments and tasks]\n5.6 Chapter Summary This chapter has presented methods for effectively combining data-driven approaches with reinforcement learning for character animation. By leveraging existing motion data within an RL framework, we can create animation systems that benefit from both the quality of recorded motions and the adaptability of learned policies.\nKey contributions include: - Novel algorithms for off-policy learning from motion capture data - Hybrid approaches that balance imitation and task-oriented learning - Demonstration of improved sample efficiency and motion quality - Applications to various animation problems including style transfer and multi-skill learning\nThe methods developed in this chapter enable more practical deployment of RL-based animation systems by reducing the need for extensive online exploration while maintaining the flexibility to adapt to new tasks and environments.\n","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"4caa4bfc750979cddab76cb7e35516cf","permalink":"https://vihanga.github.io/thesis/chapters/data-driven-rl/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/thesis/chapters/data-driven-rl/","section":"thesis","summary":"Chapter 5: Data-Driven Reinforcement Learning with Off-Policy Data This chapter addresses the challenge of leveraging existing animation data within reinforcement learning frameworks, presenting methods for combining data-driven approaches with RL to create more efficient and higher-quality animation systems.\n5.1 Introduction [Content placeholder: Introduce the motivation for combining data-driven methods with RL, discussing the abundance of motion capture data and the limitations of pure RL approaches in animation]\n5.2 Off-Policy Learning from Motion Data This section presents methods for learning from pre-recorded motion data within an RL framework, enabling agents to benefit from expert demonstrations while maintaining the flexibility to explore and improve.","tags":null,"title":"Chapter 5: Data-Driven Reinforcement Learning with Off-Policy Data","type":"thesis"},{"authors":null,"categories":null,"content":" Appendix F: Star Jump Ideal Action Calculation This appendix provides detailed mathematical derivations and calculations for determining ideal actions in the star jump motion synthesis task.\nF.1 Problem Formulation F.1.1 Star Jump Motion Definition A star jump consists of the following phases: 1. Preparation Phase: Initial crouching position 2. Launch Phase: Explosive upward movement 3. Aerial Phase: Body forms star shape in mid-air 4. Landing Phase: Controlled descent and stabilization\nF.1.2 State Space Definition The state vector s ∈ ℝⁿ consists of:\ns = [q, q̇, h, φ, τ]  Where: - q ∈ ℝ²¹: Joint angles (7 joints × 3 DOF) - q̇ ∈ ℝ²¹: Joint velocities - h ∈ ℝ: Height of center of mass - φ ∈ ℝ³: Body orientation (Euler angles) - τ ∈ [0,1]: Phase timing parameter\nF.2 Ideal Action Derivation F.2.1 Kinematic Constraints The ideal star jump configuration at peak height satisfies:\nq*_shoulder = [π/2, 0, π/4] # Arms raised laterally q*_hip = [π/4, 0, π/6] # Legs spread wide q*_spine = [0, 0, -π/12] # Slight back arch  F.2.2 Dynamic Optimization The optimal action sequence a(t) minimizes:\nJ = ∫[αE(t) + βD(q,q*) + γS(q̇) + δB(τ)]dt  Where: - E(t): Energy expenditure term - D(q,q*): Deviation from ideal pose - S(q̇): Smoothness penalty - B(τ): Balance maintenance term\nWeights: α=0.1, β=1.0, γ=0.3, δ=0.5\nF.3 Phase-Specific Calculations F.3.1 Preparation Phase (τ ∈ [0, 0.2]) Joint Targets def preparation_targets(τ): crouch_factor = smooth_interpolate(0, 1, τ/0.2) q_target = { 'hip': [-π/3 * crouch_factor, 0, 0], 'knee': [π/2 * crouch_factor, 0, 0], 'ankle': [-π/6 * crouch_factor, 0, 0], 'shoulder': [π/6 * crouch_factor, 0, 0], 'elbow': [-π/4 * crouch_factor, 0, 0] } return q_target  Force Requirements Vertical ground reaction force:\nF_z = m*g + m*a_prep a_prep = -2.5 m/s² (downward acceleration)  F.3.2 Launch Phase (τ ∈ [0.2, 0.35]) Optimal Launch Velocity Using projectile motion equations:\nv₀ = √(2gh_target) h_target = 0.5m (typical jump height) v₀ ≈ 3.13 m/s  Joint Torques Maximum torques during launch:\nτ_max = { 'hip': 250 Nm, 'knee': 180 Nm, 'ankle': 120 Nm, 'shoulder': 60 Nm }  F.3.3 Aerial Phase (τ ∈ [0.35, 0.65]) Star Formation Trajectory def aerial_trajectory(t_flight): # Normalized time in aerial phase t_norm = (t_flight - 0.35) / 0.3 # Smooth transition to star pose spread_factor = sin(π * t_norm) q_aerial = { 'shoulder_abduction': π/2 * spread_factor, 'shoulder_flexion': π/6 * spread_factor, 'hip_abduction': π/3 * spread_factor, 'hip_flexion': π/6 * spread_factor, 'spine_extension': -π/12 * spread_factor } return q_aerial  Angular Momentum Conservation Total angular momentum L = 0 (no rotation):\nL = Σᵢ(rᵢ × mᵢvᵢ) + Iω = 0  F.3.4 Landing Phase (τ ∈ [0.65, 1.0]) Impact Absorption Strategy Joint stiffness schedule:\ndef landing_stiffness(τ_land): # Progressive stiffening k_base = 500 # N·m/rad k_factor = 1 + 2 * (τ_land - 0.65) / 0.35 k_joints = { 'ankle': k_base * k_factor * 1.2, 'knee': k_base * k_factor * 1.0, 'hip': k_base * k_factor * 0.8 } return k_joints  F.4 Control Law Implementation F.4.1 PD Controller with Feedforward def ideal_action(s, τ): # Extract current state q_current, q̇_current = s.joints, s.velocities # Get phase-appropriate targets q_target = get_phase_targets(τ) q̇_target = get_phase_velocities(τ) # PD control law K_p = get_phase_gains_p(τ) K_d = get_phase_gains_d(τ) # Feedforward term τ_ff = get_feedforward_torques(τ) # Combined control τ_control = K_p @ (q_target - q_current) + \\ K_d @ (q̇_target - q̇_current) + \\ τ_ff return clip_torques(τ_control, τ_max)  F.4.2 Phase Transition Conditions def check_phase_transition(s, τ): if τ \u0026lt; 0.2: # Preparation return s.com_velocity[2] \u0026lt; -0.1 elif τ \u0026lt; 0.35: # Launch return s.ground_contact and s.com_velocity[2] \u0026gt; 2.0 elif τ \u0026lt; 0.65: # Aerial return not s.ground_contact else: # Landing return s.ground_contact and abs(s.com_velocity[2]) \u0026lt; 0.1  F.5 Validation Results F.5.1 Simulated Performance    Metric Ideal Achieved Error     Jump Height 0.50m 0.48m 4%   Flight Time 0.64s 0.62s 3%   Star Spread 1.8m 1.75m 2.8%   Landing Stability \u0026lt;5° 3.2° ✓    F.5.2 Energy Efficiency Total energy expenditure:\nE_total = E_kinetic + E_potential + E_internal = 245J + 196J + 87J = 528J  Efficiency compared to simplified model: 78%\nF.6 Implementation Notes F.6.1 Numerical Stability  Use quaternions for orientation to avoid gimbal lock Apply low-pass filtering to joint velocities Implement safety limits on all joint angles  F.6.2 Real-time Considerations  Pre-compute phase trajectories Use lookup tables for trigonometric functions Parallelize joint calculations when possible  F.6.3 Robustness Enhancements  Add compliance terms for unexpected disturbances Implement recovery strategies for off-nominal conditions Include proprioceptive feedback integration  ","date":1737504000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1737504000,"objectID":"ab40f5ed7a3ee4e00cd981489ccb50db","permalink":"https://vihanga.github.io/thesis/appendices/f/","publishdate":"2025-01-22T00:00:00Z","relpermalink":"/thesis/appendices/f/","section":"thesis","summary":"Appendix F: Star Jump Ideal Action Calculation This appendix provides detailed mathematical derivations and calculations for determining ideal actions in the star jump motion synthesis task.\nF.1 Problem Formulation F.1.1 Star Jump Motion Definition A star jump consists of the following phases: 1. Preparation Phase: Initial crouching position 2. Launch Phase: Explosive upward movement 3. Aerial Phase: Body forms star shape in mid-air 4. Landing Phase: Controlled descent and stabilization","tags":null,"title":"Appendix F: Star Jump Ideal Action Calculation","type":"thesis"},{"authors":null,"categories":null,"content":" Chapter 6: Latent Dynamics Models for Character Animation This chapter presents methods for learning compact latent representations of character dynamics, enabling more efficient learning and control of complex animation behaviors through dimensionality reduction and structured latent spaces.\n6.1 Introduction [Content placeholder: Introduce the concept of latent dynamics models and their benefits for character animation, including improved generalization and computational efficiency]\n6.2 Learning Latent Representations This section describes methods for learning meaningful latent representations of character states and dynamics.\n6.2.1 Variational Autoencoders for Motion [Content placeholder: Present VAE-based approaches for learning latent representations of character poses and motions]\n6.2.2 Dynamics in Latent Space [Content placeholder: Describe methods for learning dynamics models that operate in the learned latent space rather than the full state space]\n6.2.3 Structured Latent Spaces [Content placeholder: Discuss techniques for imposing structure on latent spaces to improve interpretability and control]\n6.3 Control in Latent Space This section presents methods for character control that operate in the learned latent representations.\n6.3.1 Latent Policy Learning [Content placeholder: Describe reinforcement learning algorithms that learn policies in latent space]\n6.3.2 Hierarchical Control Architectures [Content placeholder: Present hierarchical control methods that use latent representations at different levels of abstraction]\n6.3.3 Transfer and Generalization [Content placeholder: Discuss how latent representations enable better transfer learning and generalization across different characters and tasks]\n6.4 Applications to Complex Behaviors 6.4.1 Multi-character Coordination [Content placeholder: Demonstrate applications to coordinating multiple characters using shared latent representations]\n6.4.2 Long-horizon Planning [Content placeholder: Show how latent dynamics models enable efficient long-horizon planning for complex animation sequences]\n6.4.3 Interactive Character Control [Content placeholder: Present applications to real-time interactive control using latent space representations]\n6.5 Experimental Results 6.5.1 Representation Quality [Content placeholder: Evaluate the quality of learned latent representations through reconstruction accuracy and semantic meaningfulness]\n6.5.2 Control Performance [Content placeholder: Compare control performance using latent dynamics models versus full-state approaches]\n6.5.3 Computational Efficiency [Content placeholder: Analyze the computational benefits of operating in latent space for both learning and inference]\n6.6 Chapter Summary This chapter has presented a comprehensive framework for learning and utilizing latent dynamics models in character animation. By operating in learned latent spaces, we can achieve more efficient learning, better generalization, and more intuitive control of complex character behaviors.\nKey contributions of this chapter include: - Novel architectures for learning structured latent representations of character dynamics - Efficient control algorithms that operate in latent space - Demonstration of improved sample efficiency and generalization - Applications to challenging animation problems including multi-character coordination and long-horizon planning\nThe latent dynamics approach developed in this chapter provides a foundation for scaling character animation systems to more complex behaviors and environments while maintaining computational tractability and control quality.\n","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"b010e1faa4d04184493b3d5814db0f5e","permalink":"https://vihanga.github.io/thesis/chapters/latent-dynamics/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/thesis/chapters/latent-dynamics/","section":"thesis","summary":"Chapter 6: Latent Dynamics Models for Character Animation This chapter presents methods for learning compact latent representations of character dynamics, enabling more efficient learning and control of complex animation behaviors through dimensionality reduction and structured latent spaces.\n6.1 Introduction [Content placeholder: Introduce the concept of latent dynamics models and their benefits for character animation, including improved generalization and computational efficiency]\n6.2 Learning Latent Representations This section describes methods for learning meaningful latent representations of character states and dynamics.","tags":null,"title":"Chapter 6: Latent Dynamics Models for Character Animation","type":"thesis"},{"authors":null,"categories":null,"content":" Appendix G: RLAnimate Directory This appendix provides a comprehensive directory of the RLAnimate framework, including its architecture, components, API reference, and usage examples.\nG.1 System Architecture G.1.1 Overview RLAnimate is a modular framework for physics-based character animation using reinforcement learning. The system consists of:\nRLAnimate/ ├── core/ │ ├── agents/ │ ├── environments/ │ ├── physics/ │ └── utils/ ├── models/ │ ├── policies/ │ ├── value_functions/ │ └── networks/ ├── training/ │ ├── algorithms/ │ ├── replay_buffers/ │ └── schedulers/ ├── evaluation/ │ ├── metrics/ │ ├── visualization/ │ └── benchmarks/ └── examples/ ├── tutorials/ ├── experiments/ └── demos/  G.1.2 Core Components Physics Engine Interface class PhysicsInterface: \u0026quot;\u0026quot;\u0026quot;Abstract interface for physics simulation backends\u0026quot;\u0026quot;\u0026quot; def step(self, actions: np.ndarray) -\u0026gt; Tuple[State, float, bool, dict]: \u0026quot;\u0026quot;\u0026quot;Advance simulation by one timestep\u0026quot;\u0026quot;\u0026quot; def reset(self) -\u0026gt; State: \u0026quot;\u0026quot;\u0026quot;Reset simulation to initial state\u0026quot;\u0026quot;\u0026quot; def get_state(self) -\u0026gt; State: \u0026quot;\u0026quot;\u0026quot;Get current simulation state\u0026quot;\u0026quot;\u0026quot; def set_state(self, state: State) -\u0026gt; None: \u0026quot;\u0026quot;\u0026quot;Set simulation state\u0026quot;\u0026quot;\u0026quot;  Character Model class Character: \u0026quot;\u0026quot;\u0026quot;Articulated character representation\u0026quot;\u0026quot;\u0026quot; properties = { 'num_joints': 21, 'num_dof': 63, 'mass': 70.0, # kg 'height': 1.75 # m } joints = [ 'pelvis', 'spine', 'chest', 'neck', 'head', 'left_shoulder', 'left_elbow', 'left_wrist', 'right_shoulder', 'right_elbow', 'right_wrist', 'left_hip', 'left_knee', 'left_ankle', 'right_hip', 'right_knee', 'right_ankle' ]  G.2 API Reference G.2.1 Environment API class RLAnimateEnv(gym.Env): \u0026quot;\u0026quot;\u0026quot;Base environment for character animation tasks\u0026quot;\u0026quot;\u0026quot; def __init__(self, config: Dict[str, Any]): \u0026quot;\u0026quot;\u0026quot; Args: config: Environment configuration dictionary \u0026quot;\u0026quot;\u0026quot; def step(self, action: np.ndarray) -\u0026gt; Tuple[np.ndarray, float, bool, dict]: \u0026quot;\u0026quot;\u0026quot; Execute action and return step information Args: action: Joint torques or target positions Returns: observation: Current state observation reward: Step reward done: Episode termination flag info: Additional information \u0026quot;\u0026quot;\u0026quot; def reset(self) -\u0026gt; np.ndarray: \u0026quot;\u0026quot;\u0026quot;Reset environment to initial state\u0026quot;\u0026quot;\u0026quot; def render(self, mode: str = 'human') -\u0026gt; Optional[np.ndarray]: \u0026quot;\u0026quot;\u0026quot;Render current state\u0026quot;\u0026quot;\u0026quot;  G.2.2 Policy API class Policy(nn.Module): \u0026quot;\u0026quot;\u0026quot;Base policy network class\u0026quot;\u0026quot;\u0026quot; def forward(self, obs: torch.Tensor) -\u0026gt; Distribution: \u0026quot;\u0026quot;\u0026quot; Compute action distribution Args: obs: Observation tensor Returns: Action distribution (Normal or Categorical) \u0026quot;\u0026quot;\u0026quot; def get_action(self, obs: torch.Tensor, deterministic: bool = False) -\u0026gt; torch.Tensor: \u0026quot;\u0026quot;\u0026quot;Sample action from policy\u0026quot;\u0026quot;\u0026quot; def evaluate_actions(self, obs: torch.Tensor, actions: torch.Tensor) -\u0026gt; Tuple[torch.Tensor, torch.Tensor]: \u0026quot;\u0026quot;\u0026quot;Evaluate log probability and entropy of actions\u0026quot;\u0026quot;\u0026quot;  G.2.3 Training API class Trainer: \u0026quot;\u0026quot;\u0026quot;Main training orchestrator\u0026quot;\u0026quot;\u0026quot; def __init__(self, env: RLAnimateEnv, policy: Policy, algorithm: str = 'ppo', config: Dict[str, Any] = None): \u0026quot;\u0026quot;\u0026quot;Initialize trainer with environment and policy\u0026quot;\u0026quot;\u0026quot; def train(self, total_timesteps: int, callback: Optional[Callable] = None) -\u0026gt; Policy: \u0026quot;\u0026quot;\u0026quot; Train policy for specified timesteps Returns: Trained policy \u0026quot;\u0026quot;\u0026quot; def save(self, path: str) -\u0026gt; None: \u0026quot;\u0026quot;\u0026quot;Save trained model\u0026quot;\u0026quot;\u0026quot; def load(self, path: str) -\u0026gt; None: \u0026quot;\u0026quot;\u0026quot;Load trained model\u0026quot;\u0026quot;\u0026quot;  G.3 Configuration System G.3.1 Environment Configuration # config/env/locomotion.yaml env: name: \u0026quot;Locomotion-v1\u0026quot; physics_backend: \u0026quot;mujoco\u0026quot; timestep: 0.01 max_episode_steps: 1000 observation: include_velocities: true include_accelerations: false history_length: 3 reward: weights: task: 1.0 energy: 0.1 smoothness: 0.05 balance: 0.2 termination: fall_threshold: 0.3 max_joint_error: 45 # degrees  G.3.2 Training Configuration # config/train/ppo_default.yaml algorithm: \u0026quot;ppo\u0026quot; policy: network_type: \u0026quot;mlp\u0026quot; hidden_sizes: [256, 256] activation: \u0026quot;tanh\u0026quot; training: learning_rate: 3e-4 n_steps: 2048 batch_size: 64 n_epochs: 10 gamma: 0.99 gae_lambda: 0.95 clip_range: 0.2 schedule: learning_rate: \u0026quot;linear\u0026quot; clip_range: \u0026quot;constant\u0026quot;  G.4 Usage Examples G.4.1 Basic Training Script import rlanimate as rla # Create environment env = rla.make_env('Locomotion-v1') # Create policy policy = rla.Policy( observation_space=env.observation_space, action_space=env.action_space, hidden_sizes=[256, 256] ) # Create trainer trainer = rla.Trainer( env=env, policy=policy, algorithm='ppo', config={'learning_rate': 3e-4} ) # Train policy = trainer.train(total_timesteps=1_000_000) # Save model trainer.save('models/locomotion_ppo.zip') # Evaluate mean_reward = rla.evaluate(env, policy, n_episodes=100) print(f\u0026quot;Mean reward: {mean_reward}\u0026quot;)  G.4.2 Custom Task Definition class JumpingTask(rla.Task): \u0026quot;\u0026quot;\u0026quot;Custom jumping task\u0026quot;\u0026quot;\u0026quot; def __init__(self, target_height: float = 0.5): self.target_height = target_height def compute_reward(self, state: State, action: np.ndarray) -\u0026gt; float: # Height reward height_reward = np.exp(-abs(state.com_height - self.target_height)) # Posture reward posture_reward = self.compute_posture_reward(state) # Energy penalty energy_penalty = -0.001 * np.sum(action**2) return height_reward + 0.5 * posture_reward + energy_penalty def is_success(self, state: State) -\u0026gt; bool: return state.max_com_height \u0026gt;= self.target_height * 0.95  G.4.3 Visualization Script import rlanimate.visualization as viz # Load trained model env = rla.make_env('Locomotion-v1') policy = rla.load_policy('models/locomotion_ppo.zip') # Create visualizer visualizer = viz.Visualizer(env, policy) # Interactive visualization visualizer.run_interactive() # Record video visualizer.record_video( 'output/locomotion_demo.mp4', n_episodes=5, fps=30 ) # Generate trajectory plots trajectories = visualizer.collect_trajectories(n_episodes=10) viz.plot_trajectories(trajectories, save_path='output/trajectories.png')  G.5 Extension Guide G.5.1 Adding New Environments # rlanimate/envs/custom_env.py class CustomEnv(RLAnimateEnv): \u0026quot;\u0026quot;\u0026quot;Template for custom environment\u0026quot;\u0026quot;\u0026quot; def __init__(self, config): super().__init__(config) # Initialize custom components def _compute_reward(self, state, action): # Implement reward function pass def _get_observation(self, state): # Implement observation extraction pass # Register environment from gym.envs.registration import register register( id='CustomEnv-v1', entry_point='rlanimate.envs:CustomEnv', )  G.5.2 Adding New Algorithms # rlanimate/algorithms/custom_algo.py class CustomAlgorithm(BaseAlgorithm): \u0026quot;\u0026quot;\u0026quot;Template for custom RL algorithm\u0026quot;\u0026quot;\u0026quot; def train_step(self, batch): # Implement training step pass def update_policy(self, trajectories): # Implement policy update pass  G.6 Performance Benchmarks G.6.1 Training Speed    Algorithm Steps/Second GPU Memory Training Time (1M steps)     PPO 15,000 2.1 GB 1.1 hours   SAC 12,000 2.8 GB 1.4 hours   TD3 13,500 2.5 GB 1.2 hours    G.6.2 Task Performance    Task PPO SAC TD3 Human Demo     Walk 0.92 0.94 0.91 0.98   Run 0.88 0.91 0.89 0.97   Jump 0.85 0.82 0.86 0.95    G.7 Troubleshooting G.7.1 Common Issues  ImportError: Ensure all dependencies are installed CUDA Error: Check GPU compatibility and drivers Memory Error: Reduce batch size or model size Convergence Issues: Adjust learning rate or exploration  G.7.2 Debug Mode # Enable debug logging import logging logging.basicConfig(level=logging.DEBUG) # Run with debug flags env = rla.make_env('Locomotion-v1', debug=True) trainer = rla.Trainer(env, policy, debug=True)  G.8 License and Citation G.8.1 License RLAnimate is released under the MIT License.\nG.8.2 Citation @software{rlanimate2024, title={RLAnimate: A Framework for Physics-based Character Animation}, author={[Author Names]}, year={2024}, url={https://github.com/[username]/rlanimate} }  ","date":1737504000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1737504000,"objectID":"df16b1e5ec5509fe58e851f187bbc6b9","permalink":"https://vihanga.github.io/thesis/appendices/g/","publishdate":"2025-01-22T00:00:00Z","relpermalink":"/thesis/appendices/g/","section":"thesis","summary":"Appendix G: RLAnimate Directory This appendix provides a comprehensive directory of the RLAnimate framework, including its architecture, components, API reference, and usage examples.\nG.1 System Architecture G.1.1 Overview RLAnimate is a modular framework for physics-based character animation using reinforcement learning. The system consists of:\nRLAnimate/ ├── core/ │ ├── agents/ │ ├── environments/ │ ├── physics/ │ └── utils/ ├── models/ │ ├── policies/ │ ├── value_functions/ │ └── networks/ ├── training/ │ ├── algorithms/ │ ├── replay_buffers/ │ └── schedulers/ ├── evaluation/ │ ├── metrics/ │ ├── visualization/ │ └── benchmarks/ └── examples/ ├── tutorials/ ├── experiments/ └── demos/  G.","tags":null,"title":"Appendix G: RLAnimate Directory","type":"thesis"},{"authors":null,"categories":null,"content":" Chapter 7: Conversational Gesture Generation This chapter addresses the specific challenge of generating natural conversational gestures that accompany speech, presenting methods for creating embodied conversational agents with realistic nonverbal communication behaviors.\n7.1 Introduction [Content placeholder: Introduce the importance of conversational gestures in human communication and the challenges of automatic gesture generation for virtual agents]\n7.2 Understanding Conversational Gestures This section provides background on the nature and function of conversational gestures.\n7.2.1 Types of Conversational Gestures [Content placeholder: Categorize different types of gestures including beat gestures, iconic gestures, metaphoric gestures, and deictic gestures]\n7.2.2 Speech-Gesture Synchrony [Content placeholder: Discuss the temporal relationships between speech and gesture, including principles of synchronization]\n7.2.3 Individual and Cultural Variations [Content placeholder: Address variations in gesture patterns across individuals and cultures]\n7.3 Data-Driven Gesture Generation This section presents methods for learning gesture generation models from human data.\n7.3.1 Multimodal Data Collection [Content placeholder: Describe methods for collecting synchronized speech and motion data for training gesture generation models]\n7.3.2 Feature Extraction and Representation [Content placeholder: Present techniques for extracting relevant features from speech (prosody, semantics) and motion data]\n7.3.3 Sequence-to-Sequence Models [Content placeholder: Describe neural architectures for mapping from speech features to gesture sequences]\n7.4 Reinforcement Learning for Gesture Adaptation This section explores how RL can be used to adapt and improve gesture generation based on interaction feedback.\n7.4.1 Interactive Learning Framework [Content placeholder: Present an RL framework for learning gesture policies through interaction with human users]\n7.4.2 Reward Design for Natural Gestures [Content placeholder: Discuss reward functions that capture naturalness, expressiveness, and communicative effectiveness]\n7.4.3 Online Adaptation [Content placeholder: Describe methods for online adaptation of gesture generation to individual users and contexts]\n7.5 Evaluation Methods 7.5.1 Objective Metrics [Content placeholder: Define quantitative metrics for evaluating gesture generation including synchrony, diversity, and appropriateness]\n7.5.2 User Studies [Content placeholder: Present user study methodologies for evaluating the perceived naturalness and effectiveness of generated gestures]\n7.5.3 Comparative Analysis [Content placeholder: Compare different approaches including rule-based, data-driven, and RL-based methods]\n7.6 Applications and Case Studies 7.6.1 Virtual Assistants [Content placeholder: Demonstrate applications to embodied virtual assistants with natural gesture behaviors]\n7.6.2 Social Robots [Content placeholder: Show applications to social robotics where appropriate gesture generation enhances human-robot interaction]\n7.6.3 Virtual Reality Avatars [Content placeholder: Present applications to VR avatars that need to generate gestures in real-time during conversation]\n7.7 Chapter Summary This chapter has presented a comprehensive approach to conversational gesture generation, combining insights from human communication research with advanced machine learning techniques. The methods developed enable virtual agents to communicate more naturally and effectively through appropriate nonverbal behaviors.\nKey contributions of this chapter include: - A framework for learning gesture generation models from multimodal human data - Novel RL approaches for adapting gesture generation based on interaction feedback - Comprehensive evaluation methods for assessing gesture quality and effectiveness - Demonstration of applications across various domains including virtual assistants and social robotics\nThe work presented in this chapter advances the state of the art in embodied conversational agents, bringing us closer to virtual characters that can engage in natural, multimodal communication with humans.\n","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"b23e04332de28844bae789171e90c6e3","permalink":"https://vihanga.github.io/thesis/chapters/conversational-gestures/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/thesis/chapters/conversational-gestures/","section":"thesis","summary":"Chapter 7: Conversational Gesture Generation This chapter addresses the specific challenge of generating natural conversational gestures that accompany speech, presenting methods for creating embodied conversational agents with realistic nonverbal communication behaviors.\n7.1 Introduction [Content placeholder: Introduce the importance of conversational gestures in human communication and the challenges of automatic gesture generation for virtual agents]\n7.2 Understanding Conversational Gestures This section provides background on the nature and function of conversational gestures.","tags":null,"title":"Chapter 7: Conversational Gesture Generation","type":"thesis"},{"authors":null,"categories":null,"content":" Chapter 8: Conclusion This chapter summarizes the contributions of this thesis, discusses their implications for the field of character animation and reinforcement learning, and outlines directions for future research.\n8.1 Summary of Contributions This thesis has presented a comprehensive exploration of reinforcement learning methods for character animation, addressing fundamental challenges and proposing novel solutions across multiple domains.\n8.1.1 Technical Contributions [Content placeholder: Summarize the main technical contributions including: - Novel RL algorithms for physics-based animation - Model-based and model-free approaches for animation control - Methods for leveraging existing motion data in RL frameworks - Latent dynamics models for efficient learning and control - Gesture generation techniques for conversational agents]\n8.1.2 Theoretical Insights [Content placeholder: Discuss theoretical insights gained about: - The relationship between physics-based simulation and learning - Trade-offs between model-based and model-free approaches - The role of data priors in animation learning - Connections between human motion principles and RL objectives]\n8.1.3 Practical Applications [Content placeholder: Highlight practical applications demonstrated including: - Interactive character control systems - Animation tools for content creation - Embodied conversational agents - Motion synthesis for games and virtual environments]\n8.2 Impact and Significance 8.2.1 Advancing the State of the Art [Content placeholder: Discuss how this work advances the state of the art in character animation, comparing with previous approaches and highlighting improvements]\n8.2.2 Bridging Communities [Content placeholder: Describe how this work bridges the computer graphics and machine learning communities, fostering cross-disciplinary collaboration]\n8.2.3 Enabling New Applications [Content placeholder: Discuss new applications enabled by the methods developed in this thesis, from entertainment to robotics]\n8.3 Limitations and Challenges 8.3.1 Computational Requirements [Content placeholder: Acknowledge computational limitations and discuss trade-offs between quality and efficiency]\n8.3.2 Generalization Boundaries [Content placeholder: Discuss limitations in generalization across different characters, environments, and tasks]\n8.3.3 Evaluation Challenges [Content placeholder: Address the ongoing challenge of objectively evaluating animation quality and naturalness]\n8.4 Future Directions 8.4.1 Scaling to More Complex Behaviors [Content placeholder: Outline research directions for scaling the methods to more complex behaviors and longer time horizons]\n8.4.2 Multi-agent and Social Interactions [Content placeholder: Discuss extensions to multi-agent scenarios and social interaction modeling]\n8.4.3 Integration with Large Language Models [Content placeholder: Explore potential integration with LLMs for high-level behavior specification and control]\n8.4.4 Real-world Deployment [Content placeholder: Discuss pathways for deploying these methods in real-world applications, from games to robotics]\n8.5 Closing Remarks The work presented in this thesis represents a significant step forward in creating intelligent, adaptive character animation systems through reinforcement learning. By addressing fundamental challenges in physics-based animation, data integration, and behavioral modeling, we have demonstrated that RL can be a powerful tool for creating lifelike virtual characters.\nThe methods and insights developed here open new possibilities for interactive entertainment, virtual reality, robotics, and human-computer interaction. As we continue to push the boundaries of what is possible with learned animation systems, the vision of truly intelligent virtual characters that can adapt, learn, and interact naturally with humans comes ever closer to reality.\nThe journey of bringing artificial characters to life through learning is far from complete, but the foundations laid in this thesis provide a solid basis for future innovations. It is my hope that this work will inspire continued research at the intersection of animation and machine learning, ultimately leading to virtual characters that are indistinguishable from their human counterparts in their ability to move, gesture, and express themselves naturally.\n","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"8c2b9df60fa3dd5ba941a75a5fd50439","permalink":"https://vihanga.github.io/thesis/chapters/conclusion/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/thesis/chapters/conclusion/","section":"thesis","summary":"Chapter 8: Conclusion This chapter summarizes the contributions of this thesis, discusses their implications for the field of character animation and reinforcement learning, and outlines directions for future research.\n8.1 Summary of Contributions This thesis has presented a comprehensive exploration of reinforcement learning methods for character animation, addressing fundamental challenges and proposing novel solutions across multiple domains.\n8.1.1 Technical Contributions [Content placeholder: Summarize the main technical contributions including: - Novel RL algorithms for physics-based animation - Model-based and model-free approaches for animation control - Methods for leveraging existing motion data in RL frameworks - Latent dynamics models for efficient learning and control - Gesture generation techniques for conversational agents]","tags":null,"title":"Chapter 8: Conclusion","type":"thesis"},{"authors":null,"categories":null,"content":" Video Demonstrations Motion Synthesis Examples  Basic Locomotion: Demonstrations of walking, running, and transitioning between different gaits Complex Behaviors: Examples of jumping, turning, and obstacle avoidance Multi-character Interactions: Synchronized movements and collision avoidance  Training Progress Visualization  Evolution of motion quality over training iterations Comparison between different reward function configurations Ablation study results in video format  Additional Results Quantitative Metrics Extended evaluation metrics not included in the main chapter: - Frame-by-frame motion quality assessment - Computational performance benchmarks - Memory usage analysis during training and inference\nQualitative Comparisons  Side-by-side comparisons with baseline methods User study results and feedback Analysis of failure cases and edge conditions  Code Examples Training Configuration # Example configuration for motion synthesis training config = { 'learning_rate': 3e-4, 'batch_size': 256, 'hidden_dims': [512, 512, 256], 'action_space': 'continuous', 'reward_weights': { 'motion_quality': 0.7, 'energy_efficiency': 0.2, 'goal_reaching': 0.1 } }  Custom Reward Function Implementation def compute_reward(state, action, next_state, reference_motion): \u0026quot;\u0026quot;\u0026quot; Compute reward based on motion quality and task objectives \u0026quot;\u0026quot;\u0026quot; # Motion matching component pose_similarity = compute_pose_similarity(next_state, reference_motion) # Energy efficiency component energy_penalty = compute_energy_penalty(action) # Task-specific objectives task_reward = compute_task_reward(state, next_state) return pose_similarity - energy_penalty + task_reward  Motion Preprocessing Pipeline # Data preprocessing for motion capture sequences def preprocess_motion_data(raw_mocap_data): # Normalize joint positions normalized_data = normalize_skeleton(raw_mocap_data) # Extract motion features velocities = compute_joint_velocities(normalized_data) accelerations = compute_joint_accelerations(velocities) # Create training sequences sequences = create_overlapping_windows(normalized_data, window_size=64) return sequences, velocities, accelerations  Dataset Information Motion Capture Database  Source: CMU Motion Capture Database, custom recordings Total Sequences: 2,847 motion clips Duration: ~15 hours of motion data Subjects: 45 different performers Motion Categories:  Locomotion (40%) Sports movements (25%) Dance sequences (20%) Daily activities (15%)   Data Preprocessing Details  Sampling rate: 120 Hz downsampled to 30 Hz Coordinate system: Y-up, right-handed Joint representation: 22-joint skeleton model Data augmentation: Mirroring, time warping, noise injection  Training/Validation Split  Training set: 80% (2,278 sequences) Validation set: 10% (285 sequences) Test set: 10% (284 sequences)  Download Links  Preprocessed dataset available upon request Raw motion capture files: [Contact for access] Trained model checkpoints: [Available on project page]  Computational Resources Hardware Requirements  Training: NVIDIA V100 GPU (32GB VRAM) Inference: GTX 1080 Ti or better Memory: 16GB RAM minimum Storage: 50GB for full dataset  Training Time  Full model: ~48 hours Ablation experiments: 12-24 hours each Hyperparameter search: ~200 GPU hours total  Supplementary Figures Additional figures and visualizations that support the main chapter: - Extended ablation study results - Detailed architecture diagrams - Motion trajectory comparisons - Error distribution analyses\nFor questions or additional materials, please contact the author.\n","date":1737504000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1737504000,"objectID":"be2d5c51703a1556b5dcc0c8d7a08c43","permalink":"https://vihanga.github.io/thesis/supplementary/c4/","publishdate":"2025-01-22T00:00:00Z","relpermalink":"/thesis/supplementary/c4/","section":"thesis","summary":"Video Demonstrations Motion Synthesis Examples  Basic Locomotion: Demonstrations of walking, running, and transitioning between different gaits Complex Behaviors: Examples of jumping, turning, and obstacle avoidance Multi-character Interactions: Synchronized movements and collision avoidance  Training Progress Visualization  Evolution of motion quality over training iterations Comparison between different reward function configurations Ablation study results in video format  Additional Results Quantitative Metrics Extended evaluation metrics not included in the main chapter: - Frame-by-frame motion quality assessment - Computational performance benchmarks - Memory usage analysis during training and inference","tags":null,"title":"Chapter 4: Supplementary Material","type":"thesis"},{"authors":null,"categories":null,"content":" Video Demonstrations Physics-Based Animation Results  Bipedal Locomotion: Natural walking and running with physics constraints Balance Recovery: Dynamic responses to external perturbations Terrain Adaptation: Movement across uneven surfaces and obstacles Muscle-Driven Motion: Anatomically-inspired character control  Comparison Videos  Physics-based vs. kinematic animation side-by-side Different physics solvers and their impact on motion quality Real-time performance demonstrations  Additional Results Performance Benchmarks Detailed performance metrics across different scenarios:\n   Scenario Frame Rate (FPS) Physics Steps/Frame Memory Usage (MB)     Single Character 120 4 256   5 Characters 60 4 780   10 Characters 30 2 1,420   Complex Environment 45 3 890    Stability Analysis  Numerical stability under extreme conditions Convergence rates for different optimization methods Error accumulation over long simulations  Physical Accuracy Validation  Comparison with real-world motion capture data Energy conservation analysis Ground reaction force validation  Code Examples Character Model Definition class PhysicsCharacter: def __init__(self): self.skeleton = self.build_skeleton() self.muscles = self.attach_muscles() self.constraints = self.define_constraints() def build_skeleton(self): \u0026quot;\u0026quot;\u0026quot;Define articulated rigid body skeleton\u0026quot;\u0026quot;\u0026quot; bodies = [] joints = [] # Create body segments pelvis = RigidBody(mass=15.0, inertia=compute_inertia('box')) torso = RigidBody(mass=20.0, inertia=compute_inertia('box')) # Define joint connections spine_joint = BallJoint(pelvis, torso, position=[0, 0.1, 0]) return Skeleton(bodies, joints)  Physics Simulation Loop def simulate_physics_step(character, dt=0.01): \u0026quot;\u0026quot;\u0026quot;Single physics simulation timestep\u0026quot;\u0026quot;\u0026quot; # Apply muscle forces muscle_forces = compute_muscle_activations(character.controller_output) character.apply_muscle_forces(muscle_forces) # External forces (gravity, contacts) apply_gravity(character, g=-9.81) contact_forces = compute_ground_contacts(character) # Integrate dynamics character.integrate_forward_dynamics(dt) # Solve constraints solve_joint_constraints(character) solve_contact_constraints(character, contact_forces) return character.get_state()  Optimization-Based Control class OptimizationController: def __init__(self, character_model): self.model = character_model self.horizon = 10 # MPC horizon def compute_control(self, current_state, target_motion): \u0026quot;\u0026quot;\u0026quot;Compute optimal control using trajectory optimization\u0026quot;\u0026quot;\u0026quot; # Define optimization problem problem = TrajectoryOptimization( dynamics=self.model.dynamics, cost_function=self.motion_tracking_cost, constraints=self.physical_constraints ) # Solve using Sequential Quadratic Programming solution = solve_sqp( problem, initial_guess=self.warm_start(current_state), max_iterations=50 ) return solution.controls[0] # Return first control  Dataset Information Motion Reference Library  Athletic Movements: 500+ sequences of sports motions Locomotion Variants: Walking, running at different speeds and styles Acrobatic Motions: Flips, rolls, and complex maneuvers Interaction Sequences: Object manipulation and character interactions  Physics Parameters Validated physical properties used in simulations: - Body Segment Properties: - Mass distributions from biomechanics literature - Inertia tensors computed from 3D scans - Joint limits based on anatomical studies\n Muscle Parameters:  Maximum isometric forces Optimal fiber lengths Tendon slack lengths Force-velocity relationships   Environment Models  Ground Materials: Concrete, grass, sand, ice (friction coefficients) Obstacle Geometries: Steps, ramps, irregular terrain meshes Wind Forces: Velocity profiles for different conditions  Computational Resources Optimization Solver Requirements  CPU: Intel Xeon or AMD Ryzen (8+ cores recommended) RAM: 32GB for complex scenarios GPU: CUDA-capable for parallel trajectory optimization  Performance Optimizations # Parallel physics simulation for multiple characters @cuda.jit def parallel_dynamics_kernel(states, forces, dt, output): \u0026quot;\u0026quot;\u0026quot;GPU kernel for parallel forward dynamics\u0026quot;\u0026quot;\u0026quot; idx = cuda.grid(1) if idx \u0026lt; states.shape[0]: # Compute dynamics for character idx output[idx] = integrate_dynamics_gpu(states[idx], forces[idx], dt)  Supplementary Figures Extended Analysis  Joint torque profiles during different movements Muscle activation patterns Center of mass trajectories Ground reaction force patterns Energy expenditure graphs  Architecture Diagrams  Detailed system architecture Data flow between components Optimization algorithm flowcharts Real-time control pipeline  Validation Studies Biomechanical Validation  Comparison with human motion studies Joint angle trajectories Muscle activation timing Metabolic energy estimates  Robustness Testing  Performance under different perturbation magnitudes Adaptation to unexpected obstacles Recovery from near-fall scenarios Long-term stability analysis  Additional materials and source code available at the project repository.\n","date":1737504000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1737504000,"objectID":"684c8c6eb974cdedac7413b0677c73bb","permalink":"https://vihanga.github.io/thesis/supplementary/c5a/","publishdate":"2025-01-22T00:00:00Z","relpermalink":"/thesis/supplementary/c5a/","section":"thesis","summary":"Video Demonstrations Physics-Based Animation Results  Bipedal Locomotion: Natural walking and running with physics constraints Balance Recovery: Dynamic responses to external perturbations Terrain Adaptation: Movement across uneven surfaces and obstacles Muscle-Driven Motion: Anatomically-inspired character control  Comparison Videos  Physics-based vs. kinematic animation side-by-side Different physics solvers and their impact on motion quality Real-time performance demonstrations  Additional Results Performance Benchmarks Detailed performance metrics across different scenarios:\n   Scenario Frame Rate (FPS) Physics Steps/Frame Memory Usage (MB)     Single Character 120 4 256   5 Characters 60 4 780   10 Characters 30 2 1,420   Complex Environment 45 3 890    Stability Analysis  Numerical stability under extreme conditions Convergence rates for different optimization methods Error accumulation over long simulations  Physical Accuracy Validation  Comparison with real-world motion capture data Energy conservation analysis Ground reaction force validation  Code Examples Character Model Definition class PhysicsCharacter: def __init__(self): self.","tags":null,"title":"Chapter 5a: Supplementary Material","type":"thesis"},{"authors":null,"categories":null,"content":" Video Demonstrations Advanced Control Strategies  Predictive Control: Model Predictive Control (MPC) for dynamic movements Hierarchical Control: Multi-level control architecture demonstrations Adaptive Behaviors: Real-time adaptation to changing environments Multi-objective Optimization: Balancing multiple animation goals  Interactive Demonstrations  Real-time user control with physics constraints Style transfer between different motion types Interactive physics parameter tuning Live motion editing with physical validity  Additional Results Control Performance Metrics    Control Method Tracking Error Computation Time Robustness Score     PD Control 0.085 rad 0.1 ms 6.2\u0026frasl;10   Trajectory Opt 0.023 rad 15.2 ms 8.7\u0026frasl;10   MPC 0.031 rad 8.5 ms 9.1\u0026frasl;10   Learning-Based 0.042 rad 0.5 ms 7.8\u0026frasl;10    Scalability Analysis  Performance with increasing DOF Multi-character coordination overhead Memory scaling with simulation complexity Real-time feasibility boundaries  Code Examples Model Predictive Control Implementation class ModelPredictiveController: def __init__(self, dynamics_model, prediction_horizon=20): self.model = dynamics_model self.horizon = prediction_horizon self.dt = 0.03 # 30Hz control def solve_mpc(self, x0, reference_trajectory): \u0026quot;\u0026quot;\u0026quot;Solve MPC optimization problem\u0026quot;\u0026quot;\u0026quot; # Decision variables X = cp.Variable((self.model.state_dim, self.horizon + 1)) U = cp.Variable((self.model.control_dim, self.horizon)) # Objective function cost = 0 for t in range(self.horizon): # Tracking cost cost += cp.quad_form(X[:, t] - reference_trajectory[:, t], self.Q) # Control effort cost += cp.quad_form(U[:, t], self.R) # Constraints constraints = [X[:, 0] == x0] for t in range(self.horizon): # Dynamics constraints constraints += [X[:, t + 1] == self.model.dynamics(X[:, t], U[:, t])] # Control limits constraints += [self.u_min \u0026lt;= U[:, t], U[:, t] \u0026lt;= self.u_max] # State constraints (joint limits, etc.) constraints += [self.x_min \u0026lt;= X[:, t + 1], X[:, t + 1] \u0026lt;= self.x_max] # Solve problem = cp.Problem(cp.Minimize(cost), constraints) problem.solve(solver=cp.OSQP, warm_start=True) return U[:, 0].value  Hierarchical Control Architecture class HierarchicalController: def __init__(self): self.high_level = TaskPlanner() self.mid_level = TrajectoryGenerator() self.low_level = JointController() def control_step(self, state, goal): \u0026quot;\u0026quot;\u0026quot;Execute hierarchical control pipeline\u0026quot;\u0026quot;\u0026quot; # High-level: Task planning task_sequence = self.high_level.plan(state, goal) # Mid-level: Trajectory generation reference_trajectory = self.mid_level.generate( current_state=state, task=task_sequence[0], duration=2.0 ) # Low-level: Joint control joint_torques = self.low_level.track( current_state=state, reference=reference_trajectory[0] ) return joint_torques  Differentiable Physics Integration import torch class DifferentiableSimulator(torch.nn.Module): def __init__(self, character_model): super().__init__() self.model = character_model def forward(self, state, control, dt): \u0026quot;\u0026quot;\u0026quot;Differentiable forward dynamics\u0026quot;\u0026quot;\u0026quot; # Compute accelerations M = self.model.mass_matrix(state) C = self.model.coriolis_forces(state) g = self.model.gravity_forces(state) # Joint torques from control tau = self.model.control_matrix @ control # Forward dynamics: M * qdd = tau - C - g qdd = torch.linalg.solve(M, tau - C - g) # Semi-implicit Euler integration velocity_new = state.velocity + qdd * dt position_new = state.position + velocity_new * dt return torch.cat([position_new, velocity_new])  Dataset Information Controller Training Data  State-Action Pairs: 5M samples from optimal control solutions Trajectory Library: 10,000 optimized motion trajectories Perturbation Data: Recovery behaviors under 50,000 perturbation scenarios Style Variations: 100 different motion styles per base movement  Benchmark Scenarios Standardized test cases for controller evaluation:\n Locomotion Tasks\n Flat ground walking/running Stair climbing Slope navigation (various angles) Stepping stones  Dynamic Movements\n Jump sequences Quick direction changes Balance beam walking Parkour elements  Robustness Tests\n Push recovery Slippery surfaces Moving platforms Wind disturbances   Computational Resources Real-time Performance Requirements minimum_requirements: cpu: \u0026quot;Intel i7-8700K or AMD Ryzen 7 2700X\u0026quot; ram: \u0026quot;16 GB DDR4\u0026quot; gpu: \u0026quot;NVIDIA GTX 1660 or AMD RX 5600 XT\u0026quot; recommended_requirements: cpu: \u0026quot;Intel i9-10900K or AMD Ryzen 9 3900X\u0026quot; ram: \u0026quot;32 GB DDR4\u0026quot; gpu: \u0026quot;NVIDIA RTX 3070 or AMD RX 6800\u0026quot;  Optimization Solver Benchmarks    Solver Problem Size Solution Time Accuracy     OSQP 1000 vars 5.2 ms 1e-4   IPOPT 1000 vars 18.7 ms 1e-6   Custom GPU 1000 vars 1.8 ms 1e-3    Supplementary Figures Control Analysis  Control signal frequency analysis Phase portraits of limit cycles Stability regions in parameter space Lyapunov function evolution  Comparative Studies  Traditional vs. optimization-based control Effect of prediction horizon on performance Computational cost vs. motion quality trade-offs Generalization to unseen scenarios  Advanced Topics Contact-Rich Scenarios Special handling for complex contact situations:\ndef handle_multiple_contacts(character, environment): \u0026quot;\u0026quot;\u0026quot;Manage multiple simultaneous contacts\u0026quot;\u0026quot;\u0026quot; contacts = detect_all_contacts(character, environment) # Build contact Jacobian J_c = build_contact_jacobian(contacts) # Solve contact forces using LCP contact_forces = solve_lcp( M=character.mass_matrix, J=J_c, v=character.velocity, restitution=0.1, friction=0.8 ) return contact_forces  Learning-Augmented Control Combining model-based and learning approaches: - Neural network residual models - Learned value functions for MPC - Adaptive parameter tuning - Online model refinement\nFor implementation details and experimental data, visit the project repository.\n","date":1737504000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1737504000,"objectID":"4533a70804b1d00a48ea76985216082e","permalink":"https://vihanga.github.io/thesis/supplementary/c5b/","publishdate":"2025-01-22T00:00:00Z","relpermalink":"/thesis/supplementary/c5b/","section":"thesis","summary":"Video Demonstrations Advanced Control Strategies  Predictive Control: Model Predictive Control (MPC) for dynamic movements Hierarchical Control: Multi-level control architecture demonstrations Adaptive Behaviors: Real-time adaptation to changing environments Multi-objective Optimization: Balancing multiple animation goals  Interactive Demonstrations  Real-time user control with physics constraints Style transfer between different motion types Interactive physics parameter tuning Live motion editing with physical validity  Additional Results Control Performance Metrics    Control Method Tracking Error Computation Time Robustness Score     PD Control 0.","tags":null,"title":"Chapter 5b: Supplementary Material","type":"thesis"},{"authors":null,"categories":null,"content":" Video Demonstrations Latent Space Visualization  Motion Embeddings: t-SNE and PCA visualizations of learned representations Interpolation Results: Smooth transitions between different motion styles Latent Dynamics: Evolution of latent codes during motion sequences Disentanglement: Independent control of motion factors  Generative Results  Novel motion synthesis from latent codes Style transfer between characters Motion completion and in-betweening Multi-modal motion generation  Additional Results Quantitative Evaluation    Model Variant Reconstruction Error Latent Dim Disentanglement Score FID Score     VAE 0.082 128 0.72 45.3   β-VAE 0.091 128 0.85 48.1   VQ-VAE 0.075 512 codes 0.68 42.7   Proposed 0.071 64 0.89 38.2    Ablation Studies  Impact of latent dimensionality Different regularization strategies Architecture choices for encoder/decoder Training objective variations  Generalization Analysis  Cross-dataset performance Different skeleton configurations Unseen motion categories Temporal extrapolation  Code Examples Variational Autoencoder Architecture class MotionVAE(nn.Module): def __init__(self, input_dim, latent_dim, hidden_dims=[512, 256]): super().__init__() self.latent_dim = latent_dim # Encoder encoder_layers = [] in_dim = input_dim for h_dim in hidden_dims: encoder_layers.extend([ nn.Linear(in_dim, h_dim), nn.ReLU(), nn.BatchNorm1d(h_dim) ]) in_dim = h_dim self.encoder = nn.Sequential(*encoder_layers) self.fc_mu = nn.Linear(hidden_dims[-1], latent_dim) self.fc_var = nn.Linear(hidden_dims[-1], latent_dim) # Decoder decoder_layers = [] hidden_dims.reverse() in_dim = latent_dim for h_dim in hidden_dims: decoder_layers.extend([ nn.Linear(in_dim, h_dim), nn.ReLU(), nn.BatchNorm1d(h_dim) ]) in_dim = h_dim self.decoder = nn.Sequential(*decoder_layers) self.final_layer = nn.Linear(hidden_dims[-1], input_dim) def encode(self, x): h = self.encoder(x) return self.fc_mu(h), self.fc_var(h) def reparameterize(self, mu, logvar): std = torch.exp(0.5 * logvar) eps = torch.randn_like(std) return mu + eps * std def decode(self, z): h = self.decoder(z) return self.final_layer(h) def forward(self, x): mu, logvar = self.encode(x) z = self.reparameterize(mu, logvar) return self.decode(z), mu, logvar  Latent Dynamics Model class LatentDynamics(nn.Module): def __init__(self, latent_dim, action_dim, hidden_dim=256): super().__init__() self.dynamics_net = nn.Sequential( nn.Linear(latent_dim + action_dim, hidden_dim), nn.ReLU(), nn.Linear(hidden_dim, hidden_dim), nn.ReLU(), nn.Linear(hidden_dim, latent_dim) ) # Learned prior for regularization self.prior_net = nn.Sequential( nn.Linear(latent_dim, hidden_dim), nn.ReLU(), nn.Linear(hidden_dim, latent_dim * 2) # mu and logvar ) def forward(self, z_t, a_t): \u0026quot;\u0026quot;\u0026quot;Predict next latent state given current state and action\u0026quot;\u0026quot;\u0026quot; z_next = self.dynamics_net(torch.cat([z_t, a_t], dim=-1)) return z_t + z_next # Residual connection def get_prior(self, z_t): \u0026quot;\u0026quot;\u0026quot;Compute prior distribution for next state\u0026quot;\u0026quot;\u0026quot; prior_params = self.prior_net(z_t) mu, logvar = torch.chunk(prior_params, 2, dim=-1) return mu, logvar  Training Pipeline def train_latent_model(model, dynamics_model, dataloader, epochs=100): optimizer = torch.optim.Adam( list(model.parameters()) + list(dynamics_model.parameters()), lr=1e-3 ) for epoch in range(epochs): for batch in dataloader: motion_sequence = batch['motion'] # Shape: (B, T, D) # Encode entire sequence latent_sequence = [] for t in range(motion_sequence.size(1)): mu, logvar = model.encode(motion_sequence[:, t]) z = model.reparameterize(mu, logvar) latent_sequence.append(z) latent_sequence = torch.stack(latent_sequence, dim=1) # Reconstruction loss recon_loss = 0 for t in range(motion_sequence.size(1)): recon = model.decode(latent_sequence[:, t]) recon_loss += F.mse_loss(recon, motion_sequence[:, t]) # Dynamics loss dynamics_loss = 0 for t in range(motion_sequence.size(1) - 1): z_pred = dynamics_model(latent_sequence[:, t], batch['actions'][:, t]) dynamics_loss += F.mse_loss(z_pred, latent_sequence[:, t + 1]) # KL divergence for VAE kl_loss = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp()) # Total loss loss = recon_loss + 0.1 * dynamics_loss + 0.001 * kl_loss optimizer.zero_grad() loss.backward() optimizer.step()  Dataset Information Motion Sequences Used  CMU MoCap: 2,500 sequences covering 100+ motion types Human3.6M: 3.6 million frames of daily activities Custom Dance Dataset: 500 professional dance sequences Sports Motion Library: 1,000 athletic movement clips  Data Preprocessing # Motion data normalization pipeline def preprocess_motion_sequences(raw_data): # Global position normalization normalized = normalize_root_position(raw_data) # Velocity and acceleration features velocities = compute_finite_differences(normalized, order=1) accelerations = compute_finite_differences(normalized, order=2) # Joint angle representation joint_angles = forward_kinematics_to_angles(normalized) # Combine features features = np.concatenate([ normalized, velocities * 0.1, # Scale velocities accelerations * 0.01, # Scale accelerations joint_angles ], axis=-1) return features  Evaluation Protocols  Reconstruction Quality\n Per-joint position error Velocity matching Foot sliding artifacts Motion smoothness  Latent Space Quality\n Interpolation smoothness Disentanglement metrics Coverage and diversity Clustering quality   Computational Resources Training Infrastructure  GPU: 4x NVIDIA A100 (40GB) Training Time: 72 hours for full model Batch Size: 256 sequences Memory Usage: ~35GB GPU memory  Inference Performance    Operation Time (ms) Memory (MB)     Encode 0.8 45   Decode 1.2 52   Dynamics Step 0.3 18   Full Pipeline 2.5 120    Supplementary Figures Latent Space Analysis  Distribution of learned codes Principal components visualization Trajectory analysis in latent space Correlation with semantic attributes  Architecture Variants  Different encoder/decoder architectures Skip connections impact Attention mechanisms Temporal modeling choices  Advanced Applications Motion Editing Interface class LatentMotionEditor: def __init__(self, vae_model): self.model = vae_model self.attribute_directions = self.learn_attribute_directions() def edit_motion(self, motion, attribute, strength): \u0026quot;\u0026quot;\u0026quot;Edit motion by manipulating latent codes\u0026quot;\u0026quot;\u0026quot; # Encode to latent space z, _ = self.model.encode(motion) # Apply attribute direction direction = self.attribute_directions[attribute] z_edited = z + strength * direction # Decode back to motion edited_motion = self.model.decode(z_edited) return edited_motion def interpolate(self, motion1, motion2, alpha): \u0026quot;\u0026quot;\u0026quot;Smooth interpolation between motions\u0026quot;\u0026quot;\u0026quot; z1, _ = self.model.encode(motion1) z2, _ = self.model.encode(motion2) # Spherical linear interpolation z_interp = slerp(z1, z2, alpha) return self.model.decode(z_interp)  Real-time Applications  Interactive motion synthesis Online motion compression Latent space control interfaces Motion prediction and completion  Code and trained models available at the project repository. Contact for dataset access.\n","date":1737504000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1737504000,"objectID":"10c1b5d05179a4d74b11d745a8d44096","permalink":"https://vihanga.github.io/thesis/supplementary/c6/","publishdate":"2025-01-22T00:00:00Z","relpermalink":"/thesis/supplementary/c6/","section":"thesis","summary":"Video Demonstrations Latent Space Visualization  Motion Embeddings: t-SNE and PCA visualizations of learned representations Interpolation Results: Smooth transitions between different motion styles Latent Dynamics: Evolution of latent codes during motion sequences Disentanglement: Independent control of motion factors  Generative Results  Novel motion synthesis from latent codes Style transfer between characters Motion completion and in-betweening Multi-modal motion generation  Additional Results Quantitative Evaluation    Model Variant Reconstruction Error Latent Dim Disentanglement Score FID Score     VAE 0.","tags":null,"title":"Chapter 6: Supplementary Material","type":"thesis"},{"authors":null,"categories":null,"content":" Video Demonstrations Gesture Generation Results  Speech-Driven Gestures: Synchronized hand movements with speech prosody Emotion-Aware Generation: Gestures reflecting different emotional states Multi-Speaker Scenarios: Turn-taking and interaction gestures Cultural Variations: Gesture styles across different cultural contexts  Comparative Studies  Human gesture capture vs. generated gestures Different model architectures comparison Ablation study visualizations Real-time generation demonstrations  Additional Results Quantitative Metrics    Method Sync Score Diversity Naturalness User Rating     Baseline RNN 0.62 0.45 6.2\u0026frasl;10 5.8\u0026frasl;10   Transformer 0.78 0.68 7.5\u0026frasl;10 7.2\u0026frasl;10   Audio2Gesture 0.81 0.72 7.8\u0026frasl;10 7.6\u0026frasl;10   Proposed 0.89 0.85 8.7\u0026frasl;10 8.5\u0026frasl;10    User Study Details  Participants: 120 subjects (diverse backgrounds) Evaluation Protocol: A/B testing, Likert scales Statistical Significance: p \u0026lt; 0.001 for all comparisons Qualitative Feedback: Thematic analysis of comments  Cross-Dataset Evaluation Performance on different speech-gesture datasets: - Trinity Speech-Gesture Dataset - GENEA Challenge Dataset - Talking With Hands Dataset - Custom Multi-cultural Dataset\nCode Examples Audio Feature Extraction class AudioFeatureExtractor: def __init__(self, sample_rate=16000): self.sample_rate = sample_rate self.mel_spec = torchaudio.transforms.MelSpectrogram( sample_rate=sample_rate, n_fft=1024, hop_length=160, n_mels=80 ) def extract_features(self, audio_waveform): \u0026quot;\u0026quot;\u0026quot;Extract multi-scale audio features\u0026quot;\u0026quot;\u0026quot; # Mel-spectrogram mel_features = self.mel_spec(audio_waveform) # Prosodic features pitch = self.extract_pitch(audio_waveform) energy = self.extract_energy(audio_waveform) # Rhythm features onset_env = librosa.onset.onset_strength( y=audio_waveform.numpy(), sr=self.sample_rate ) tempo, beats = librosa.beat.beat_track( onset_envelope=onset_env, sr=self.sample_rate ) # Voice activity detection vad = self.compute_vad(audio_waveform) return { 'mel': mel_features, 'pitch': pitch, 'energy': energy, 'rhythm': onset_env, 'tempo': tempo, 'vad': vad }  Gesture Generation Model class GestureGenerator(nn.Module): def __init__(self, audio_dim, text_dim, gesture_dim, hidden_dim=512): super().__init__() # Audio encoder self.audio_encoder = nn.TransformerEncoder( nn.TransformerEncoderLayer( d_model=audio_dim, nhead=8, dim_feedforward=2048 ), num_layers=6 ) # Text encoder (for semantic understanding) self.text_encoder = nn.TransformerEncoder( nn.TransformerEncoderLayer( d_model=text_dim, nhead=8, dim_feedforward=2048 ), num_layers=4 ) # Cross-modal attention self.cross_attention = nn.MultiheadAttention( embed_dim=hidden_dim, num_heads=8 ) # Gesture decoder self.gesture_decoder = nn.TransformerDecoder( nn.TransformerDecoderLayer( d_model=hidden_dim, nhead=8, dim_feedforward=2048 ), num_layers=6 ) # Output projection self.output_projection = nn.Linear(hidden_dim, gesture_dim) def forward(self, audio_features, text_features, style_embedding=None): # Encode modalities audio_encoded = self.audio_encoder(audio_features) text_encoded = self.text_encoder(text_features) # Cross-modal fusion fused_features, _ = self.cross_attention( query=audio_encoded, key=text_encoded, value=text_encoded ) # Add style conditioning if provided if style_embedding is not None: fused_features = fused_features + style_embedding # Generate gestures gesture_features = self.gesture_decoder( tgt=fused_features, memory=fused_features ) # Project to gesture space gestures = self.output_projection(gesture_features) return gestures  Training with Adversarial Loss class GestureGAN: def __init__(self, generator, discriminator): self.generator = generator self.discriminator = discriminator def train_step(self, real_data, audio_features, text_features): # Train discriminator fake_gestures = self.generator(audio_features, text_features) real_score = self.discriminator(real_data, audio_features) fake_score = self.discriminator(fake_gestures.detach(), audio_features) d_loss = -torch.mean(real_score) + torch.mean(fake_score) # Gradient penalty for WGAN-GP gradient_penalty = self.compute_gradient_penalty( real_data, fake_gestures, audio_features ) d_loss += 10 * gradient_penalty # Train generator fake_score = self.discriminator(fake_gestures, audio_features) g_loss = -torch.mean(fake_score) # Additional losses sync_loss = self.compute_sync_loss(fake_gestures, audio_features) smooth_loss = self.compute_smoothness_loss(fake_gestures) g_total_loss = g_loss + 0.1 * sync_loss + 0.05 * smooth_loss return g_total_loss, d_loss  Dataset Information Speech-Gesture Corpus  Total Duration: 50 hours of aligned speech-gesture data Speakers: 30 individuals (diverse demographics) Languages: English, Spanish, Japanese, Arabic Gesture Types:  Iconic (35%) Metaphoric (25%) Deictic (20%) Beat (20%)   Annotation Schema { \u0026quot;gesture_annotation\u0026quot;: { \u0026quot;id\u0026quot;: \u0026quot;gesture_001\u0026quot;, \u0026quot;start_time\u0026quot;: 1.23, \u0026quot;end_time\u0026quot;: 2.45, \u0026quot;type\u0026quot;: \u0026quot;iconic\u0026quot;, \u0026quot;hands\u0026quot;: [\u0026quot;right\u0026quot;, \u0026quot;left\u0026quot;], \u0026quot;phases\u0026quot;: { \u0026quot;preparation\u0026quot;: [1.23, 1.45], \u0026quot;stroke\u0026quot;: [1.45, 2.10], \u0026quot;retraction\u0026quot;: [2.10, 2.45] }, \u0026quot;semantic_label\u0026quot;: \u0026quot;describing_size\u0026quot;, \u0026quot;intensity\u0026quot;: 0.8 } }  Motion Capture Setup  System: OptiTrack with 24 cameras Markers: 53 markers (full body + fingers) Frame Rate: 120 FPS Audio: Synchronized at 48kHz Video: 1080p at 30 FPS for reference  Computational Resources Training Configuration training: batch_size: 32 learning_rate: 0.0001 epochs: 200 gradient_clip: 1.0 warmup_steps: 4000 hardware: gpus: 2x NVIDIA V100 memory: 64GB RAM storage: 2TB SSD preprocessing: audio_normalization: true gesture_normalization: true augmentation: time_warping: 0.1 noise_injection: 0.05 style_mixing: true  Real-time Performance    Component Latency (ms) Throughput     Audio Processing 5 200 FPS   Text Encoding 8 125 FPS   Gesture Generation 15 66 FPS   Total Pipeline 30 33 FPS    Supplementary Figures Evaluation Visualizations  Gesture phase timing analysis Cross-correlation between speech and motion Style embedding space visualization Error distribution across gesture types  Ablation Study Results  Impact of different audio features Text modality contribution Architecture component analysis Loss function combinations  Advanced Features Style Transfer class GestureStyleTransfer: def __init__(self, model, style_encoder): self.model = model self.style_encoder = style_encoder def extract_style(self, reference_gestures): \u0026quot;\u0026quot;\u0026quot;Extract style embedding from reference\u0026quot;\u0026quot;\u0026quot; style_features = self.style_encoder(reference_gestures) # Average pooling over time style_embedding = torch.mean(style_features, dim=1) return style_embedding def transfer_style(self, audio, text, target_style): \u0026quot;\u0026quot;\u0026quot;Generate gestures with target style\u0026quot;\u0026quot;\u0026quot; style_embedding = self.extract_style(target_style) gestures = self.model(audio, text, style_embedding) return gestures  Interactive Demo Interface class InteractiveGestureDemo: def __init__(self, model_path): self.model = load_model(model_path) self.audio_processor = AudioFeatureExtractor() self.text_processor = TextEncoder() def generate_from_speech(self, audio_input, transcript=None): \u0026quot;\u0026quot;\u0026quot;Real-time gesture generation from speech input\u0026quot;\u0026quot;\u0026quot; # Process audio audio_features = self.audio_processor.extract_features(audio_input) # Process text if available if transcript: text_features = self.text_processor.encode(transcript) else: # Use ASR if no transcript provided transcript = self.asr_model(audio_input) text_features = self.text_processor.encode(transcript) # Generate gestures with torch.no_grad(): gestures = self.model(audio_features, text_features) # Post-process for smoothness gestures = self.post_process(gestures) return gestures, transcript  Evaluation Metrics Implementation def compute_synchrony_score(gestures, audio_features): \u0026quot;\u0026quot;\u0026quot;Measure gesture-speech synchronization\u0026quot;\u0026quot;\u0026quot; # Extract gesture velocity peaks gesture_velocity = np.diff(gestures, axis=0) gesture_peaks = find_peaks(np.linalg.norm(gesture_velocity, axis=-1)) # Extract audio emphasis points audio_peaks = find_peaks(audio_features['energy']) # Compute correlation sync_score = compute_peak_correlation(gesture_peaks, audio_peaks) return sync_score  Demo videos and interactive examples available at the project website. Code repository includes pre-trained models and evaluation scripts.\n","date":1737504000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1737504000,"objectID":"d370faf13eef94886084b843b011fa53","permalink":"https://vihanga.github.io/thesis/supplementary/c7/","publishdate":"2025-01-22T00:00:00Z","relpermalink":"/thesis/supplementary/c7/","section":"thesis","summary":"Video Demonstrations Gesture Generation Results  Speech-Driven Gestures: Synchronized hand movements with speech prosody Emotion-Aware Generation: Gestures reflecting different emotional states Multi-Speaker Scenarios: Turn-taking and interaction gestures Cultural Variations: Gesture styles across different cultural contexts  Comparative Studies  Human gesture capture vs. generated gestures Different model architectures comparison Ablation study visualizations Real-time generation demonstrations  Additional Results Quantitative Metrics    Method Sync Score Diversity Naturalness User Rating     Baseline RNN 0.","tags":null,"title":"Chapter 7: Supplementary Material","type":"thesis"},{"authors":["Vihanga Gamage"],"categories":[],"content":"Originally published on RTÉ Brainstorm\nThe solar system is full of debris and rocks floating around. A meteor shower or a shooting star is debris burning up upon entry to the atmosphere. On occasion, pieces of rock make it all the way through. A much larger rock that could make its way through is an asteroid. About 66 million years ago, the impact from an asteroid brought about the Cretaceous extinction. A repetition of this event is theoretically possible - and the same could be said about all-powerful Artificial Intelligence Overlords marginalising the human race.\nSince its inception in the mid 20th century, the field of artificial intelligence has had an interesting ride. Much like the financial markets or Hollywood, there’s been breakthroughs and failures and booms and busts. Driven by advances in deep learning, a great deal of success has been achieved this decade, ushering in the newest AI golden age, along with a frenzy of interest and investment.\nThe current popular views of AI are being shaped primarily by several vocal figures in technology. Microsoft founder Bill Gates has said that AI carries the potential to change society deeply and is both exciting and dangerous. IBM CEO Ginni Rometty has stated that she believes advances in AI will create different jobs rather than no jobs. Facebook founder Mark Zuckerberg has expressed the view that AI will deliver various improvements to human quality of life, while Tesla founder Elon Musk has called AI \u0026ldquo;a fundamental risk to the existence of human civilisation\u0026rdquo;.\nThe future dangers AI could pose to humanity is a frequently occurring subject in conversation, be it to break the ice on a first date, to fill the time before a meeting starts or while waiting for a pint at the pub on a Friday evening. And arguably, Musk\u0026rsquo;s statement carries the most bite. It certainly is a very catchy line that makes for a more topical conversation starter than asteroids or the Cretaceous extinction.\nAs a result, these hypothetical yet sensationalist doomsday opinions are at a point where they dominate the conversation about AI, so much so that the bigger picture on the subject may at the point of being overshadowed and given very little consideration. And that could pose an even greater threat, especially in an age where hype and rhetoric without regard for fact has had a great effect on the fate of the world.\nAdvancements in AI could lead to a superintelligence that could pose a fundamental risk to human existence - and so could climate change or a 5 km-wide space rock colliding with the Earth. But there are many points of difference between these three risks. AI is a long way away advancing to the point of superintelligence and it’s not a certainty that superintelligence would lead to the end of humanity. Climate change is a very real and present danger, with the time to take action to prevent irreversible consequences continuing to decrease. And then there are the chances of an asteroid striking the earth.\nOf course, there is no good that can come from an asteroid strike or global warming, but AI can literally be one of the best things to happen to humans. This is not said enough - and it can’t be said enough. AI can and is being used to as a tool in many incredible ways. For example, it is being used to help improve prevention of cancer, provide effective treatment, and could one day even be part of the solution.\nAI is also being leveraged as a promising way to combat climate change. AI-powered simulations and analysis helps make for sustainable urban planning and ocean preservation. It is possible to make more accurate predictions regarding weather events with the help of AI leading to reduced damage to life and property. It can help power grids to be more energy efficient by predicting peak over of usage allowing for better preparation and enhances the benefits of clean energy methods by deploying in ways such as to incorporate weather and other relevant data to make wind turbines more efficient.\nAnd yet arguments on how AI could hypothetically end humanity as we know it at some point in the future, seem to drown out how AI is already helping humanity’s cause. Such contributions pale in comparison to the very real possibility that AI could be the key to finding the answers for some of humanity’s biggest problems.\nAt a 2018 hearing at the US House of Representatives Committee on Science, Space and Technology, Stanford Professor Fei-Fei Li said that AI is bound to alter the human experience, and not necessarily for the better. After acknowledging this, Li pointed out that ensuring AI will transform the world for the better is very much in our hands. \u0026ldquo;There\u0026rsquo;s nothing artificial about AI\u0026rdquo;, she said.\n \u0026ldquo;It’s inspired by people; it’s created by people, and—most importantly—it impacts people. It is a powerful tool we are only just beginning to understand, and that is a profound responsibility.\u0026rdquo;\n Concern that AI could pose a threat is very much reasonable. But this possibility is just that - a possibility. Let’s not forget AI is a fundamental opportunity to further the cause of the human civilisation, and that it is in our hands to ensure that it impacts us positively, and be optimistic about what it can mean for us. The world can only be the better place for it.\n","date":1566061379,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1566061379,"objectID":"a75ccec893290ac194456b20ef62ffa9","permalink":"https://vihanga.github.io/post/blog/AI-Oppurtunity-not-danger/","publishdate":"2019-08-17T17:02:59Z","relpermalink":"/post/blog/AI-Oppurtunity-not-danger/","section":"post","summary":"Originally published on RTÉ Brainstorm\nThe solar system is full of debris and rocks floating around. A meteor shower or a shooting star is debris burning up upon entry to the atmosphere. On occasion, pieces of rock make it all the way through. A much larger rock that could make its way through is an asteroid. About 66 million years ago, the impact from an asteroid brought about the Cretaceous extinction.","tags":["blog"],"title":"Why AI is an opportunity rather than a danger","type":"post"},{"authors":["Vihanga Gamage"],"categories":[],"content":"\u0026lt;\u0026lt; restricted access to posting \u0026gt;\u0026gt;\n","date":1542474179,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1542474179,"objectID":"8ea6b228c1d9f27f638832de44099fc4","permalink":"https://vihanga.github.io/post/research/MIG18PaperStudy/","publishdate":"2018-11-17T17:02:59Z","relpermalink":"/post/research/MIG18PaperStudy/","section":"post","summary":"\u0026lt;\u0026lt; restricted access to posting \u0026gt;\u0026gt;","tags":["research"],"title":"A pilot study into virtual character application as pedagogical agents","type":"post"},{"authors":["Vihanga Gamage"],"categories":[],"content":"\u0026lt;\u0026lt; restricted access to posting \u0026gt;\u0026gt;\n","date":1542474179,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1542474179,"objectID":"e46c42befa06751c790f5912e50fc00e","permalink":"https://vihanga.github.io/post/blog/MIG18blog/","publishdate":"2018-11-17T17:02:59Z","relpermalink":"/post/blog/MIG18blog/","section":"post","summary":"\u0026lt;\u0026lt; restricted access to posting \u0026gt;\u0026gt;","tags":["blog"],"title":"Motion, Interaction and Games 2018","type":"post"},{"authors":["Vihanga Gamage"],"categories":[],"content":"   After the horrible start to the season I had at the City of Dublin in September, I was desperate to turn my form and results around. I had signed up to play in as much fixtures as I can for the club\u0026rsquo;s second team who plays in the third division of the Leinster Chess Union leagues, and played in all three opening fixtures.\nAnd will you look at that, the results are quite alright.\n  Funnily enough, I seem to have played three consecutive games with the black pieces due to playing on three different boards, and the team did alright especially given our standing on joint second after 2 wins and a narrow loss to one of the highest rated teams in the division.\n  Match 1 : Opening game vs Elm Mount - A very lucky escape. Very lucky indeed. This was a bit of a weird one. I was playing black, on board 5, fairly confident, decent position coming out of the opening, and I was in a mood that day. So, when it looked like there was potential to sacrifice a piece on h3, I was fairly focussed on this.\nAt this, position, I was so preoccupied whether to take h3 with the bishop on c8 or knight on f4, I didn\u0026rsquo;t notice that my opponents last move, Nf1, opens an attack from the bishop on c1 to my knight on f4.\n After a lot of deliberation in my head, I took h3 with the Bishop, and I didn\u0026rsquo;t notice my blunder till my opponent took the piece on f4.\nAfter that blunder, I thought my best chance of recovering was to capitalize on whatever little attacking potential I had in this position, and I forged ahead with my plan to put his castle under pressure. My rationale here was that it was easier for me to get both my rooks involved in an assault on his castle than it was for him to make his rook on a1 active, and maybe I might be able to overwhelm his pieces.\nAnd as luck would have it, that\u0026rsquo;s just what happened.\n Presumably, due to the perceived threat from f4 that would make open the file for my rook on f8 and further weaken his king, he made the move Qd2.\n..Nxg2, he responded with Nf1, again, as what I presume was a hail mary move in the hopes I might overlook a threat on that diagonal, again. But I didn\u0026rsquo;t, he resigned after my next move Nf4+ as the only continuation lead to either losing his queen or checkmate.\n I was glad I got away with the win after that blunder, the team got a 4-2 win in the opening fixture. We went to the pub afterwards, played ping pong while having pints, and watched Man United pull off that awesome 3-2 comeback win against Newcastle.\nOverall, it was an alright day, but it could have just gone very, very, wrong, for myself, just as well as for United and Jose Mourinho.\nMatch 2 : Converting an advantage with surprising efficiency Next up was a game against Lucan, all the other 5 games had been played the week before; my board had been deferred as an old friend was visiting me the day of the match. The match was balanced on 2.5-2.5 with 5 games played, and I was playing their captain on board 4.\nThe opening was nothing special, left a nice evenly balanced position coming into the midgame. \nMy move here was to castle, rather than the engine suggested move of capturing the e4 pawn, but the open file was what lead to my advantage, given my opponent sought to make a rash advance with his rook to apply pressure on my castle.\n But in this position, ...Nc5. Qc3, Ne6. R7xe6,Bxe6. And I was up a rook to a bishop. It had been a recent pattern of mine to play out some fidgety plans leading to drawn positions even when I had the advantage, or even devolve into a loss, but I played through to a winning endgame in what was, on recent form, uncharacteristically surprising efficiency.  It would\u0026rsquo;ve been nicer if I had spotted the forced mating sequence here:\n\u0026hellip;e4+. Kf4, Rgf2+. Kg4, Rf3. Rf7+, Kxf7. Kg5, Rg2+. Kh6, Rh3. c7, Rxh4#\nBut I had less than a minute on the clock, so I was playing on increments, so it\u0026rsquo;s entirely justifiable that I did things the old fashioned way.\n My opponent ran out of time here, when I was winning quite comfortably; and the team scored a 3.5-2.5 win.\nMatch 3 : A fast 15 move draw It was a Monday evening, I was again playing a couple of days after the match, one of two differred boards. I had just flew back to Dublin the evening before from the MIG\u0026rsquo;18 conference in Cyprus.\nI had popped into a pub near the playing venue to get a pint, and opened my Macbook to finish up some emails when I see a window with my Twitter home feed showing that Stan Lee had died.\nI really wasn\u0026rsquo;t in the mood to play a game after seeing this, 10 minutes before the start time. I went in, my oppoennet played the Polish opening, that was a first. And I was more than happy to play a line that lead to a draw.\n Looking at the position, it can be said that I was let off, as my isolated pawn and overall position isn\u0026rsquo;t likely to lead to a whole lot of joy. My move here would\u0026rsquo;ve been Qd6 or Qc7, and it wouldn\u0026rsquo;t have been the hardest to defend here. Qd6 might even allow for some attacking momentum if I can get my knight involved. My opponent seemed to agree, and it seemed he had studied the opening and likely positions pretty thoroughly; also the engine concurred with this analysis.\nBut, as my opponent said, we could\u0026rsquo;ve played for a couple of hours and ended up drawn anyway, or agreed a draw early on that basis and gone home early.\nWe lost the game 2.5-3.5, but given that league standings are based on game points rather than match points, it\u0026rsquo;s not the end of the world. We are tied second on points in the league standings, we seem quite competitive for a newly promoted team with an average rating that is on the latter end in the league.\nIf we can keep this up, come April, the team could be in an interesting position.\n  And, oh hello, a 55 point boost to my FIDE rating. This was a nice surprise, thanks to the K-Factor of 40, as it\u0026rsquo;s less than 30 of my games were FIDE rated.\n  I had initially planned to play in the Kilkenny Weekender, where I played my first competitive chess since moving to Ireland, for the third consecutive year. But with the fatigue from the travel and the conference and with some heavy weeks ahead of me at work, it seemed the right idea to sit this year out.\nI\u0026rsquo;m likely to play at 2 more games in 2018, with our last Ennis shield fixture again Skerries, and a substitute appearance for the first time this season in the Armstrong Cup for the first team.\nMy form seems to be improving after the blunders in the City of Dublin weekender and the less than ideal play early in the first league match. This is a good sign for my potential performance at the 9 round closed all-play-all in early January, that I entered for.\n","date":1542474179,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1542474179,"objectID":"8a6c13decec0c37ebef0cef29eb5a232","permalink":"https://vihanga.github.io/post/chess/2018LeagueEnnisR123/","publishdate":"2018-11-17T17:02:59Z","relpermalink":"/post/chess/2018LeagueEnnisR123/","section":"post","summary":"After the horrible start to the season I had at the City of Dublin in September, I was desperate to turn my form and results around. I had signed up to play in as much fixtures as I can for the club\u0026rsquo;s second team who plays in the third division of the Leinster Chess Union leagues, and played in all three opening fixtures.\nAnd will you look at that, the results are quite alright.","tags":["chess"],"title":"The form awakens* (*seems to stir awake from a 4 month slumber) - 2.5/3 : Rounds 1, 2 \u0026 3 of the Ennis Shield (Leinster League Division 3)","type":"post"},{"authors":["Vihanga Gamage, Cathy Ennis"],"categories":null,"content":"I presented my first conference paper at the 11th Motion, Interactions and Games conference held in November 2018. In this paper, we examined the broad effects a virtual character can have on user experience and performance when incorporated as a pedagogical agent in a serious game. We also explored character appearance personalization as a potential way of heightening engagement with the character.\nOur findings can be summaries as follows: - A user is more engaged with a lesson when delivered through a virtual character compared to a non-character control version. - A user retains knowledge better when a lesson is delivered through a virtual character compared to a non-character control version. - There is a propensity for the majority of user attention to be directed at the virtual character, resulting in little attention being paid to the rest of the game environment. - Surprising, character appearance personalization has an effect opposite to what we expected, resulting in participants being significantly less engaged with a personalized character compare to one with a default appearance.\nSupplementary material in the form of the full questionnaire used in the study and free-form user feedback and some interesting points raised by attendees at the conference following my thoughts can be found here, and a blog post about my experience at the conference and some photographs can be found here.\n","date":1542473248,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1542473248,"objectID":"c6965129ead2fdcac70fc04d9f02f39e","permalink":"https://vihanga.github.io/publication/MIG18/","publishdate":"2018-11-17T16:47:28Z","relpermalink":"/publication/MIG18/","section":"publication","summary":"Virtual characters have been employed for many purposes including interacting with players of serious games, with a purpose to increase engagement. These characters are often embodied conversational agents playing diverse roles, such as demonstrators, guides, teachers or interviewers. Recently, much research has been conducted into properties that affect the realism and plausibility of virtual characters, but it is less clear whether the inclusion of interactive agents in serious applications can enhance a user's engagement with the application, or indeed increase efficacy. In a first step towards answering these questions, we conducted a study where a Virtual Learning Environment was used to examine the effect of employing a virtual character to deliver a lesson. In order to investigate whether increased familiarity between the player and the character would help achieve learning outcomes, we allowed participants to customize the physical appearance of the character. We used direct and indirect measures to assess engagement and learning; we measured knowledge retention to ascertain learning via a test at the end of the lesson, and also measured participants' perceived engagement with the lesson. Our findings show that a virtual character can be an effective learning aid, causing heightened engagement and retention of knowledge. However, allowing participants to customize character appearance resulted in inhibited engagement, which was contrary to what we expected.","tags":[],"title":"Examining the effects of a virtual character on learning and engagement in serious games","type":"publication"},{"authors":["Vihanga Gamage"],"categories":[],"content":"\u0026lt;\u0026lt; restricted access to posting \u0026gt;\u0026gt;\u0026hellip;.\n","date":1542128579,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1542128579,"objectID":"70e8f7b753362d5f38eacdfd83b8feae","permalink":"https://vihanga.github.io/post/blog/StanLee/","publishdate":"2018-11-13T17:02:59Z","relpermalink":"/post/blog/StanLee/","section":"post","summary":"\u0026lt;\u0026lt; restricted access to posting \u0026gt;\u0026gt;\u0026hellip;.","tags":["blog"],"title":"Excelsior!","type":"post"},{"authors":["Vihanga Gamage"],"categories":[],"content":" \u0026ldquo;My soul is painted like the wings of butterflies, fairy tales of yesterday will grow but never die. I can fly, my friends\u0026rdquo;\nThe Show Must Go On, Queen (1991)\u0026rdquo;\n \u0026lt;\u0026lt; restricted access to posting \u0026gt;\u0026gt;\n","date":1540832579,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1540832579,"objectID":"26178ba6ce0c35ef14bd61c03b23aa50","permalink":"https://vihanga.github.io/post/blog/BohemianRhapsody/","publishdate":"2018-10-29T17:02:59Z","relpermalink":"/post/blog/BohemianRhapsody/","section":"post","summary":"\u0026ldquo;My soul is painted like the wings of butterflies, fairy tales of yesterday will grow but never die. I can fly, my friends\u0026rdquo;\nThe Show Must Go On, Queen (1991)\u0026rdquo;\n \u0026lt;\u0026lt; restricted access to posting \u0026gt;\u0026gt;","tags":["blog"],"title":"\"....fairy tales of yesterday will grow but never die.\" , My thoughts on Bohemian Rhapsody, the Queen biopic","type":"post"},{"authors":["Vihanga Gamage"],"categories":[],"content":"  The 2018 / 2019 ICU season kicked off at the City of Dublin weekender held in September. This season was one I approached with as much intent to perform well, as I had done in a while. This is because I will be taking a step back from playing relatively seriously, at the end of the season, for about a year to focus on the last year of my PhD.\nDespite my showing at the Irish Championship Supporting events in August not being as convincing as I had hoped after a few good performances earlier in the year, I was at least happy with the fact that I had consolidated the 180 point rating jump I achieved at the Ennis Open in May.\nI was seeded in the top 10 of the draw in my section. The first round I was playing back against a solid player. After a fairly even middle game phase, I managed to capitalise on my opponent\u0026rsquo;s misstep to go into the end game with a winning position.\nand the stage was set\u0026hellip;\u0026hellip;for a trademark pawn ending screw up from me. This has been happening a lot recently.\nIn my defence, I had studied up on pawn ending theory that I was continually getting wrong, and I had a couple of minutes plus increments on the clock, but this was pretty much inexcusable.\nIn this position, it was my move, and instead of c5, I played Bxc3 leading the game to a dead draw.\n Bxc3, Bxc3 Kxc3, Ke3 Kb3, Kxe4 Kxa3, f4\nand hold on to your hats people because here comes the height of lunacy. Instead of calculating properly and making c4, Kxb4 ??????\nand that was it. First game of the season, arguably the worst game I played since February.\n The next morning, I picked up right where I left off, a toothless game and a dumb theoretical blunder towards the end.\nTwo games, both against players rated about 200 points beyond me, my rating had already dropped by 50.\nIt did not seem like I could get out of the rut I was in, and a tailspin seemed imminent, so to avoid a dropping an even hundred more rating points, I withdrew at this point.\nMy next competitive game is likely going to be the first match round of the new league season. I will be playing the full league campaign for the Dublin University Ennis Shield team; here\u0026rsquo;s hoping I plug the holes in my endgame and mid-game play isn\u0026rsquo;t so flat.\n","date":1539795779,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1539795779,"objectID":"1b2d65a0738b96dd92c771c08112086e","permalink":"https://vihanga.github.io/post/chess/2018CoD/","publishdate":"2018-10-17T17:02:59Z","relpermalink":"/post/chess/2018CoD/","section":"post","summary":"The 2018 / 2019 ICU season kicked off at the City of Dublin weekender held in September. This season was one I approached with as much intent to perform well, as I had done in a while. This is because I will be taking a step back from playing relatively seriously, at the end of the season, for about a year to focus on the last year of my PhD.","tags":["chess"],"title":"A calamitous start to the season - 0/2 at the City of Dublin 2018","type":"post"},{"authors":null,"categories":null,"content":" Vihanga Gamage\u0026rsquo;s Public Calendar  ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"a631103cf606bbead447236a29c6889e","permalink":"https://vihanga.github.io/calendar/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/calendar/","section":"","summary":" Vihanga Gamage\u0026rsquo;s Public Calendar  ","tags":null,"title":"","type":"page"}]