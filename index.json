[{"authors":null,"categories":null,"content":" Introduction This thesis tackles a fundamental challenge in computer animation: how can we create virtual characters that move naturally and responsively without the prohibitive costs of motion capture?\nThe Problem Traditional character animation relies heavily on motion capture - an expensive, time-consuming process that requires recording human actors for every possible behavior variation. While this produces high-quality results, it\u0026rsquo;s simply not scalable for interactive applications where characters need to respond dynamically to user input or changing scenarios.\nMeanwhile, existing alternatives have significant limitations. Supervised learning methods can generate animations but lack flexibility. Physics-based reinforcement learning works brilliantly for athletic movements but fails for social behaviors like gestures - there\u0026rsquo;s no physics simulation to tell us if a wave looks \u0026ldquo;friendly\u0026rdquo; or a gesture feels \u0026ldquo;natural.\u0026rdquo;\nMy Approach I developed RLAnimate, a framework that fundamentally rethinks how we approach character animation. Instead of using reward functions (the traditional RL approach), I use motion capture data as learning objectives. This allows agents to learn what \u0026ldquo;human-like\u0026rdquo; means directly from examples, while maintaining the flexibility and responsiveness that makes RL powerful.\nThe key insight is treating animation as a model-based reinforcement learning problem where agents learn the dynamics of human movement. By understanding how joint rotations create behaviors, agents can generate novel animations that look natural while adapting to changing requirements in real-time.\nKey Contributions  A new paradigm for animation RL - Using motion capture data as objectives rather than rewards Multi-behavior agents - Single agents that can wave, point, and generate conversational gestures Technical innovations - Quaternion neural networks, realism regularization, and hierarchical dynamics models Proven results - Animations that are statistically indistinguishable from human motion capture  Research Questions The thesis systematically addresses four key questions: - Can RL work for animation without physics? (Yes - through latent dynamics learning) - Can we make it human-like? (Yes - using motion capture as objectives) - How do we handle complexity like fingers? (Quaternions + dedicated animation dynamics) - Can it work for conversational gestures? (Yes - with realism regularization)\nThesis Journey The work progresses from establishing basic feasibility (Chapter 4) through developing the core RLAnimate methodology (Chapter 5), extending to complex movements with fingers (Chapter 6), and culminating in conversational beat gestures that match human quality (Chapter 7).\n","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"981946ce109800ca64235ec2433b470c","permalink":"https://vihanga.github.io/thesis/chapters/introduction/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/thesis/chapters/introduction/","section":"thesis","summary":"Introduction This thesis tackles a fundamental challenge in computer animation: how can we create virtual characters that move naturally and responsively without the prohibitive costs of motion capture?\nThe Problem Traditional character animation relies heavily on motion capture - an expensive, time-consuming process that requires recording human actors for every possible behavior variation. While this produces high-quality results, it\u0026rsquo;s simply not scalable for interactive applications where characters need to respond dynamically to user input or changing scenarios.","tags":null,"title":"Chapter 1: Introduction","type":"thesis"},{"authors":null,"categories":null,"content":" Virtual Character Animation and Perception This chapter establishes the foundation for understanding virtual character animation - both the technical challenges and the perceptual requirements that make animation \u0026ldquo;feel right\u0026rdquo; to human viewers.\nThe Animation Pipeline Creating believable virtual characters isn\u0026rsquo;t just about making them move - it\u0026rsquo;s a complex pipeline that starts with 3D modeling and ends with real-time rendering. I explore how animation data flows through this pipeline, from motion capture or procedural generation to the final pixels on screen. The key insight? Each step introduces opportunities for both quality improvements and computational optimizations.\nWhy Perception Matters Here\u0026rsquo;s what most animation systems miss: humans are incredibly sensitive to movement patterns. We can spot \u0026ldquo;unnatural\u0026rdquo; motion in milliseconds, even if we can\u0026rsquo;t articulate why. I dive into the perceptual studies that reveal what makes animation feel human-like - from timing and smoothness to the subtle coordination between body parts.\nThis understanding drives a crucial design decision in my work: animation quality isn\u0026rsquo;t just about technical metrics like joint angles or velocities. It\u0026rsquo;s about matching the statistical patterns of human movement that our brains have evolved to recognize.\nCurrent Approaches and Their Limits The field has tried three main approaches:\nMotion Graphs and Blending: Works well for predetermined scenarios but lacks flexibility. You need exponentially more data as behaviors become more complex.\nNeural Networks: Can generate smooth animations but often produce \u0026ldquo;average\u0026rdquo; motions that lack character and specificity. They struggle with rare behaviors and edge cases.\nPhysics-based RL: Produces incredibly athletic movements - backflips, martial arts, parkour. But try to make a physics-based agent wave \u0026ldquo;friendly\u0026rdquo; and you\u0026rsquo;ll see the problem. There\u0026rsquo;s no physics equation for social appropriateness.\nThe Gap This Thesis Fills What\u0026rsquo;s missing is a method that combines the flexibility of RL with the quality of motion capture data, without requiring physics simulation. That\u0026rsquo;s exactly what RLAnimate provides - and understanding this background shows why that combination is so powerful.\nThe chapter sets up the key tension: we want animation that\u0026rsquo;s both high-quality (matching human perception) and flexible (responding to dynamic scenarios). Traditional methods excel at one or the other, but not both. This creates the perfect motivation for the model-based RL approach I develop in the following chapters.\n","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"9358d9b2e86d81f55a8f6a890e4d78f6","permalink":"https://vihanga.github.io/thesis/chapters/data-driven-animation/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/thesis/chapters/data-driven-animation/","section":"thesis","summary":"Virtual Character Animation and Perception This chapter establishes the foundation for understanding virtual character animation - both the technical challenges and the perceptual requirements that make animation \u0026ldquo;feel right\u0026rdquo; to human viewers.\nThe Animation Pipeline Creating believable virtual characters isn\u0026rsquo;t just about making them move - it\u0026rsquo;s a complex pipeline that starts with 3D modeling and ends with real-time rendering. I explore how animation data flows through this pipeline, from motion capture or procedural generation to the final pixels on screen.","tags":null,"title":"Chapter 2: Virtual Character Animation and Perception","type":"thesis"},{"authors":null,"categories":null,"content":" Model-based Reinforcement Learning This chapter provides the theoretical foundation for understanding how model-based RL can revolutionize character animation. I explain why learning a model of the world enables more sample-efficient and generalizable agents compared to traditional approaches.\nThe Power of World Models Most RL systems are model-free - they learn through trial and error what actions lead to good outcomes. That works great if you have millions of attempts to practice. But what if you need to learn from limited data? That\u0026rsquo;s where model-based RL shines.\nBy learning a model of how the world works - in our case, how human bodies move - agents can plan ahead, imagine consequences, and generalize from limited examples. It\u0026rsquo;s the difference between memorizing dance moves and understanding the principles of movement.\nLatent Dynamics: The Secret Sauce Here\u0026rsquo;s the key insight that makes RLAnimate work: we don\u0026rsquo;t need to model every muscle and bone. Instead, we learn compact \u0026ldquo;latent\u0026rdquo; representations that capture the essence of movement patterns.\nI introduce the mathematical framework of latent dynamics models, showing how they compress high-dimensional movement data into manageable representations while preserving the critical information for generating natural motion. This is what allows our agents to run at 5ms per frame - we\u0026rsquo;re working in an efficient latent space, not raw joint angles.\nRepresentation Learning and Realism The chapter explores how to learn good representations for animation. Not all latent spaces are created equal - some preserve the nuances that make motion feel human, others lose them. I examine techniques from variational autoencoders to adversarial learning, setting up the theoretical foundation for the realism regularization used in later chapters.\nWhy Model-based for Animation? The chapter concludes by connecting these abstract concepts to character animation:\n Sample Efficiency: Learn new behaviors from just a few motion capture examples Generalization: Interpolate and extrapolate beyond the training data Planning: Generate coherent long-term motion sequences Speed: Efficient latent representations enable real-time performance  This theoretical grounding is essential for understanding why RLAnimate succeeds where other methods fail. Model-based RL isn\u0026rsquo;t just another technique - it\u0026rsquo;s fundamentally the right abstraction for the animation problem.\n","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"752bb61cd070490e6fd7dd8b8fad4585","permalink":"https://vihanga.github.io/thesis/chapters/model-based-rl/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/thesis/chapters/model-based-rl/","section":"thesis","summary":"Model-based Reinforcement Learning This chapter provides the theoretical foundation for understanding how model-based RL can revolutionize character animation. I explain why learning a model of the world enables more sample-efficient and generalizable agents compared to traditional approaches.\nThe Power of World Models Most RL systems are model-free - they learn through trial and error what actions lead to good outcomes. That works great if you have millions of attempts to practice.","tags":null,"title":"Chapter 3: Model-based Reinforcement Learning","type":"thesis"},{"authors":null,"categories":null,"content":" Model-based Character Animation This chapter presents exploratory work establishing the feasibility of applying reinforcement learning to dynamic character animation tasks without reliance on physics simulation. The work addresses a fundamental research question: can reinforcement learning effectively control character animation for portraying social behaviours?\nMotivation and Objectives Previous work in RL-based animation has relied heavily on physics signals for training, rendering such approaches unsuitable for social behaviours where physics feedback is absent or irrelevant. This chapter establishes a methodology for training agents that achieve three key objectives:\n Agent functionality relies only on signals relevant to all animation sequences, regardless of the behaviour being portrayed Multiple types of behaviours can be portrayed by a single agent instance using shared dynamics models Animation generation occurs dynamically on a frame-by-frame basis, enabling real-time flexibility  Experimental Approach The investigation begins with model-free reinforcement learning experiments, following the precedent set by physics-based methods. These initial experiments reveal limitations that motivate the exploration of model-based approaches, which offer the robust sample efficiency demonstrated in supervised learning applications for character animation.\nThe experiments utilise star jump animations as a test case - a behaviour that requires coordination across multiple joints while remaining sufficiently constrained for systematic analysis. This choice enables focused investigation of the core technical challenges without the confounding factors present in more complex social behaviours.\nModel-based Learning Results The model-based approach demonstrates several advantages over model-free alternatives:\n Sample efficiency: Effective learning from limited motion capture data (5 minutes) Motion quality: Generated animations maintain temporal coherence and naturalness Computational efficiency: Real-time performance with inference times under 5ms per frame Implicit regularisation: The learned dynamics model constrains generated motion to remain within the manifold of human movement  Key Contributions This chapter establishes that model-based reinforcement learning provides a viable paradigm for character animation without physics simulation. The learned latent dynamics models effectively capture movement patterns from motion capture data, enabling agents to generate novel animations while maintaining human-like qualities.\nThe primary contribution is the establishment of a basic framework that will be extended in subsequent chapters. This work was published in \u0026ldquo;Learned Dynamics Models and Online Planning for Model-Based Animation Agents\u0026rdquo; at the KES Agents and Multi-Agent Systems 2021 conference.\nLimitations and Future Directions While successful within its scope, this initial work reveals several limitations that motivate subsequent developments: - Single-behaviour agents lack the flexibility required for interactive applications - Absence of interaction with physical environments limits applicability - The framework requires extension to handle more complex social behaviours\nThese limitations directly inform the development of RLAnimate in Chapter 5, which addresses these constraints through a more sophisticated approach to behaviour representation and multi-task learning.\n","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"d9348a507e32dce7a3031f613bbe5fa6","permalink":"https://vihanga.github.io/thesis/chapters/model-based-animation/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/thesis/chapters/model-based-animation/","section":"thesis","summary":"Model-based Character Animation This chapter presents exploratory work establishing the feasibility of applying reinforcement learning to dynamic character animation tasks without reliance on physics simulation. The work addresses a fundamental research question: can reinforcement learning effectively control character animation for portraying social behaviours?\nMotivation and Objectives Previous work in RL-based animation has relied heavily on physics signals for training, rendering such approaches unsuitable for social behaviours where physics feedback is absent or irrelevant.","tags":null,"title":"Chapter 4: Model-based Character Animation","type":"thesis"},{"authors":null,"categories":null,"content":" RLAnimate: Data-driven Reinforcement Learning for Character Animation This chapter introduces RLAnimate, a novel framework that fundamentally reconceptualises reinforcement learning for character animation. Rather than relying on hand-crafted reward functions, RLAnimate utilises motion capture data as learning objectives, addressing the challenge of articulating human-like movement mathematically.\nTheoretical Foundation The central insight of RLAnimate lies in reformulating the animation problem: instead of defining rewards for desired outcomes, the framework learns to match the dynamics of human movement. This approach acknowledges that while defining \u0026ldquo;wave friendly\u0026rdquo; or \u0026ldquo;point naturally\u0026rdquo; through equations proves intractable, motion capture data implicitly contains this information.\nThe framework addresses the research question: \u0026ldquo;To what extent can a RL-based algorithm for animation control be trained to generate human-like social behaviours?\u0026rdquo; The answer, as demonstrated through extensive evaluation, is that data-driven objectives enable generation of animations that match human motion capture in both quantitative metrics and perceptual studies.\nThe A1 Architecture The initial RLAnimate architecture (A1) establishes the core components:\n Behaviour encoders: Learn representations of intended behaviours from motion capture exemplars Perceptual encoders: Process current character state into task-relevant features Latent dynamics models: Capture temporal evolution of human movement patterns Multi-behaviour support: Enable single agents to perform multiple distinct behaviours  This architecture demonstrates that motion capture data can serve as sufficient supervision for learning complex animation controllers without explicit reward engineering.\nMulti-Behaviour Learning A key contribution is the framework\u0026rsquo;s ability to train single agents capable of multiple behaviours. This contrasts with previous approaches requiring separate models for each behaviour type. The shared latent dynamics capture common principles of human movement, while behaviour-specific encoders provide the necessary specialisation.\nEmpirical results demonstrate: - 93% behaviour accuracy: Agents correctly perform requested behaviours - 5ms inference time: Suitable for real-time interactive applications - Statistical motion quality: Generated animations match human motion capture statistics - Data efficiency: Effective learning from minutes rather than hours of motion capture\nThe A2 Architecture: Physical Grounding The A2 variant introduces physical grounding while maintaining the data-driven approach:\n Contact modelling: Explicit representation of foot-ground contacts Momentum consistency: Conservation principles integrated into dynamics learning Object interaction: Framework for manipulating virtual objects Maintained ease of training: Physical constraints learned from data rather than hard-coded  Methodological Contributions RLAnimate contributes several methodological innovations:\n Idealness formulation: Motion capture defines ideal actions at each timestep, eliminating reward function design Latent planning: Efficient trajectory optimisation in learned representation spaces Behaviour composability: Architecture supporting seamless transitions between behaviours Sample efficiency: Orders of magnitude reduction in required training data compared to model-free approaches  Publications and Impact The work presented in this chapter resulted in two peer-reviewed publications: - \u0026ldquo;Data-driven reinforcement learning for virtual character animation control\u0026rdquo; (ALA Workshop, AAMAS 2021) - \u0026ldquo;Latent Dynamics for Artefact-Free Character Animation via Data-Driven Reinforcement Learning\u0026rdquo; (ICANN 2021)\nSignificance RLAnimate establishes a new paradigm for animation synthesis, demonstrating that reinforcement learning can produce human-quality animation without physics simulation or reward engineering. By treating animation as matching human movement dynamics rather than optimising explicit objectives, the framework achieves:\n Natural quality without manual tuning Multi-behaviour flexibility in single agents Practical training requirements Real-time performance suitable for deployment  This foundation enables the subsequent developments in finger animation (Chapter 6) and conversational gestures (Chapter 7), validating the extensibility and robustness of the data-driven approach.\n","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"4caa4bfc750979cddab76cb7e35516cf","permalink":"https://vihanga.github.io/thesis/chapters/data-driven-rl/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/thesis/chapters/data-driven-rl/","section":"thesis","summary":"RLAnimate: Data-driven Reinforcement Learning for Character Animation This chapter introduces RLAnimate, a novel framework that fundamentally reconceptualises reinforcement learning for character animation. Rather than relying on hand-crafted reward functions, RLAnimate utilises motion capture data as learning objectives, addressing the challenge of articulating human-like movement mathematically.\nTheoretical Foundation The central insight of RLAnimate lies in reformulating the animation problem: instead of defining rewards for desired outcomes, the framework learns to match the dynamics of human movement.","tags":null,"title":"Chapter 5: RLAnimate - Data-driven RL for Character Animation","type":"thesis"},{"authors":null,"categories":null,"content":" Quaternions and Finger Animation This chapter addresses the challenges of extending RLAnimate to accommodate fine-grained articulations, specifically finger animation. The inclusion of an additional thirty joints for finger control revealed limitations in the original architecture, necessitating fundamental modifications to both the representation and dynamics modelling approaches.\nTechnical Challenges in Finger Animation The incorporation of finger movements presents unique challenges for animation synthesis:\n Increased dimensionality: Addition of 30+ joints substantially expands the state and action spaces Hierarchical dependencies: Finger movements operate within the broader kinematic structure of arm and torso motion Subtle articulations: Small inaccuracies in finger positions produce visually apparent artefacts Representational singularities: Euler angle representations suffer from gimbal lock, particularly problematic for the complex rotations required in hand animation  These challenges necessitated architectural innovations to maintain animation quality while managing the increased complexity.\nQuaternion Representations for Robust Animation The transition from Euler angles to quaternion representations addresses fundamental limitations in rotation parameterisation:\n Singularity-free representation: Quaternions eliminate gimbal lock, ensuring continuous rotation spaces Compact encoding: Four parameters provide complete orientation information versus nine for rotation matrices Smooth interpolation: Unit quaternion interpolation naturally produces intermediate orientations Stable gradients: The quaternion manifold provides well-behaved gradients for neural network training  A novel quaternion activation function ensures all network outputs remain valid unit quaternions, eliminating a class of animation artefacts at the architectural level.\nHierarchical Dynamics Architecture The A3 architecture introduces a dedicated animation dynamics module that captures the hierarchical nature of human movement:\nSeparated dynamics modelling: Physical dynamics (body movement) and animation dynamics (detailed articulations) are modelled separately, acknowledging their different statistical properties and control requirements.\nHierarchical state propagation: The animation dynamics module receives information from physical dynamics, enabling finger movements to respond appropriately to arm and body motion.\nStochastic components: Natural variation in finger movements is captured through learned stochastic elements, preventing robotic uniformity in generated animations.\nEmpirical Validation Evaluation on waving and pointing behaviours augmented with finger animation demonstrates:\n Quantitative improvements: 5.34 point increase in animation similarity scores compared to A2 baseline Perceptual quality: All velocity errors remain below the 0.4 threshold for human perception Computational efficiency: Maintained 5ms inference time despite 30+ additional joints Visual fidelity: Natural finger curls during waving and appropriate hand shapes during pointing  Contributions and Implications This chapter\u0026rsquo;s primary contributions include:\n Quaternion neural networks for animation: A complete framework for quaternion-based rotation learning in animation contexts Hierarchical latent dynamics: Architectural patterns for modelling nested movement dependencies Scalability demonstration: Evidence that the RLAnimate framework can accommodate substantial increases in animation complexity  The successful incorporation of finger animation validates the extensibility of the RLAnimate framework and establishes architectural patterns for handling fine-grained articulations. These developments prove essential for the subsequent challenge of conversational gesture generation, where subtle hand movements convey significant communicative content.\n","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"b010e1faa4d04184493b3d5814db0f5e","permalink":"https://vihanga.github.io/thesis/chapters/latent-dynamics/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/thesis/chapters/latent-dynamics/","section":"thesis","summary":"Quaternions and Finger Animation This chapter addresses the challenges of extending RLAnimate to accommodate fine-grained articulations, specifically finger animation. The inclusion of an additional thirty joints for finger control revealed limitations in the original architecture, necessitating fundamental modifications to both the representation and dynamics modelling approaches.\nTechnical Challenges in Finger Animation The incorporation of finger movements presents unique challenges for animation synthesis:\n Increased dimensionality: Addition of 30+ joints substantially expands the state and action spaces Hierarchical dependencies: Finger movements operate within the broader kinematic structure of arm and torso motion Subtle articulations: Small inaccuracies in finger positions produce visually apparent artefacts Representational singularities: Euler angle representations suffer from gimbal lock, particularly problematic for the complex rotations required in hand animation  These challenges necessitated architectural innovations to maintain animation quality while managing the increased complexity.","tags":null,"title":"Chapter 6: Quaternions and Finger Animation","type":"thesis"},{"authors":null,"categories":null,"content":" Conversational Beat Gesture Generation This chapter presents the application of RLAnimate to conversational beat gesture generation, representing the most challenging test of the framework\u0026rsquo;s capabilities. Beat gestures - rhythmic hand movements that accompany speech - lack goal-directed objectives and require precise synchronisation with speech prosody, making them unsuitable for physics-based or reward-engineered approaches.\nThe Challenge of Beat Gesture Synthesis Beat gestures present unique challenges that distinguish them from previously addressed behaviours:\n Absence of explicit objectives: Unlike pointing or waving, beat gestures have no target or completion criteria Speech-motion coupling: Gestures must synchronise with prosodic features at multiple temporal scales Subjective evaluation: Quality assessment relies entirely on human perception of naturalness High variability: The same utterance permits multiple valid gesture realisations  These characteristics necessitate a fundamental rethinking of how reinforcement learning can be applied to animation synthesis.\nRealism Regularisation Framework The A4 architecture introduces a comprehensive realism regularisation framework comprising four complementary components:\n Rotation regularisation: Constrains joint configurations to anatomically plausible ranges Physics regularisation: Ensures conservation of momentum and energy consistency Smoothness regularisation: Penalises high-frequency jitter and discontinuous motion Adversarial regularisation: Learned discriminator distinguishing generated from human motion  The synergistic combination of these regularisation methods creates a robust prior for human-like movement, addressing the challenge of defining \u0026ldquo;natural\u0026rdquo; motion mathematically.\nSpeech-Motion Architecture The hierarchical speech encoder processes acoustic features at multiple temporal resolutions:\n Phoneme-level processing: Captures fine-grained articulation timing Word-level aggregation: Identifies stress and emphasis patterns Phrase-level context: Maintains coherent gesture sequences across utterances  This multi-scale processing enables the generation of gestures that respond appropriately to both local prosodic variations and global speech patterns.\nPerceptual Validation Study A rigorous perceptual evaluation with 24 participants viewing 480 video clips provides definitive validation:\nStatistical parity with human motion: No significant difference between RLAnimate and motion capture across three evaluation criteria (p \u0026gt; 0.31 for all measures)\nSuperiority over state-of-the-art: Significant improvements over Gesticulator, the leading supervised learning approach: - Human-likeness: 4.33 vs 3.19 (p \u0026lt; 0.001) - Speech reflection: 4.12 vs 3.42 (p \u0026lt; 0.001) - Synchronisation: 4.48 vs 3.60 (p \u0026lt; 0.001)\nQualitative assessment: Participants could not reliably distinguish RLAnimate-generated gestures from recorded human motion.\nTechnical Contributions This chapter advances the state of the art through several innovations:\n Realism regularisation: A principled framework for incorporating multiple complementary constraints on generated motion Speech-conditioned generation: Architecture patterns for synchronising movement with acoustic features Stochastic variability: Mechanisms for generating diverse but appropriate gestures for repeated utterances Perceptual validation methodology: Rigorous experimental design for assessing animation quality  Implications and Significance The successful generation of perceptually indistinguishable beat gestures validates the core thesis proposition: model-based reinforcement learning with motion capture objectives can produce animation quality matching human performance. This achievement has immediate applications in:\n Virtual assistants requiring natural nonverbal communication Educational technology with engaging animated instructors Therapeutic applications demanding believable social presence Entertainment systems with dynamically responsive characters  The work demonstrates that reinforcement learning can address even the most subjective and culturally-dependent aspects of human movement, establishing a new baseline for what constitutes \u0026ldquo;human-like\u0026rdquo; in artificial animation systems.\n","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"b23e04332de28844bae789171e90c6e3","permalink":"https://vihanga.github.io/thesis/chapters/conversational-gestures/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/thesis/chapters/conversational-gestures/","section":"thesis","summary":"Conversational Beat Gesture Generation This chapter presents the application of RLAnimate to conversational beat gesture generation, representing the most challenging test of the framework\u0026rsquo;s capabilities. Beat gestures - rhythmic hand movements that accompany speech - lack goal-directed objectives and require precise synchronisation with speech prosody, making them unsuitable for physics-based or reward-engineered approaches.\nThe Challenge of Beat Gesture Synthesis Beat gestures present unique challenges that distinguish them from previously addressed behaviours:","tags":null,"title":"Chapter 7: Beat Gestures - The Ultimate Test","type":"thesis"},{"authors":null,"categories":null,"content":" Conclusions and Future Impact This thesis fundamentally changes how we think about character animation. By treating animation as a model-based RL problem with motion capture objectives, I\u0026rsquo;ve created a system that produces human-quality movement at real-time speeds. But this is just the beginning.\nWhat We\u0026rsquo;ve Achieved RLAnimate represents several breakthrough achievements:\nHuman-quality animation from RL: For the first time, RL agents generate movement that humans can\u0026rsquo;t distinguish from motion capture. This isn\u0026rsquo;t incremental improvement - it\u0026rsquo;s a paradigm shift.\nReal-time performance: 5ms per frame means these aren\u0026rsquo;t just research demos. They\u0026rsquo;re ready for games, VR, and interactive applications today.\nMulti-behavior flexibility: Single agents that wave, point, and gesture naturally - no behavior-specific engineering required.\nScalability to complexity: From simple waves to 30+ finger joints to speech-synchronized gestures, the approach scales.\nThe Technical Revolution Three key innovations make this possible:\n Motion capture as objectives: Eliminating reward engineering by learning directly from human examples Latent dynamics models: Efficient representations that capture movement essence Realism regularization: Multiple complementary methods ensuring human-like quality  Together, these create a framework that\u0026rsquo;s both theoretically principled and practically effective.\nWhy This Matters Beyond Animation The implications extend far beyond making pretty animations:\nFor AI: We\u0026rsquo;ve shown that complex, subjective human behaviors can be learned without explicit programming. The approach could apply to any domain where we have examples but can\u0026rsquo;t write rules.\nFor HCI: Natural movement is crucial for acceptance of virtual agents. This work enables a new generation of interfaces that communicate through body language.\nFor Science: By learning what makes movement \u0026ldquo;human-like,\u0026rdquo; we\u0026rsquo;re gaining insights into human motor control and social signaling.\nLimitations and Honesty No system is perfect. Current limitations include: - Training requires motion capture data (though much less than alternatives) - Style control is implicit rather than parametric - Physical interactions remain challenging - Cultural gesture variations need more exploration\nThese aren\u0026rsquo;t fundamental barriers - they\u0026rsquo;re the next research challenges.\nThe Road Ahead This thesis opens several exciting directions:\nBehavioral complexity: Extending to full-body social interactions, emotional expressions, and context-aware responses.\nZero-shot generalization: Learning movement principles that transfer across characters and scenarios without retraining.\nIntegration with LLMs: Imagine language models that don\u0026rsquo;t just speak but move naturally as they communicate.\nReal-world robotics: The principles could enable robots that move in ways humans find natural and non-threatening.\nA Personal Vision I believe we\u0026rsquo;re at an inflection point. Just as deep learning revolutionized computer vision, model-based RL with human objectives will revolutionize character animation. We\u0026rsquo;re moving from \u0026ldquo;making characters move\u0026rdquo; to \u0026ldquo;bringing characters to life.\u0026rdquo;\nThe tools are here. The methods work. What we create with them - more engaging games, more effective education, more natural human-computer interaction - is limited only by imagination.\nFinal Thoughts This thesis asked whether RL could create human-like animation without physics simulation or reward engineering. The answer is definitively yes. But more importantly, it\u0026rsquo;s shown a path forward for creating AI systems that capture the subtlety and beauty of human movement.\nVirtual characters that move like us aren\u0026rsquo;t just technically impressive - they\u0026rsquo;re emotionally resonant. They make technology feel more human. In a world increasingly mediated by screens and virtual interactions, that\u0026rsquo;s not just an academic achievement. It\u0026rsquo;s a step toward technology that truly understands and reflects our humanity.\n","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"8c2b9df60fa3dd5ba941a75a5fd50439","permalink":"https://vihanga.github.io/thesis/chapters/conclusion/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/thesis/chapters/conclusion/","section":"thesis","summary":"Conclusions and Future Impact This thesis fundamentally changes how we think about character animation. By treating animation as a model-based RL problem with motion capture objectives, I\u0026rsquo;ve created a system that produces human-quality movement at real-time speeds. But this is just the beginning.\nWhat We\u0026rsquo;ve Achieved RLAnimate represents several breakthrough achievements:\nHuman-quality animation from RL: For the first time, RL agents generate movement that humans can\u0026rsquo;t distinguish from motion capture.","tags":null,"title":"Chapter 8: Conclusions and Future Impact","type":"thesis"},{"authors":null,"categories":null,"content":" Chapter 4: Model-Based Character Animation This page contains supplementary material referenced in Chapter 4 of the thesis, demonstrating the model-based reinforcement learning agents trained to portray gazing and pointing behaviors.\nVideo Demonstrations Model-Based Animation Agents (AMSTA 2021) The following video demonstrates the agents trained using learned dynamics models and online planning for target-driven behaviors:\n  This video shows: - Gazing behavior with dynamic target tracking - Pointing behavior with both left and right arms - Combined gaze and point behaviors - Real-time responsiveness to changing targets - Comparison with inverse kinematics baseline\nKey Results Demonstrated  Computational Efficiency: 5ms per frame (40x faster than IK baseline) Dynamic Flexibility: Agents adapt to moving targets without computational overhead Multi-behavior Support: Single agent architecture supports gaze, point, and combined behaviors Beta Distribution Planning: Novel approach to smooth animation generation  Navigation  Chapter 5 Part A: Model-free RL → ← Back to thesis main page Chapter 4 in main thesis  ","date":1737504000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1737504000,"objectID":"be2d5c51703a1556b5dcc0c8d7a08c43","permalink":"https://vihanga.github.io/thesis/supplementary/c4/","publishdate":"2025-01-22T00:00:00Z","relpermalink":"/thesis/supplementary/c4/","section":"thesis","summary":"Chapter 4: Model-Based Character Animation This page contains supplementary material referenced in Chapter 4 of the thesis, demonstrating the model-based reinforcement learning agents trained to portray gazing and pointing behaviors.\nVideo Demonstrations Model-Based Animation Agents (AMSTA 2021) The following video demonstrates the agents trained using learned dynamics models and online planning for target-driven behaviors:\n  This video shows: - Gazing behavior with dynamic target tracking - Pointing behavior with both left and right arms - Combined gaze and point behaviors - Real-time responsiveness to changing targets - Comparison with inverse kinematics baseline","tags":null,"title":"Chapter 4: Supplementary Material","type":"thesis"},{"authors":null,"categories":null,"content":" Chapter 5 Part A: RLAnimate Output Sequences This page contains supplementary material for Chapter 5, demonstrating the RLAnimate A1 agents trained to portray waving and pointing behaviors.\nVideo Demonstrations RLAnimate Output Sequences and Comparison to Control Agents (ALA 2021) The following video shows RLAnimate agents performing waving and pointing behaviors, with comparisons to baseline control agents:\n  This video demonstrates: - A1 agent performing waving behaviors with varying exaggeration levels - A1 agent performing pointing behaviors to different targets - Comparison with single dynamics baseline - Comparison with supervised learning baseline - Natural, human-like motion generation\nKey Results Shown  Multi-behavior Support: Single agent portraying both waving and pointing Dynamic Control: Real-time adaptation to behavior parameters Human-like Quality: Smooth, natural movements without artifacts Superior Performance: Outperforms baselines on test set (87.23 vs 70.94)  Navigation  Chapter 5 Part B: Behavior Flexibility → ← Back to thesis main page Chapter 5 in main thesis  ","date":1737504000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1737504000,"objectID":"684c8c6eb974cdedac7413b0677c73bb","permalink":"https://vihanga.github.io/thesis/supplementary/c5a/","publishdate":"2025-01-22T00:00:00Z","relpermalink":"/thesis/supplementary/c5a/","section":"thesis","summary":"Chapter 5 Part A: RLAnimate Output Sequences This page contains supplementary material for Chapter 5, demonstrating the RLAnimate A1 agents trained to portray waving and pointing behaviors.\nVideo Demonstrations RLAnimate Output Sequences and Comparison to Control Agents (ALA 2021) The following video shows RLAnimate agents performing waving and pointing behaviors, with comparisons to baseline control agents:\n  This video demonstrates: - A1 agent performing waving behaviors with varying exaggeration levels - A1 agent performing pointing behaviors to different targets - Comparison with single dynamics baseline - Comparison with supervised learning baseline - Natural, human-like motion generation","tags":null,"title":"Chapter 5 Supplementary Material - Part A: RLAnimate Output Sequences","type":"thesis"},{"authors":null,"categories":null,"content":" Chapter 5 Part B: RLAnimate Behavior Portrayal Flexibility This page contains supplementary material for Chapter 5, demonstrating the flexibility and dynamic control capabilities of RLAnimate agents.\nVideo Demonstrations RLAnimate Behavior Portrayal Flexibility (ALA 2021) The following video demonstrates the dynamic flexibility of RLAnimate agents in portraying varied behaviors:\n  This video shows: - Waving with continuously variable exaggeration levels - Pointing to dynamically changing targets - Smooth transitions between behaviors - Real-time responsiveness to parameter changes - Natural variation in movement execution\nKey Capabilities Demonstrated  Continuous Control: Smooth interpolation between behavior parameters Dynamic Adaptation: Real-time response to changing objectives Natural Variation: Human-like variability in repeated behaviors Behavior Transitions: Seamless switching between waving and pointing  Navigation  ← Chapter 5 Part A: Output Sequences Chapter 6 Supplementary Material → ← Back to thesis main page Chapter 5 in main thesis  ","date":1737504000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1737504000,"objectID":"4533a70804b1d00a48ea76985216082e","permalink":"https://vihanga.github.io/thesis/supplementary/c5b/","publishdate":"2025-01-22T00:00:00Z","relpermalink":"/thesis/supplementary/c5b/","section":"thesis","summary":"Chapter 5 Part B: RLAnimate Behavior Portrayal Flexibility This page contains supplementary material for Chapter 5, demonstrating the flexibility and dynamic control capabilities of RLAnimate agents.\nVideo Demonstrations RLAnimate Behavior Portrayal Flexibility (ALA 2021) The following video demonstrates the dynamic flexibility of RLAnimate agents in portraying varied behaviors:\n  This video shows: - Waving with continuously variable exaggeration levels - Pointing to dynamically changing targets - Smooth transitions between behaviors - Real-time responsiveness to parameter changes - Natural variation in movement execution","tags":null,"title":"Chapter 5 Supplementary Material - Part B: Behavior Flexibility","type":"thesis"},{"authors":null,"categories":null,"content":" Chapter 6: Latent Dynamic-augmented Animation Output This page contains supplementary material referenced in Chapter 6 of the thesis, demonstrating the A3 architecture with quaternion rotations and animation dynamics for finger animation.\nA3 Architecture Results The A3 architecture introduced in Chapter 6 addresses the challenge of incorporating finger animation through:\n Quaternion Representations: Eliminating gimbal lock and improving computational efficiency Animation Dynamics Model: Dedicated latent dynamics for animation generation Hierarchical Learning: Better capture of joint dependencies  Performance Improvements  A2 + Quaternion baseline: 81.23 similarity score A3 Standard: 86.57 similarity score (5.34 point improvement) A3 with all latents: 86.19 similarity score  All velocity errors remain under the 0.4 perceptual threshold, confirming that the A3 architecture successfully maintains motion quality while handling the additional complexity of finger animation. As noted in the thesis (Section 6.3), visual analysis confirms that the A3 architecture maintains animation quality despite the increased complexity of animating 30+ finger joints.\nTechnical Contributions Quaternion Activation Function The novel quaternion activation function ensures all neural network outputs are valid unit quaternions:\nf(w,x,y,z) = [w,x,y,z] / sqrt(w² + x² + y² + z²)  This eliminates singularities (except at origin) and provides smooth gradients for training.\nAnimation Dynamics Model The dedicated animation dynamics model learns hierarchical relationships between joints through: - Deterministic state: ht^a = f(h{t-1}^a, s_{t-1}, h_t^p, p_t, h_t^b, b_t) - Stochastic component for natural variation - Temporal consistency through recurrent processing\nA3 Agent Capabilities The A3 agents demonstrate: - Successful incorporation of finger movements in waving behaviors - Natural finger positioning during pointing tasks - Smooth transitions without artifacts - Maintained performance under the 0.4 velocity error threshold\nNavigation  ← Chapter 5 Part B: Behavior Flexibility Chapter 7 Supplementary Material → ← Back to thesis main page Chapter 6 in main thesis  ","date":1737504000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1737504000,"objectID":"10c1b5d05179a4d74b11d745a8d44096","permalink":"https://vihanga.github.io/thesis/supplementary/c6/","publishdate":"2025-01-22T00:00:00Z","relpermalink":"/thesis/supplementary/c6/","section":"thesis","summary":"Chapter 6: Latent Dynamic-augmented Animation Output This page contains supplementary material referenced in Chapter 6 of the thesis, demonstrating the A3 architecture with quaternion rotations and animation dynamics for finger animation.\nA3 Architecture Results The A3 architecture introduced in Chapter 6 addresses the challenge of incorporating finger animation through:\n Quaternion Representations: Eliminating gimbal lock and improving computational efficiency Animation Dynamics Model: Dedicated latent dynamics for animation generation Hierarchical Learning: Better capture of joint dependencies  Performance Improvements  A2 + Quaternion baseline: 81.","tags":null,"title":"Chapter 6 Supplementary Material: Quaternions and Finger Animation","type":"thesis"},{"authors":null,"categories":null,"content":" Chapter 7: Portraying Conversational Gestures via Realism Regularisation This page contains supplementary material referenced in Chapter 7, including the video stimuli used in the perceptual evaluation study comparing RLAnimate A4, Gesticulator, and motion capture.\nPerceptual Study Video Stimuli The following videos were used in the perceptual evaluation with 28 participants. Each stimulus set was rendered with three methods: RLAnimate A4, Gesticulator, and Motion Capture (ground truth).\n .video-set { margin-bottom: 50px; background: #f5f5f5; padding: 20px; border-radius: 8px; } .video-grid { display: grid; grid-template-columns: repeat(3, 1fr); gap: 20px; margin-bottom: 20px; } .video-container { position: relative; } .video-container h4 { text-align: center; margin-bottom: 10px; font-size: 1.1em; color: #333; } .video-container video { width: 100%; border-radius: 4px; box-shadow: 0 2px 4px rgba(0,0,0,0.1); } .play-controls { text-align: center; margin-top: 20px; } .play-button { background-color: #007bff; color: white; border: none; padding: 12px 30px; font-size: 16px; border-radius: 4px; cursor: pointer; transition: background-color 0.3s; } .play-button:hover { background-color: #0056b3; } .play-button.playing { background-color: #dc3545; } .play-button.playing:hover { background-color: #c82333; } .play-button:disabled { background-color: #6c757d; cursor: not-allowed; } /* Loading indicator */ .loading-indicator { position: fixed; top: 20px; right: 20px; background: #333; color: white; padding: 15px 20px; border-radius: 8px; display: none; z-index: 1000; box-shadow: 0 4px 6px rgba(0,0,0,0.3); } .loading-indicator.show { display: block; } .loading-progress { font-size: 14px; margin-top: 5px; } /* Buffer status indicator */ .buffer-status { position: absolute; top: 5px; right: 5px; background: rgba(0,0,0,0.7); color: white; padding: 2px 8px; border-radius: 3px; font-size: 11px; display: none; } .buffer-status.loading { display: block; background: rgba(255,193,7,0.9); color: #333; } .buffer-status.ready { display: block; background: rgba(40,167,69,0.9); } @media (max-width: 768px) { .video-grid { grid-template-columns: 1fr; } .loading-indicator { top: 10px; right: 10px; font-size: 14px; } }  Loading videos... 0%   let videosLoaded = 0; let totalVideos = 0; const videoBufferStatus = new Map(); function updateLoadingProgress() { const progress = Math.round((videosLoaded / totalVideos) * 100); const progressEl = document.getElementById('loadingProgress'); const indicator = document.getElementById('loadingIndicator'); if (progressEl) { progressEl.textContent = `${progress}% (${videosLoaded}/${totalVideos} videos)`; } if (videosLoaded === totalVideos \u0026\u0026 indicator) { setTimeout(() = { indicator.classList.remove('show'); }, 1000); } } function preloadVideo(video, setId) { // Add buffer status indicator const container = video.closest('.video-container'); const statusEl = document.createElement('div'); statusEl.className = 'buffer-status loading'; statusEl.textContent = 'Loading...'; container.appendChild(statusEl); // Set preload attribute video.preload = 'auto'; // Track loading progress video.addEventListener('loadeddata', () = { statusEl.textContent = 'Buffering...'; }); video.addEventListener('canplaythrough', () = { videosLoaded++; videoBufferStatus.set(video, true); statusEl.className = 'buffer-status ready'; statusEl.textContent = 'Ready'; updateLoadingProgress(); // Hide ready status after 3 seconds setTimeout(() = { statusEl.style.display = 'none'; }, 3000); // Enable play button when all videos in set are ready const setVideos = document.querySelectorAll(`#${setId} video`); const allReady = Array.from(setVideos).every(v = videoBufferStatus.get(v)); if (allReady) { const button = document.querySelector(`#${setId} .play-button`); if (button) { button.disabled = false; button.textContent = '▶ Play All'; } } }); video.addEventListener('error', () = { statusEl.className = 'buffer-status loading'; statusEl.textContent = 'Error'; statusEl.style.background = 'rgba(220,53,69,0.9)'; }); // Force load by setting currentTime video.load(); } function toggleVideos(setId) { const videos = document.querySelectorAll(`#${setId} video`); const button = document.querySelector(`#${setId} .play-button`); // Check if all videos are ready const allReady = Array.from(videos).every(v = videoBufferStatus.get(v)); if (!allReady) { alert('Videos are still loading. Please wait...'); return; } const isPlaying = videos[0].paused; videos.forEach(video = { if (isPlaying) { video.play().catch(e = console.error('Play error:', e)); } else { video.pause(); } }); if (isPlaying) { button.textContent = '⏸ Pause All'; button.classList.add('playing'); } else { button.textContent = '▶ Play All'; button.classList.remove('playing'); } } // Sync video playback positions function syncVideos(setId) { const videos = document.querySelectorAll(`#${setId} video`); const masterVideo = videos[0]; masterVideo.addEventListener('timeupdate', () = { videos.forEach((video, index) = { if (index !== 0 \u0026\u0026 Math.abs(video.currentTime - masterVideo.currentTime)  0.1) { video.currentTime = masterVideo.currentTime; } }); }); // Reset all videos when one ends videos.forEach(video = { video.addEventListener('ended', () = { videos.forEach(v = { v.currentTime = 0; v.pause(); }); const button = document.querySelector(`#${setId} .play-button`); button.textContent = '▶ Play All'; button.classList.remove('playing'); }); }); } // Initialize when page loads document.addEventListener('DOMContentLoaded', function() { const sets = ['setA', 'setB', 'setC', 'setD', 'setE', 'setF', 'setG', 'setH']; const allVideos = document.querySelectorAll('video'); totalVideos = allVideos.length; // Show loading indicator const indicator = document.getElementById('loadingIndicator'); if (indicator \u0026\u0026 totalVideos  0) { indicator.classList.add('show'); } // Disable all play buttons initially document.querySelectorAll('.play-button').forEach(button = { button.disabled = true; button.textContent = '⏳ Loading...'; }); sets.forEach(setId = { if (document.getElementById(setId)) { syncVideos(setId); // Preload all videos in the set const videos = document.querySelectorAll(`#${setId} video`); videos.forEach(video = { preloadVideo(video, setId); }); } }); });  Stimulus Set A RLAnimate A4   Gesticulator   Motion Capture    ▶ Play All   Stimulus Set B RLAnimate A4   Gesticulator   Motion Capture    ▶ Play All   Stimulus Set C RLAnimate A4   Gesticulator   Motion Capture    ▶ Play All   Stimulus Set D RLAnimate A4   Gesticulator   Motion Capture    ▶ Play All   Stimulus Set E RLAnimate A4   Gesticulator   Motion Capture    ▶ Play All   Stimulus Set F RLAnimate A4   Gesticulator   Motion Capture    ▶ Play All   Stimulus Set G RLAnimate A4   Gesticulator   Motion Capture    ▶ Play All   Stimulus Set H RLAnimate A4   Gesticulator   Motion Capture    ▶ Play All   Perceptual Study Results Statistical Results RLAnimate A4 vs Gesticulator (one-sided t-tests, α = 0.05): - Human-likeness: 4.33 vs 3.19 (p \u0026lt; 0.001) ✓ - Speech reflection: 4.12 vs 3.42 (p \u0026lt; 0.001) ✓ - Synchronization: 4.48 vs 3.60 (p \u0026lt; 0.001) ✓\nRLAnimate A4 vs Motion Capture (two-sided t-tests, α = 0.05): - Human-likeness: 4.33 vs 4.65 (p = 0.083, NS) - Speech reflection: 4.12 vs 4.38 (p = 0.174, NS) - Synchronization: 4.48 vs 4.67 (p = 0.268, NS)\nKey Finding RLAnimate A4 achieves statistical parity with human motion capture while significantly outperforming the state-of-the-art Gesticulator system across all evaluation criteria.\nNavigation  ← Chapter 6 Supplementary Material ← Back to thesis main page Chapter 7 in main thesis  ","date":1737504000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1737504000,"objectID":"d370faf13eef94886084b843b011fa53","permalink":"https://vihanga.github.io/thesis/supplementary/c7/","publishdate":"2025-01-22T00:00:00Z","relpermalink":"/thesis/supplementary/c7/","section":"thesis","summary":"Chapter 7: Portraying Conversational Gestures via Realism Regularisation This page contains supplementary material referenced in Chapter 7, including the video stimuli used in the perceptual evaluation study comparing RLAnimate A4, Gesticulator, and motion capture.\nPerceptual Study Video Stimuli The following videos were used in the perceptual evaluation with 28 participants. Each stimulus set was rendered with three methods: RLAnimate A4, Gesticulator, and Motion Capture (ground truth).\n .video-set { margin-bottom: 50px; background: #f5f5f5; padding: 20px; border-radius: 8px; } .","tags":null,"title":"Chapter 7 Supplementary Material: Beat Gestures","type":"thesis"},{"authors":null,"categories":null,"content":" Examining the effects of a virtual character on learning and engagement in serious games Authors: Vihanga Gamage, Cathy Ennis\nConference: MIG \u0026lsquo;18: Motion, Interaction and Games, November 8–10, 2018, Limassol, Cyprus\nDOI: 10.1145\u0026frasl;3274247.3274499\nAbstract Virtual characters have been employed for many purposes including interacting with players of serious games, with a purpose to increase engagement. These characters are often embodied conversational agents playing diverse roles, such as demonstrators, guides, teachers or interviewers. Recently, much research has been conducted into properties that affect the realism and plausibility of virtual characters, but it is less clear whether the inclusion of interactive agents in serious applications can enhance a user\u0026rsquo;s engagement with the application, or indeed increase efficacy. In a first step towards answering these questions, we conducted a study where a Virtual Learning Environment was used to examine the effect of employing a virtual character to deliver a lesson. In order to investigate whether increased familiarity between the player and the character would help achieve learning outcomes, we allowed participants to customize the physical appearance of the character. We used direct and indirect measures to assess engagement and learning; we measured knowledge retention to ascertain learning via a test at the end of the lesson, and also measured participants\u0026rsquo; perceived engagement with the lesson. Our findings show that a virtual character can be an effective learning aid, causing heightened engagement and retention of knowledge. However, allowing participants to customize character appearance resulted in inhibited engagement, which was contrary to what we expected.\nKey Findings  Enhanced Engagement: Users showed significantly higher engagement when lessons were delivered through a virtual character compared to non-character controls.\n Improved Knowledge Retention: Virtual character delivery resulted in better knowledge retention compared to traditional presentation methods.\n Attention Focus: User attention was predominantly directed at the virtual character, with limited attention to other environmental elements.\n Customization Paradox: Contrary to expectations, character appearance personalization led to significantly lower engagement compared to default appearances.\n  Relevance to Thesis This work laid important groundwork for understanding how virtual characters affect user engagement and learning. While this paper focused on static virtual characters in educational contexts, it motivated the later development of RLAnimate for creating more dynamic, responsive virtual characters that can adapt their behaviors in real-time - addressing some of the limitations identified in this study.\nLinks  Paper PDF (ACM Digital Library) Conference presentation thoughts and feedback MIG\u0026rsquo;18 conference blog post Back to thesis  ","date":1737504000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1737504000,"objectID":"91a78759d0b2980c5906b19f9be44565","permalink":"https://vihanga.github.io/thesis/publications/MIG18/","publishdate":"2025-01-22T00:00:00Z","relpermalink":"/thesis/publications/MIG18/","section":"thesis","summary":"Examining the effects of a virtual character on learning and engagement in serious games Authors: Vihanga Gamage, Cathy Ennis\nConference: MIG \u0026lsquo;18: Motion, Interaction and Games, November 8–10, 2018, Limassol, Cyprus\nDOI: 10.1145\u0026frasl;3274247.3274499\nAbstract Virtual characters have been employed for many purposes including interacting with players of serious games, with a purpose to increase engagement. These characters are often embodied conversational agents playing diverse roles, such as demonstrators, guides, teachers or interviewers.","tags":null,"title":"MIG'18 Paper: Virtual Characters in Serious Games","type":"thesis"},{"authors":["Vihanga Gamage"],"categories":[],"content":"Originally published on RTÉ Brainstorm\nThe solar system is full of debris and rocks floating around. A meteor shower or a shooting star is debris burning up upon entry to the atmosphere. On occasion, pieces of rock make it all the way through. A much larger rock that could make its way through is an asteroid. About 66 million years ago, the impact from an asteroid brought about the Cretaceous extinction. A repetition of this event is theoretically possible - and the same could be said about all-powerful Artificial Intelligence Overlords marginalising the human race.\nSince its inception in the mid 20th century, the field of artificial intelligence has had an interesting ride. Much like the financial markets or Hollywood, there’s been breakthroughs and failures and booms and busts. Driven by advances in deep learning, a great deal of success has been achieved this decade, ushering in the newest AI golden age, along with a frenzy of interest and investment.\nThe current popular views of AI are being shaped primarily by several vocal figures in technology. Microsoft founder Bill Gates has said that AI carries the potential to change society deeply and is both exciting and dangerous. IBM CEO Ginni Rometty has stated that she believes advances in AI will create different jobs rather than no jobs. Facebook founder Mark Zuckerberg has expressed the view that AI will deliver various improvements to human quality of life, while Tesla founder Elon Musk has called AI \u0026ldquo;a fundamental risk to the existence of human civilisation\u0026rdquo;.\nThe future dangers AI could pose to humanity is a frequently occurring subject in conversation, be it to break the ice on a first date, to fill the time before a meeting starts or while waiting for a pint at the pub on a Friday evening. And arguably, Musk\u0026rsquo;s statement carries the most bite. It certainly is a very catchy line that makes for a more topical conversation starter than asteroids or the Cretaceous extinction.\nAs a result, these hypothetical yet sensationalist doomsday opinions are at a point where they dominate the conversation about AI, so much so that the bigger picture on the subject may at the point of being overshadowed and given very little consideration. And that could pose an even greater threat, especially in an age where hype and rhetoric without regard for fact has had a great effect on the fate of the world.\nAdvancements in AI could lead to a superintelligence that could pose a fundamental risk to human existence - and so could climate change or a 5 km-wide space rock colliding with the Earth. But there are many points of difference between these three risks. AI is a long way away advancing to the point of superintelligence and it’s not a certainty that superintelligence would lead to the end of humanity. Climate change is a very real and present danger, with the time to take action to prevent irreversible consequences continuing to decrease. And then there are the chances of an asteroid striking the earth.\nOf course, there is no good that can come from an asteroid strike or global warming, but AI can literally be one of the best things to happen to humans. This is not said enough - and it can’t be said enough. AI can and is being used to as a tool in many incredible ways. For example, it is being used to help improve prevention of cancer, provide effective treatment, and could one day even be part of the solution.\nAI is also being leveraged as a promising way to combat climate change. AI-powered simulations and analysis helps make for sustainable urban planning and ocean preservation. It is possible to make more accurate predictions regarding weather events with the help of AI leading to reduced damage to life and property. It can help power grids to be more energy efficient by predicting peak over of usage allowing for better preparation and enhances the benefits of clean energy methods by deploying in ways such as to incorporate weather and other relevant data to make wind turbines more efficient.\nAnd yet arguments on how AI could hypothetically end humanity as we know it at some point in the future, seem to drown out how AI is already helping humanity’s cause. Such contributions pale in comparison to the very real possibility that AI could be the key to finding the answers for some of humanity’s biggest problems.\nAt a 2018 hearing at the US House of Representatives Committee on Science, Space and Technology, Stanford Professor Fei-Fei Li said that AI is bound to alter the human experience, and not necessarily for the better. After acknowledging this, Li pointed out that ensuring AI will transform the world for the better is very much in our hands. \u0026ldquo;There\u0026rsquo;s nothing artificial about AI\u0026rdquo;, she said.\n \u0026ldquo;It’s inspired by people; it’s created by people, and—most importantly—it impacts people. It is a powerful tool we are only just beginning to understand, and that is a profound responsibility.\u0026rdquo;\n Concern that AI could pose a threat is very much reasonable. But this possibility is just that - a possibility. Let’s not forget AI is a fundamental opportunity to further the cause of the human civilisation, and that it is in our hands to ensure that it impacts us positively, and be optimistic about what it can mean for us. The world can only be the better place for it.\n","date":1566061379,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1566061379,"objectID":"a75ccec893290ac194456b20ef62ffa9","permalink":"https://vihanga.github.io/post/blog/AI-Oppurtunity-not-danger/","publishdate":"2019-08-17T17:02:59Z","relpermalink":"/post/blog/AI-Oppurtunity-not-danger/","section":"post","summary":"Originally published on RTÉ Brainstorm\nThe solar system is full of debris and rocks floating around. A meteor shower or a shooting star is debris burning up upon entry to the atmosphere. On occasion, pieces of rock make it all the way through. A much larger rock that could make its way through is an asteroid. About 66 million years ago, the impact from an asteroid brought about the Cretaceous extinction.","tags":["blog"],"title":"Why AI is an opportunity rather than a danger","type":"post"},{"authors":["Vihanga Gamage"],"categories":[],"content":"\u0026lt;\u0026lt; restricted access to posting \u0026gt;\u0026gt;\n","date":1542474179,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1542474179,"objectID":"8ea6b228c1d9f27f638832de44099fc4","permalink":"https://vihanga.github.io/post/research/MIG18PaperStudy/","publishdate":"2018-11-17T17:02:59Z","relpermalink":"/post/research/MIG18PaperStudy/","section":"post","summary":"\u0026lt;\u0026lt; restricted access to posting \u0026gt;\u0026gt;","tags":["research"],"title":"A pilot study into virtual character application as pedagogical agents","type":"post"},{"authors":["Vihanga Gamage"],"categories":[],"content":"\u0026lt;\u0026lt; restricted access to posting \u0026gt;\u0026gt;\n","date":1542474179,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1542474179,"objectID":"e46c42befa06751c790f5912e50fc00e","permalink":"https://vihanga.github.io/post/blog/MIG18blog/","publishdate":"2018-11-17T17:02:59Z","relpermalink":"/post/blog/MIG18blog/","section":"post","summary":"\u0026lt;\u0026lt; restricted access to posting \u0026gt;\u0026gt;","tags":["blog"],"title":"Motion, Interaction and Games 2018","type":"post"},{"authors":["Vihanga Gamage"],"categories":[],"content":"   After the horrible start to the season I had at the City of Dublin in September, I was desperate to turn my form and results around. I had signed up to play in as much fixtures as I can for the club\u0026rsquo;s second team who plays in the third division of the Leinster Chess Union leagues, and played in all three opening fixtures.\nAnd will you look at that, the results are quite alright.\n  Funnily enough, I seem to have played three consecutive games with the black pieces due to playing on three different boards, and the team did alright especially given our standing on joint second after 2 wins and a narrow loss to one of the highest rated teams in the division.\n  Match 1 : Opening game vs Elm Mount - A very lucky escape. Very lucky indeed. This was a bit of a weird one. I was playing black, on board 5, fairly confident, decent position coming out of the opening, and I was in a mood that day. So, when it looked like there was potential to sacrifice a piece on h3, I was fairly focussed on this.\nAt this, position, I was so preoccupied whether to take h3 with the bishop on c8 or knight on f4, I didn\u0026rsquo;t notice that my opponents last move, Nf1, opens an attack from the bishop on c1 to my knight on f4.\n After a lot of deliberation in my head, I took h3 with the Bishop, and I didn\u0026rsquo;t notice my blunder till my opponent took the piece on f4.\nAfter that blunder, I thought my best chance of recovering was to capitalize on whatever little attacking potential I had in this position, and I forged ahead with my plan to put his castle under pressure. My rationale here was that it was easier for me to get both my rooks involved in an assault on his castle than it was for him to make his rook on a1 active, and maybe I might be able to overwhelm his pieces.\nAnd as luck would have it, that\u0026rsquo;s just what happened.\n Presumably, due to the perceived threat from f4 that would make open the file for my rook on f8 and further weaken his king, he made the move Qd2.\n..Nxg2, he responded with Nf1, again, as what I presume was a hail mary move in the hopes I might overlook a threat on that diagonal, again. But I didn\u0026rsquo;t, he resigned after my next move Nf4+ as the only continuation lead to either losing his queen or checkmate.\n I was glad I got away with the win after that blunder, the team got a 4-2 win in the opening fixture. We went to the pub afterwards, played ping pong while having pints, and watched Man United pull off that awesome 3-2 comeback win against Newcastle.\nOverall, it was an alright day, but it could have just gone very, very, wrong, for myself, just as well as for United and Jose Mourinho.\nMatch 2 : Converting an advantage with surprising efficiency Next up was a game against Lucan, all the other 5 games had been played the week before; my board had been deferred as an old friend was visiting me the day of the match. The match was balanced on 2.5-2.5 with 5 games played, and I was playing their captain on board 4.\nThe opening was nothing special, left a nice evenly balanced position coming into the midgame. \nMy move here was to castle, rather than the engine suggested move of capturing the e4 pawn, but the open file was what lead to my advantage, given my opponent sought to make a rash advance with his rook to apply pressure on my castle.\n But in this position, ...Nc5. Qc3, Ne6. R7xe6,Bxe6. And I was up a rook to a bishop. It had been a recent pattern of mine to play out some fidgety plans leading to drawn positions even when I had the advantage, or even devolve into a loss, but I played through to a winning endgame in what was, on recent form, uncharacteristically surprising efficiency.  It would\u0026rsquo;ve been nicer if I had spotted the forced mating sequence here:\n\u0026hellip;e4+. Kf4, Rgf2+. Kg4, Rf3. Rf7+, Kxf7. Kg5, Rg2+. Kh6, Rh3. c7, Rxh4#\nBut I had less than a minute on the clock, so I was playing on increments, so it\u0026rsquo;s entirely justifiable that I did things the old fashioned way.\n My opponent ran out of time here, when I was winning quite comfortably; and the team scored a 3.5-2.5 win.\nMatch 3 : A fast 15 move draw It was a Monday evening, I was again playing a couple of days after the match, one of two differred boards. I had just flew back to Dublin the evening before from the MIG\u0026rsquo;18 conference in Cyprus.\nI had popped into a pub near the playing venue to get a pint, and opened my Macbook to finish up some emails when I see a window with my Twitter home feed showing that Stan Lee had died.\nI really wasn\u0026rsquo;t in the mood to play a game after seeing this, 10 minutes before the start time. I went in, my oppoennet played the Polish opening, that was a first. And I was more than happy to play a line that lead to a draw.\n Looking at the position, it can be said that I was let off, as my isolated pawn and overall position isn\u0026rsquo;t likely to lead to a whole lot of joy. My move here would\u0026rsquo;ve been Qd6 or Qc7, and it wouldn\u0026rsquo;t have been the hardest to defend here. Qd6 might even allow for some attacking momentum if I can get my knight involved. My opponent seemed to agree, and it seemed he had studied the opening and likely positions pretty thoroughly; also the engine concurred with this analysis.\nBut, as my opponent said, we could\u0026rsquo;ve played for a couple of hours and ended up drawn anyway, or agreed a draw early on that basis and gone home early.\nWe lost the game 2.5-3.5, but given that league standings are based on game points rather than match points, it\u0026rsquo;s not the end of the world. We are tied second on points in the league standings, we seem quite competitive for a newly promoted team with an average rating that is on the latter end in the league.\nIf we can keep this up, come April, the team could be in an interesting position.\n  And, oh hello, a 55 point boost to my FIDE rating. This was a nice surprise, thanks to the K-Factor of 40, as it\u0026rsquo;s less than 30 of my games were FIDE rated.\n  I had initially planned to play in the Kilkenny Weekender, where I played my first competitive chess since moving to Ireland, for the third consecutive year. But with the fatigue from the travel and the conference and with some heavy weeks ahead of me at work, it seemed the right idea to sit this year out.\nI\u0026rsquo;m likely to play at 2 more games in 2018, with our last Ennis shield fixture again Skerries, and a substitute appearance for the first time this season in the Armstrong Cup for the first team.\nMy form seems to be improving after the blunders in the City of Dublin weekender and the less than ideal play early in the first league match. This is a good sign for my potential performance at the 9 round closed all-play-all in early January, that I entered for.\n","date":1542474179,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1542474179,"objectID":"8a6c13decec0c37ebef0cef29eb5a232","permalink":"https://vihanga.github.io/post/chess/2018LeagueEnnisR123/","publishdate":"2018-11-17T17:02:59Z","relpermalink":"/post/chess/2018LeagueEnnisR123/","section":"post","summary":"After the horrible start to the season I had at the City of Dublin in September, I was desperate to turn my form and results around. I had signed up to play in as much fixtures as I can for the club\u0026rsquo;s second team who plays in the third division of the Leinster Chess Union leagues, and played in all three opening fixtures.\nAnd will you look at that, the results are quite alright.","tags":["chess"],"title":"The form awakens* (*seems to stir awake from a 4 month slumber) - 2.5/3 : Rounds 1, 2 \u0026 3 of the Ennis Shield (Leinster League Division 3)","type":"post"},{"authors":["Vihanga Gamage, Cathy Ennis"],"categories":null,"content":"I presented my first conference paper at the 11th Motion, Interactions and Games conference held in November 2018. In this paper, we examined the broad effects a virtual character can have on user experience and performance when incorporated as a pedagogical agent in a serious game. We also explored character appearance personalization as a potential way of heightening engagement with the character.\nOur findings can be summaries as follows: - A user is more engaged with a lesson when delivered through a virtual character compared to a non-character control version. - A user retains knowledge better when a lesson is delivered through a virtual character compared to a non-character control version. - There is a propensity for the majority of user attention to be directed at the virtual character, resulting in little attention being paid to the rest of the game environment. - Surprising, character appearance personalization has an effect opposite to what we expected, resulting in participants being significantly less engaged with a personalized character compare to one with a default appearance.\nSupplementary material in the form of the full questionnaire used in the study and free-form user feedback and some interesting points raised by attendees at the conference following my thoughts can be found here, and a blog post about my experience at the conference and some photographs can be found here.\n","date":1542473248,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1542473248,"objectID":"c6965129ead2fdcac70fc04d9f02f39e","permalink":"https://vihanga.github.io/publication/MIG18/","publishdate":"2018-11-17T16:47:28Z","relpermalink":"/publication/MIG18/","section":"publication","summary":"Virtual characters have been employed for many purposes including interacting with players of serious games, with a purpose to increase engagement. These characters are often embodied conversational agents playing diverse roles, such as demonstrators, guides, teachers or interviewers. Recently, much research has been conducted into properties that affect the realism and plausibility of virtual characters, but it is less clear whether the inclusion of interactive agents in serious applications can enhance a user's engagement with the application, or indeed increase efficacy. In a first step towards answering these questions, we conducted a study where a Virtual Learning Environment was used to examine the effect of employing a virtual character to deliver a lesson. In order to investigate whether increased familiarity between the player and the character would help achieve learning outcomes, we allowed participants to customize the physical appearance of the character. We used direct and indirect measures to assess engagement and learning; we measured knowledge retention to ascertain learning via a test at the end of the lesson, and also measured participants' perceived engagement with the lesson. Our findings show that a virtual character can be an effective learning aid, causing heightened engagement and retention of knowledge. However, allowing participants to customize character appearance resulted in inhibited engagement, which was contrary to what we expected.","tags":[],"title":"Examining the effects of a virtual character on learning and engagement in serious games","type":"publication"},{"authors":["Vihanga Gamage"],"categories":[],"content":"\u0026lt;\u0026lt; restricted access to posting \u0026gt;\u0026gt;\u0026hellip;.\n","date":1542128579,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1542128579,"objectID":"70e8f7b753362d5f38eacdfd83b8feae","permalink":"https://vihanga.github.io/post/blog/StanLee/","publishdate":"2018-11-13T17:02:59Z","relpermalink":"/post/blog/StanLee/","section":"post","summary":"\u0026lt;\u0026lt; restricted access to posting \u0026gt;\u0026gt;\u0026hellip;.","tags":["blog"],"title":"Excelsior!","type":"post"},{"authors":["Vihanga Gamage"],"categories":[],"content":" \u0026ldquo;My soul is painted like the wings of butterflies, fairy tales of yesterday will grow but never die. I can fly, my friends\u0026rdquo;\nThe Show Must Go On, Queen (1991)\u0026rdquo;\n \u0026lt;\u0026lt; restricted access to posting \u0026gt;\u0026gt;\n","date":1540832579,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1540832579,"objectID":"26178ba6ce0c35ef14bd61c03b23aa50","permalink":"https://vihanga.github.io/post/blog/BohemianRhapsody/","publishdate":"2018-10-29T17:02:59Z","relpermalink":"/post/blog/BohemianRhapsody/","section":"post","summary":"\u0026ldquo;My soul is painted like the wings of butterflies, fairy tales of yesterday will grow but never die. I can fly, my friends\u0026rdquo;\nThe Show Must Go On, Queen (1991)\u0026rdquo;\n \u0026lt;\u0026lt; restricted access to posting \u0026gt;\u0026gt;","tags":["blog"],"title":"\"....fairy tales of yesterday will grow but never die.\" , My thoughts on Bohemian Rhapsody, the Queen biopic","type":"post"},{"authors":["Vihanga Gamage"],"categories":[],"content":"  The 2018 / 2019 ICU season kicked off at the City of Dublin weekender held in September. This season was one I approached with as much intent to perform well, as I had done in a while. This is because I will be taking a step back from playing relatively seriously, at the end of the season, for about a year to focus on the last year of my PhD.\nDespite my showing at the Irish Championship Supporting events in August not being as convincing as I had hoped after a few good performances earlier in the year, I was at least happy with the fact that I had consolidated the 180 point rating jump I achieved at the Ennis Open in May.\nI was seeded in the top 10 of the draw in my section. The first round I was playing back against a solid player. After a fairly even middle game phase, I managed to capitalise on my opponent\u0026rsquo;s misstep to go into the end game with a winning position.\nand the stage was set\u0026hellip;\u0026hellip;for a trademark pawn ending screw up from me. This has been happening a lot recently.\nIn my defence, I had studied up on pawn ending theory that I was continually getting wrong, and I had a couple of minutes plus increments on the clock, but this was pretty much inexcusable.\nIn this position, it was my move, and instead of c5, I played Bxc3 leading the game to a dead draw.\n Bxc3, Bxc3 Kxc3, Ke3 Kb3, Kxe4 Kxa3, f4\nand hold on to your hats people because here comes the height of lunacy. Instead of calculating properly and making c4, Kxb4 ??????\nand that was it. First game of the season, arguably the worst game I played since February.\n The next morning, I picked up right where I left off, a toothless game and a dumb theoretical blunder towards the end.\nTwo games, both against players rated about 200 points beyond me, my rating had already dropped by 50.\nIt did not seem like I could get out of the rut I was in, and a tailspin seemed imminent, so to avoid a dropping an even hundred more rating points, I withdrew at this point.\nMy next competitive game is likely going to be the first match round of the new league season. I will be playing the full league campaign for the Dublin University Ennis Shield team; here\u0026rsquo;s hoping I plug the holes in my endgame and mid-game play isn\u0026rsquo;t so flat.\n","date":1539795779,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1539795779,"objectID":"1b2d65a0738b96dd92c771c08112086e","permalink":"https://vihanga.github.io/post/chess/2018CoD/","publishdate":"2018-10-17T17:02:59Z","relpermalink":"/post/chess/2018CoD/","section":"post","summary":"The 2018 / 2019 ICU season kicked off at the City of Dublin weekender held in September. This season was one I approached with as much intent to perform well, as I had done in a while. This is because I will be taking a step back from playing relatively seriously, at the end of the season, for about a year to focus on the last year of my PhD.","tags":["chess"],"title":"A calamitous start to the season - 0/2 at the City of Dublin 2018","type":"post"},{"authors":null,"categories":null,"content":" Vihanga Gamage\u0026rsquo;s Public Calendar  ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"a631103cf606bbead447236a29c6889e","permalink":"https://vihanga.github.io/calendar/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/calendar/","section":"","summary":" Vihanga Gamage\u0026rsquo;s Public Calendar  ","tags":null,"title":"","type":"page"}]