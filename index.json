[{"authors":null,"categories":null,"content":" Introduction This thesis tackles a fundamental challenge in computer animation: how can we create virtual characters that move naturally and responsively without the prohibitive costs of motion capture?\nThe Problem Traditional character animation relies heavily on motion capture - an expensive, time-consuming process that requires recording human actors for every possible behavior variation. While this produces high-quality results, it\u0026rsquo;s simply not scalable for interactive applications where characters need to respond dynamically to user input or changing scenarios.\nMeanwhile, existing alternatives have significant limitations. Supervised learning methods can generate animations but lack flexibility. Physics-based reinforcement learning works brilliantly for athletic movements but fails for social behaviors like gestures - there\u0026rsquo;s no physics simulation to tell us if a wave looks \u0026ldquo;friendly\u0026rdquo; or a gesture feels \u0026ldquo;natural.\u0026rdquo;\nMy Approach I developed RLAnimate, a framework that fundamentally rethinks how we approach character animation. Instead of using reward functions (the traditional RL approach), I use motion capture data as learning objectives. This allows agents to learn what \u0026ldquo;human-like\u0026rdquo; means directly from examples, while maintaining the flexibility and responsiveness that makes RL powerful.\nThe key insight is treating animation as a model-based reinforcement learning problem where agents learn the dynamics of human movement. By understanding how joint rotations create behaviors, agents can generate novel animations that look natural while adapting to changing requirements in real-time.\nKey Contributions  A new paradigm for animation RL - Using motion capture data as objectives rather than rewards Multi-behavior agents - Single agents that can wave, point, and generate conversational gestures Technical innovations - Quaternion neural networks, realism regularization, and hierarchical dynamics models Proven results - Animations that are statistically indistinguishable from human motion capture  Research Questions The thesis systematically addresses four key questions: - Can RL work for animation without physics? (Yes - through latent dynamics learning) - Can we make it human-like? (Yes - using motion capture as objectives) - How do we handle complexity like fingers? (Quaternions + dedicated animation dynamics) - Can it work for conversational gestures? (Yes - with realism regularization)\nThesis Journey The work progresses from establishing basic feasibility (Chapter 4) through developing the core RLAnimate methodology (Chapter 5), extending to complex movements with fingers (Chapter 6), and culminating in conversational beat gestures that match human quality (Chapter 7).\n","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"981946ce109800ca64235ec2433b470c","permalink":"https://vihanga.github.io/thesis/chapters/introduction/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/thesis/chapters/introduction/","section":"thesis","summary":"Introduction This thesis tackles a fundamental challenge in computer animation: how can we create virtual characters that move naturally and responsively without the prohibitive costs of motion capture?\nThe Problem Traditional character animation relies heavily on motion capture - an expensive, time-consuming process that requires recording human actors for every possible behavior variation. While this produces high-quality results, it\u0026rsquo;s simply not scalable for interactive applications where characters need to respond dynamically to user input or changing scenarios.","tags":null,"title":"Chapter 1: Introduction","type":"thesis"},{"authors":null,"categories":null,"content":" Virtual Character Animation and Perception This chapter establishes the foundation for understanding virtual character animation - both the technical challenges and the perceptual requirements that make animation \u0026ldquo;feel right\u0026rdquo; to human viewers.\nThe Animation Pipeline Creating believable virtual characters isn\u0026rsquo;t just about making them move - it\u0026rsquo;s a complex pipeline that starts with 3D modeling and ends with real-time rendering. I explore how animation data flows through this pipeline, from motion capture or procedural generation to the final pixels on screen. The key insight? Each step introduces opportunities for both quality improvements and computational optimizations.\nWhy Perception Matters Here\u0026rsquo;s what most animation systems miss: humans are incredibly sensitive to movement patterns. We can spot \u0026ldquo;unnatural\u0026rdquo; motion in milliseconds, even if we can\u0026rsquo;t articulate why. I dive into the perceptual studies that reveal what makes animation feel human-like - from timing and smoothness to the subtle coordination between body parts.\nThis understanding drives a crucial design decision in my work: animation quality isn\u0026rsquo;t just about technical metrics like joint angles or velocities. It\u0026rsquo;s about matching the statistical patterns of human movement that our brains have evolved to recognize.\nCurrent Approaches and Their Limits The field has tried three main approaches:\nMotion Graphs and Blending: Works well for predetermined scenarios but lacks flexibility. You need exponentially more data as behaviors become more complex.\nNeural Networks: Can generate smooth animations but often produce \u0026ldquo;average\u0026rdquo; motions that lack character and specificity. They struggle with rare behaviors and edge cases.\nPhysics-based RL: Produces incredibly athletic movements - backflips, martial arts, parkour. But try to make a physics-based agent wave \u0026ldquo;friendly\u0026rdquo; and you\u0026rsquo;ll see the problem. There\u0026rsquo;s no physics equation for social appropriateness.\nThe Gap This Thesis Fills What\u0026rsquo;s missing is a method that combines the flexibility of RL with the quality of motion capture data, without requiring physics simulation. That\u0026rsquo;s exactly what RLAnimate provides - and understanding this background shows why that combination is so powerful.\nThe chapter sets up the key tension: we want animation that\u0026rsquo;s both high-quality (matching human perception) and flexible (responding to dynamic scenarios). Traditional methods excel at one or the other, but not both. This creates the perfect motivation for the model-based RL approach I develop in the following chapters.\n","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"9358d9b2e86d81f55a8f6a890e4d78f6","permalink":"https://vihanga.github.io/thesis/chapters/data-driven-animation/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/thesis/chapters/data-driven-animation/","section":"thesis","summary":"Virtual Character Animation and Perception This chapter establishes the foundation for understanding virtual character animation - both the technical challenges and the perceptual requirements that make animation \u0026ldquo;feel right\u0026rdquo; to human viewers.\nThe Animation Pipeline Creating believable virtual characters isn\u0026rsquo;t just about making them move - it\u0026rsquo;s a complex pipeline that starts with 3D modeling and ends with real-time rendering. I explore how animation data flows through this pipeline, from motion capture or procedural generation to the final pixels on screen.","tags":null,"title":"Chapter 2: Virtual Character Animation and Perception","type":"thesis"},{"authors":null,"categories":null,"content":" Model-based Reinforcement Learning This chapter provides the theoretical foundation for understanding how model-based RL can revolutionize character animation. I explain why learning a model of the world enables more sample-efficient and generalizable agents compared to traditional approaches.\nThe Power of World Models Most RL systems are model-free - they learn through trial and error what actions lead to good outcomes. That works great if you have millions of attempts to practice. But what if you need to learn from limited data? That\u0026rsquo;s where model-based RL shines.\nBy learning a model of how the world works - in our case, how human bodies move - agents can plan ahead, imagine consequences, and generalize from limited examples. It\u0026rsquo;s the difference between memorizing dance moves and understanding the principles of movement.\nLatent Dynamics: The Secret Sauce Here\u0026rsquo;s the key insight that makes RLAnimate work: we don\u0026rsquo;t need to model every muscle and bone. Instead, we learn compact \u0026ldquo;latent\u0026rdquo; representations that capture the essence of movement patterns.\nI introduce the mathematical framework of latent dynamics models, showing how they compress high-dimensional movement data into manageable representations while preserving the critical information for generating natural motion. This is what allows our agents to run at 5ms per frame - we\u0026rsquo;re working in an efficient latent space, not raw joint angles.\nRepresentation Learning and Realism The chapter explores how to learn good representations for animation. Not all latent spaces are created equal - some preserve the nuances that make motion feel human, others lose them. I examine techniques from variational autoencoders to adversarial learning, setting up the theoretical foundation for the realism regularization used in later chapters.\nWhy Model-based for Animation? The chapter concludes by connecting these abstract concepts to character animation:\n Sample Efficiency: Learn new behaviors from just a few motion capture examples Generalization: Interpolate and extrapolate beyond the training data Planning: Generate coherent long-term motion sequences Speed: Efficient latent representations enable real-time performance  This theoretical grounding is essential for understanding why RLAnimate succeeds where other methods fail. Model-based RL isn\u0026rsquo;t just another technique - it\u0026rsquo;s fundamentally the right abstraction for the animation problem.\n","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"752bb61cd070490e6fd7dd8b8fad4585","permalink":"https://vihanga.github.io/thesis/chapters/model-based-rl/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/thesis/chapters/model-based-rl/","section":"thesis","summary":"Model-based Reinforcement Learning This chapter provides the theoretical foundation for understanding how model-based RL can revolutionize character animation. I explain why learning a model of the world enables more sample-efficient and generalizable agents compared to traditional approaches.\nThe Power of World Models Most RL systems are model-free - they learn through trial and error what actions lead to good outcomes. That works great if you have millions of attempts to practice.","tags":null,"title":"Chapter 3: Model-based Reinforcement Learning","type":"thesis"},{"authors":null,"categories":null,"content":" Model-based Character Animation This chapter marks the beginning of my core contributions - demonstrating that model-based RL can work for character animation without physics simulation. It\u0026rsquo;s the proof-of-concept that sets the stage for everything that follows.\nThe Experiment That Started It All I started with a simple question: can an RL agent learn to animate a character using only a learned model of movement dynamics? No physics engine, no hand-crafted rewards - just data and learning.\nThe answer turned out to be a resounding yes, but with important caveats that shaped the rest of my research.\nStar Jumps: A Perfect Test Case Why star jumps? They\u0026rsquo;re deceptively complex - requiring coordination of arms and legs, maintaining balance, and producing a recognizable pattern. Yet they\u0026rsquo;re simple enough to analyze rigorously. This choice let me focus on the core challenge: can model-based RL work for animation at all?\nUsing just 5 minutes of motion capture data, I trained agents that could: - Generate continuous star jump animations - Maintain synchronization between limbs - Adapt to different speeds and styles - Run in real-time (under 5ms per frame)\nModel-free vs Model-based: The Showdown The chapter presents a direct comparison between model-free (PPO) and model-based (Dreamer) approaches. The results were eye-opening:\nModel-free: Achieved the objective but produced unnatural, robotic movements. The agent found shortcuts that technically worked but looked wrong to human eyes.\nModel-based: Produced natural-looking animations that closely matched the motion capture data. The learned dynamics model acted as an implicit regularizer, keeping movements within the manifold of human motion.\nKey Insights This early work revealed three critical insights that guided the rest of the thesis:\n Dynamics models are regularizers: By learning how humans actually move, the model prevents unnatural animations Latent planning works: Planning in a learned latent space is both efficient and effective Physics isn\u0026rsquo;t necessary: For many animation tasks, learned dynamics are sufficient  Setting the Stage While successful, this chapter also exposed limitations - single-behavior agents, no interaction with physics, limited complexity. These limitations motivated the innovations in subsequent chapters, but the core finding stands: model-based RL is a viable paradigm for character animation.\nThe supplementary material shows these agents in action, demonstrating that even this early prototype could produce animations that feel genuinely human-like.\n","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"d9348a507e32dce7a3031f613bbe5fa6","permalink":"https://vihanga.github.io/thesis/chapters/model-based-animation/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/thesis/chapters/model-based-animation/","section":"thesis","summary":"Model-based Character Animation This chapter marks the beginning of my core contributions - demonstrating that model-based RL can work for character animation without physics simulation. It\u0026rsquo;s the proof-of-concept that sets the stage for everything that follows.\nThe Experiment That Started It All I started with a simple question: can an RL agent learn to animate a character using only a learned model of movement dynamics? No physics engine, no hand-crafted rewards - just data and learning.","tags":null,"title":"Chapter 4: Model-based Character Animation","type":"thesis"},{"authors":null,"categories":null,"content":" RLAnimate: Data-driven RL for Character Animation This is where everything comes together. Chapter 5 introduces RLAnimate - my core framework that revolutionizes how we think about RL for animation. Instead of rewards, we use motion capture data as objectives. It\u0026rsquo;s a simple idea with profound implications.\nThe Breakthrough: Objectives, Not Rewards Traditional RL for animation struggles with a fundamental problem: how do you write a reward function for \u0026ldquo;wave friendly\u0026rdquo; or \u0026ldquo;point naturally\u0026rdquo;? You can\u0026rsquo;t. Previous work tried complex reward engineering, but it always felt hacky.\nMy insight: we don\u0026rsquo;t need rewards at all. We have motion capture data showing humans performing these behaviors. So instead of rewarding specific outcomes, I train agents to match the dynamics of human movement. The \u0026ldquo;ideal action\u0026rdquo; at each timestep is simply what a human would do.\nThe A1 Architecture RLAnimate\u0026rsquo;s first architecture (A1) demonstrates the concept: - Behavior encoders that understand what motion the agent should perform - Perceptual encoders that process the current body state - Latent dynamics that model how movements unfold over time - Multi-behavior support - one agent, multiple skills\nThe results speak for themselves: agents that can wave, point, and transition between behaviors smoothly, all while maintaining human-like quality.\nMulti-Behavior Magic Previous systems needed separate agents for each behavior. RLAnimate changes that. By learning shared representations of movement dynamics, a single agent can: - Wave with different styles and speeds - Point at moving targets accurately - Blend behaviors naturally - Generalize to new variations\nThis isn\u0026rsquo;t just convenient - it\u0026rsquo;s essential for interactive applications where characters need to respond dynamically.\nThe Numbers That Matter  5ms inference time: Fast enough for any real-time application 93% behavior accuracy: Correctly performs requested behaviors Statistical parity: Movement statistics match human motion capture Efficient training: Learn from minutes, not hours, of mocap data  A2: Scaling Up The chapter also introduces A2, which adds physical grounding: - Foot contacts that respect physics - Momentum preservation during movement - Interaction with virtual objects - All while maintaining the ease of training\nWhy This Changes Everything RLAnimate isn\u0026rsquo;t just another animation method - it\u0026rsquo;s a new paradigm. By treating animation as matching human movement dynamics rather than optimizing rewards, we get: 1. Natural quality without reward engineering 2. Multi-behavior flexibility 3. Practical training times 4. Real-time performance\nThe supplementary materials showcase these agents in action, demonstrating capabilities that simply weren\u0026rsquo;t possible before. This is the foundation that enables everything in the following chapters - from finger animation to conversational gestures.\n","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"4caa4bfc750979cddab76cb7e35516cf","permalink":"https://vihanga.github.io/thesis/chapters/data-driven-rl/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/thesis/chapters/data-driven-rl/","section":"thesis","summary":"RLAnimate: Data-driven RL for Character Animation This is where everything comes together. Chapter 5 introduces RLAnimate - my core framework that revolutionizes how we think about RL for animation. Instead of rewards, we use motion capture data as objectives. It\u0026rsquo;s a simple idea with profound implications.\nThe Breakthrough: Objectives, Not Rewards Traditional RL for animation struggles with a fundamental problem: how do you write a reward function for \u0026ldquo;wave friendly\u0026rdquo; or \u0026ldquo;point naturally\u0026rdquo;?","tags":null,"title":"Chapter 5: RLAnimate - Data-driven RL for Character Animation","type":"thesis"},{"authors":null,"categories":null,"content":" Quaternions and Finger Animation Chapter 6 tackles one of animation\u0026rsquo;s nastiest problems: fingers. With 30+ joints that move in complex, coordinated patterns, finger animation has long been the bane of procedural methods. My solution? Quaternions and a dedicated animation dynamics model.\nWhy Fingers Matter (And Why They\u0026rsquo;re Hard) Watch someone wave - their fingers aren\u0026rsquo;t static. They curl, extend, and move with subtle patterns that make the gesture feel alive. Miss these details and even perfect arm movements feel robotic.\nThe challenge is combinatorial explosion. Each finger has multiple joints, each with constraints, and they all need to coordinate. Traditional methods either ignore fingers entirely or produce uncanny \u0026ldquo;spider hands\u0026rdquo; that distract from otherwise good animation.\nThe Quaternion Revolution The first breakthrough was switching from Euler angles to quaternions. This isn\u0026rsquo;t just a mathematical nicety - it fundamentally changes what the neural networks can learn:\n No gimbal lock: Fingers can rotate freely without singularities Smooth interpolation: Natural transitions between poses Efficient computation: 4 parameters instead of 9 for rotation matrices Better gradients: Networks train more stably  I developed a custom quaternion activation function that guarantees valid unit quaternions, eliminating a whole class of animation artifacts.\nA3: The Animation Dynamics Architecture The real innovation is the A3 architecture\u0026rsquo;s animation dynamics model. Instead of trying to force finger movements into the same model as body dynamics, A3 uses:\n Dedicated animation dynamics: A separate latent model just for animation generation Hierarchical dependencies: Fingers know what the wrist is doing Stochastic variation: Natural movement isn\u0026rsquo;t perfectly deterministic  This separation is key. Physical dynamics (for the body) and animation dynamics (for detailed movements) have different requirements. By acknowledging this, A3 achieves both physical plausibility and expressive detail.\nThe Results Speak (Or Rather, Wave) The numbers tell the story: - 5.34 point improvement in animation quality over A2 - All velocity errors under 0.4 - below human perceptual threshold - 30+ additional joints handled without performance degradation - Same 5ms inference time despite the added complexity\nBut watching the animations tells it better. A3 agents wave with natural finger curls, point with appropriate hand shapes, and maintain hand poses that feel human.\nTechnical Contributions Beyond the practical results, this chapter contributes: 1. Quaternion neural networks for animation 2. Hierarchical latent dynamics modeling 3. Animation-specific dynamics separate from physics 4. Proof that detail doesn\u0026rsquo;t require sacrificing speed\nWhy This Matters Fingers might seem like a detail, but they\u0026rsquo;re symptomatic of a larger challenge: how do we handle the full complexity of human movement? A3 shows that with the right representations and architecture, we can capture fine details while maintaining real-time performance.\nThis sets the stage for Chapter 7\u0026rsquo;s ultimate test - conversational gestures where every detail matters for believability.\n","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"b010e1faa4d04184493b3d5814db0f5e","permalink":"https://vihanga.github.io/thesis/chapters/latent-dynamics/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/thesis/chapters/latent-dynamics/","section":"thesis","summary":"Quaternions and Finger Animation Chapter 6 tackles one of animation\u0026rsquo;s nastiest problems: fingers. With 30+ joints that move in complex, coordinated patterns, finger animation has long been the bane of procedural methods. My solution? Quaternions and a dedicated animation dynamics model.\nWhy Fingers Matter (And Why They\u0026rsquo;re Hard) Watch someone wave - their fingers aren\u0026rsquo;t static. They curl, extend, and move with subtle patterns that make the gesture feel alive.","tags":null,"title":"Chapter 6: Quaternions and Finger Animation","type":"thesis"},{"authors":null,"categories":null,"content":" Beat Gestures - The Ultimate Test This chapter represents the culmination of everything - applying RLAnimate to conversational beat gestures. It\u0026rsquo;s the hardest test possible: can RL-generated animations be indistinguishable from real human gestures? The answer, validated through rigorous perceptual studies, is yes.\nWhy Beat Gestures Are Different Beat gestures - those rhythmic hand movements we make while speaking - are uniquely challenging: - They\u0026rsquo;re not goal-directed (unlike pointing or waving) - They must synchronize with speech prosody - They require subtle variety to feel natural - There\u0026rsquo;s no \u0026ldquo;correct\u0026rdquo; gesture - only human-like ones\nThis is where physics-based RL completely fails. There\u0026rsquo;s no physics objective for \u0026ldquo;emphasize this word appropriately.\u0026rdquo; Yet these gestures are crucial for believable virtual humans.\nThe A4 Architecture: Realism First A4 builds on A3 but adds my most important innovation: realism regularization. Four components work together:\n Rotation regularization: Prevents unnatural joint configurations Physics regularization: Respects momentum and energy conservation Smoothness regularization: Eliminates jittery movements Adversarial regularization: Learned realism from data  The key insight: by combining multiple forms of regularization, we achieve robustness. Any single method might fail, but together they create a strong prior for human-like movement.\nThe Perceptual Study: Proof in the Pudding I didn\u0026rsquo;t just claim success - I proved it. 24 participants watched 480 video clips comparing: - RLAnimate (A4) gestures - Gesticulator (state-of-the-art supervised learning) - Real motion capture\nThe results were stunning: - No significant difference between RLAnimate and motion capture (p \u0026gt; 0.31) - Significant improvement over Gesticulator (p \u0026lt; 0.001) - Participants literally couldn\u0026rsquo;t tell our animations from real humans\nTechnical Innovations Beyond the perceptual success, A4 contributes several technical advances:\nSpeech-motion synchronization: A hierarchical encoder that processes speech at multiple timescales, capturing both phoneme-level detail and phrase-level rhythm.\nStochastic generation: The same speech produces varied but appropriate gestures, just like humans.\nStyle control: Implicit style learning allows generating gestures in different \u0026ldquo;personalities.\u0026rdquo;\nThe Bigger Picture This chapter proves something profound: RL can match human quality for the most subjective, culturally-dependent aspects of movement. We\u0026rsquo;re not just animating characters - we\u0026rsquo;re capturing the essence of human nonverbal communication.\nThe implications extend beyond animation: - Virtual assistants that gesture naturally - More engaging educational avatars - Believable characters for therapy and training - A new baseline for what \u0026ldquo;human-like\u0026rdquo; means in AI\nWhy This Matters Beat gestures are the Turing test of character animation. They\u0026rsquo;re purely about human perception - there\u0026rsquo;s no objective metric, no physics to simulate, just the question: does this feel right?\nThat RLAnimate passes this test validates the entire approach. Motion capture as objectives, model-based learning, realism regularization - it all comes together to create something indistinguishable from human movement.\nThe supplementary material contains all 24 video stimuli from the perceptual study. Watch them yourself - can you tell which ones are generated?\n","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"b23e04332de28844bae789171e90c6e3","permalink":"https://vihanga.github.io/thesis/chapters/conversational-gestures/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/thesis/chapters/conversational-gestures/","section":"thesis","summary":"Beat Gestures - The Ultimate Test This chapter represents the culmination of everything - applying RLAnimate to conversational beat gestures. It\u0026rsquo;s the hardest test possible: can RL-generated animations be indistinguishable from real human gestures? The answer, validated through rigorous perceptual studies, is yes.\nWhy Beat Gestures Are Different Beat gestures - those rhythmic hand movements we make while speaking - are uniquely challenging: - They\u0026rsquo;re not goal-directed (unlike pointing or waving) - They must synchronize with speech prosody - They require subtle variety to feel natural - There\u0026rsquo;s no \u0026ldquo;correct\u0026rdquo; gesture - only human-like ones","tags":null,"title":"Chapter 7: Beat Gestures - The Ultimate Test","type":"thesis"},{"authors":null,"categories":null,"content":" Conclusions and Future Impact This thesis fundamentally changes how we think about character animation. By treating animation as a model-based RL problem with motion capture objectives, I\u0026rsquo;ve created a system that produces human-quality movement at real-time speeds. But this is just the beginning.\nWhat We\u0026rsquo;ve Achieved RLAnimate represents several breakthrough achievements:\nHuman-quality animation from RL: For the first time, RL agents generate movement that humans can\u0026rsquo;t distinguish from motion capture. This isn\u0026rsquo;t incremental improvement - it\u0026rsquo;s a paradigm shift.\nReal-time performance: 5ms per frame means these aren\u0026rsquo;t just research demos. They\u0026rsquo;re ready for games, VR, and interactive applications today.\nMulti-behavior flexibility: Single agents that wave, point, and gesture naturally - no behavior-specific engineering required.\nScalability to complexity: From simple waves to 30+ finger joints to speech-synchronized gestures, the approach scales.\nThe Technical Revolution Three key innovations make this possible:\n Motion capture as objectives: Eliminating reward engineering by learning directly from human examples Latent dynamics models: Efficient representations that capture movement essence Realism regularization: Multiple complementary methods ensuring human-like quality  Together, these create a framework that\u0026rsquo;s both theoretically principled and practically effective.\nWhy This Matters Beyond Animation The implications extend far beyond making pretty animations:\nFor AI: We\u0026rsquo;ve shown that complex, subjective human behaviors can be learned without explicit programming. The approach could apply to any domain where we have examples but can\u0026rsquo;t write rules.\nFor HCI: Natural movement is crucial for acceptance of virtual agents. This work enables a new generation of interfaces that communicate through body language.\nFor Science: By learning what makes movement \u0026ldquo;human-like,\u0026rdquo; we\u0026rsquo;re gaining insights into human motor control and social signaling.\nLimitations and Honesty No system is perfect. Current limitations include: - Training requires motion capture data (though much less than alternatives) - Style control is implicit rather than parametric - Physical interactions remain challenging - Cultural gesture variations need more exploration\nThese aren\u0026rsquo;t fundamental barriers - they\u0026rsquo;re the next research challenges.\nThe Road Ahead This thesis opens several exciting directions:\nBehavioral complexity: Extending to full-body social interactions, emotional expressions, and context-aware responses.\nZero-shot generalization: Learning movement principles that transfer across characters and scenarios without retraining.\nIntegration with LLMs: Imagine language models that don\u0026rsquo;t just speak but move naturally as they communicate.\nReal-world robotics: The principles could enable robots that move in ways humans find natural and non-threatening.\nA Personal Vision I believe we\u0026rsquo;re at an inflection point. Just as deep learning revolutionized computer vision, model-based RL with human objectives will revolutionize character animation. We\u0026rsquo;re moving from \u0026ldquo;making characters move\u0026rdquo; to \u0026ldquo;bringing characters to life.\u0026rdquo;\nThe tools are here. The methods work. What we create with them - more engaging games, more effective education, more natural human-computer interaction - is limited only by imagination.\nFinal Thoughts This thesis asked whether RL could create human-like animation without physics simulation or reward engineering. The answer is definitively yes. But more importantly, it\u0026rsquo;s shown a path forward for creating AI systems that capture the subtlety and beauty of human movement.\nVirtual characters that move like us aren\u0026rsquo;t just technically impressive - they\u0026rsquo;re emotionally resonant. They make technology feel more human. In a world increasingly mediated by screens and virtual interactions, that\u0026rsquo;s not just an academic achievement. It\u0026rsquo;s a step toward technology that truly understands and reflects our humanity.\n","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"8c2b9df60fa3dd5ba941a75a5fd50439","permalink":"https://vihanga.github.io/thesis/chapters/conclusion/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/thesis/chapters/conclusion/","section":"thesis","summary":"Conclusions and Future Impact This thesis fundamentally changes how we think about character animation. By treating animation as a model-based RL problem with motion capture objectives, I\u0026rsquo;ve created a system that produces human-quality movement at real-time speeds. But this is just the beginning.\nWhat We\u0026rsquo;ve Achieved RLAnimate represents several breakthrough achievements:\nHuman-quality animation from RL: For the first time, RL agents generate movement that humans can\u0026rsquo;t distinguish from motion capture.","tags":null,"title":"Chapter 8: Conclusions and Future Impact","type":"thesis"},{"authors":null,"categories":null,"content":" Chapter 4: Model-Based Character Animation This page contains supplementary material referenced in Chapter 4 of the thesis, demonstrating the model-based reinforcement learning agents trained to portray gazing and pointing behaviors.\nVideo Demonstrations Model-Based Animation Agents (AMSTA 2021) The following video demonstrates the agents trained using learned dynamics models and online planning for target-driven behaviors:\n  This video shows: - Gazing behavior with dynamic target tracking - Pointing behavior with both left and right arms - Combined gaze and point behaviors - Real-time responsiveness to changing targets - Comparison with inverse kinematics baseline\nKey Results Demonstrated  Computational Efficiency: 5ms per frame (40x faster than IK baseline) Dynamic Flexibility: Agents adapt to moving targets without computational overhead Multi-behavior Support: Single agent architecture supports gaze, point, and combined behaviors Beta Distribution Planning: Novel approach to smooth animation generation  Navigation  Chapter 5 Part A: Model-free RL → ← Back to thesis main page Chapter 4 in main thesis  ","date":1737504000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1737504000,"objectID":"be2d5c51703a1556b5dcc0c8d7a08c43","permalink":"https://vihanga.github.io/thesis/supplementary/c4/","publishdate":"2025-01-22T00:00:00Z","relpermalink":"/thesis/supplementary/c4/","section":"thesis","summary":"Chapter 4: Model-Based Character Animation This page contains supplementary material referenced in Chapter 4 of the thesis, demonstrating the model-based reinforcement learning agents trained to portray gazing and pointing behaviors.\nVideo Demonstrations Model-Based Animation Agents (AMSTA 2021) The following video demonstrates the agents trained using learned dynamics models and online planning for target-driven behaviors:\n  This video shows: - Gazing behavior with dynamic target tracking - Pointing behavior with both left and right arms - Combined gaze and point behaviors - Real-time responsiveness to changing targets - Comparison with inverse kinematics baseline","tags":null,"title":"Chapter 4: Supplementary Material","type":"thesis"},{"authors":null,"categories":null,"content":" Chapter 5 Part A: RLAnimate Output Sequences This page contains supplementary material for Chapter 5, demonstrating the RLAnimate A1 agents trained to portray waving and pointing behaviors.\nVideo Demonstrations RLAnimate Output Sequences and Comparison to Control Agents (ALA 2021) The following video shows RLAnimate agents performing waving and pointing behaviors, with comparisons to baseline control agents:\n  This video demonstrates: - A1 agent performing waving behaviors with varying exaggeration levels - A1 agent performing pointing behaviors to different targets - Comparison with single dynamics baseline - Comparison with supervised learning baseline - Natural, human-like motion generation\nKey Results Shown  Multi-behavior Support: Single agent portraying both waving and pointing Dynamic Control: Real-time adaptation to behavior parameters Human-like Quality: Smooth, natural movements without artifacts Superior Performance: Outperforms baselines on test set (87.23 vs 70.94)  Navigation  Chapter 5 Part B: Behavior Flexibility → ← Back to thesis main page Chapter 5 in main thesis  ","date":1737504000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1737504000,"objectID":"684c8c6eb974cdedac7413b0677c73bb","permalink":"https://vihanga.github.io/thesis/supplementary/c5a/","publishdate":"2025-01-22T00:00:00Z","relpermalink":"/thesis/supplementary/c5a/","section":"thesis","summary":"Chapter 5 Part A: RLAnimate Output Sequences This page contains supplementary material for Chapter 5, demonstrating the RLAnimate A1 agents trained to portray waving and pointing behaviors.\nVideo Demonstrations RLAnimate Output Sequences and Comparison to Control Agents (ALA 2021) The following video shows RLAnimate agents performing waving and pointing behaviors, with comparisons to baseline control agents:\n  This video demonstrates: - A1 agent performing waving behaviors with varying exaggeration levels - A1 agent performing pointing behaviors to different targets - Comparison with single dynamics baseline - Comparison with supervised learning baseline - Natural, human-like motion generation","tags":null,"title":"Chapter 5 Supplementary Material - Part A: RLAnimate Output Sequences","type":"thesis"},{"authors":null,"categories":null,"content":" Chapter 5 Part B: RLAnimate Behavior Portrayal Flexibility This page contains supplementary material for Chapter 5, demonstrating the flexibility and dynamic control capabilities of RLAnimate agents.\nVideo Demonstrations RLAnimate Behavior Portrayal Flexibility (ALA 2021) The following video demonstrates the dynamic flexibility of RLAnimate agents in portraying varied behaviors:\n  This video shows: - Waving with continuously variable exaggeration levels - Pointing to dynamically changing targets - Smooth transitions between behaviors - Real-time responsiveness to parameter changes - Natural variation in movement execution\nKey Capabilities Demonstrated  Continuous Control: Smooth interpolation between behavior parameters Dynamic Adaptation: Real-time response to changing objectives Natural Variation: Human-like variability in repeated behaviors Behavior Transitions: Seamless switching between waving and pointing  Navigation  ← Chapter 5 Part A: Output Sequences Chapter 6 Supplementary Material → ← Back to thesis main page Chapter 5 in main thesis  ","date":1737504000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1737504000,"objectID":"4533a70804b1d00a48ea76985216082e","permalink":"https://vihanga.github.io/thesis/supplementary/c5b/","publishdate":"2025-01-22T00:00:00Z","relpermalink":"/thesis/supplementary/c5b/","section":"thesis","summary":"Chapter 5 Part B: RLAnimate Behavior Portrayal Flexibility This page contains supplementary material for Chapter 5, demonstrating the flexibility and dynamic control capabilities of RLAnimate agents.\nVideo Demonstrations RLAnimate Behavior Portrayal Flexibility (ALA 2021) The following video demonstrates the dynamic flexibility of RLAnimate agents in portraying varied behaviors:\n  This video shows: - Waving with continuously variable exaggeration levels - Pointing to dynamically changing targets - Smooth transitions between behaviors - Real-time responsiveness to parameter changes - Natural variation in movement execution","tags":null,"title":"Chapter 5 Supplementary Material - Part B: Behavior Flexibility","type":"thesis"},{"authors":null,"categories":null,"content":" Chapter 6: Latent Dynamic-augmented Animation Output This page contains supplementary material referenced in Chapter 6 of the thesis, demonstrating the A3 architecture with quaternion rotations and animation dynamics for finger animation.\nA3 Architecture Results The A3 architecture introduced in Chapter 6 addresses the challenge of incorporating finger animation through:\n Quaternion Representations: Eliminating gimbal lock and improving computational efficiency Animation Dynamics Model: Dedicated latent dynamics for animation generation Hierarchical Learning: Better capture of joint dependencies  Performance Improvements  A2 + Quaternion baseline: 81.23 similarity score A3 Standard: 86.57 similarity score (5.34 point improvement) A3 with all latents: 86.19 similarity score  All velocity errors remain under the 0.4 perceptual threshold, confirming that the A3 architecture successfully maintains motion quality while handling the additional complexity of finger animation. As noted in the thesis (Section 6.3), visual analysis confirms that the A3 architecture maintains animation quality despite the increased complexity of animating 30+ finger joints.\nTechnical Contributions Quaternion Activation Function The novel quaternion activation function ensures all neural network outputs are valid unit quaternions:\nf(w,x,y,z) = [w,x,y,z] / sqrt(w² + x² + y² + z²)  This eliminates singularities (except at origin) and provides smooth gradients for training.\nAnimation Dynamics Model The dedicated animation dynamics model learns hierarchical relationships between joints through: - Deterministic state: ht^a = f(h{t-1}^a, s_{t-1}, h_t^p, p_t, h_t^b, b_t) - Stochastic component for natural variation - Temporal consistency through recurrent processing\nA3 Agent Capabilities The A3 agents demonstrate: - Successful incorporation of finger movements in waving behaviors - Natural finger positioning during pointing tasks - Smooth transitions without artifacts - Maintained performance under the 0.4 velocity error threshold\nNavigation  ← Chapter 5 Part B: Behavior Flexibility Chapter 7 Supplementary Material → ← Back to thesis main page Chapter 6 in main thesis  ","date":1737504000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1737504000,"objectID":"10c1b5d05179a4d74b11d745a8d44096","permalink":"https://vihanga.github.io/thesis/supplementary/c6/","publishdate":"2025-01-22T00:00:00Z","relpermalink":"/thesis/supplementary/c6/","section":"thesis","summary":"Chapter 6: Latent Dynamic-augmented Animation Output This page contains supplementary material referenced in Chapter 6 of the thesis, demonstrating the A3 architecture with quaternion rotations and animation dynamics for finger animation.\nA3 Architecture Results The A3 architecture introduced in Chapter 6 addresses the challenge of incorporating finger animation through:\n Quaternion Representations: Eliminating gimbal lock and improving computational efficiency Animation Dynamics Model: Dedicated latent dynamics for animation generation Hierarchical Learning: Better capture of joint dependencies  Performance Improvements  A2 + Quaternion baseline: 81.","tags":null,"title":"Chapter 6 Supplementary Material: Quaternions and Finger Animation","type":"thesis"},{"authors":null,"categories":null,"content":" Chapter 7: Portraying Conversational Gestures via Realism Regularisation This page contains supplementary material referenced in Chapter 7, including the video stimuli used in the perceptual evaluation study comparing RLAnimate A4, Gesticulator, and motion capture.\nPerceptual Study Video Stimuli The following videos were used in the perceptual evaluation with 28 participants. Each stimulus set was rendered with three methods: RLAnimate A4, Gesticulator, and Motion Capture (ground truth).\n .video-set { margin-bottom: 50px; background: #f5f5f5; padding: 20px; border-radius: 8px; } .video-grid { display: grid; grid-template-columns: repeat(3, 1fr); gap: 20px; margin-bottom: 20px; } .video-container { position: relative; } .video-container h4 { text-align: center; margin-bottom: 10px; font-size: 1.1em; color: #333; } .video-container video { width: 100%; border-radius: 4px; box-shadow: 0 2px 4px rgba(0,0,0,0.1); } .play-controls { text-align: center; margin-top: 20px; } .play-button { background-color: #007bff; color: white; border: none; padding: 12px 30px; font-size: 16px; border-radius: 4px; cursor: pointer; transition: background-color 0.3s; } .play-button:hover { background-color: #0056b3; } .play-button.playing { background-color: #dc3545; } .play-button.playing:hover { background-color: #c82333; } @media (max-width: 768px) { .video-grid { grid-template-columns: 1fr; } }   function toggleVideos(setId) { const videos = document.querySelectorAll(`#${setId} video`); const button = document.querySelector(`#${setId} .play-button`); const isPlaying = videos[0].paused; videos.forEach(video = { if (isPlaying) { video.play(); } else { video.pause(); } }); if (isPlaying) { button.textContent = '⏸ Pause All'; button.classList.add('playing'); } else { button.textContent = '▶ Play All'; button.classList.remove('playing'); } } // Sync video playback positions function syncVideos(setId) { const videos = document.querySelectorAll(`#${setId} video`); const masterVideo = videos[0]; masterVideo.addEventListener('timeupdate', () = { videos.forEach((video, index) = { if (index !== 0 \u0026\u0026 Math.abs(video.currentTime - masterVideo.currentTime)  0.1) { video.currentTime = masterVideo.currentTime; } }); }); // Reset all videos when one ends videos.forEach(video = { video.addEventListener('ended', () = { videos.forEach(v = { v.currentTime = 0; v.pause(); }); const button = document.querySelector(`#${setId} .play-button`); button.textContent = '▶ Play All'; button.classList.remove('playing'); }); }); } // Initialize when page loads document.addEventListener('DOMContentLoaded', function() { const sets = ['setA', 'setB', 'setC', 'setD', 'setE', 'setF', 'setG', 'setH']; sets.forEach(setId = { if (document.getElementById(setId)) { syncVideos(setId); } }); });  Stimulus Set A RLAnimate A4   Gesticulator   Motion Capture    ▶ Play All   Stimulus Set B RLAnimate A4   Gesticulator   Motion Capture    ▶ Play All   Stimulus Set C RLAnimate A4   Gesticulator   Motion Capture    ▶ Play All   Stimulus Set D RLAnimate A4   Gesticulator   Motion Capture    ▶ Play All   Stimulus Set E RLAnimate A4   Gesticulator   Motion Capture    ▶ Play All   Stimulus Set F RLAnimate A4   Gesticulator   Motion Capture    ▶ Play All   Stimulus Set G RLAnimate A4   Gesticulator   Motion Capture    ▶ Play All   Stimulus Set H RLAnimate A4   Gesticulator   Motion Capture    ▶ Play All   Perceptual Study Results Statistical Results RLAnimate A4 vs Gesticulator (one-sided t-tests, α = 0.05): - Human-likeness: 4.33 vs 3.19 (p \u0026lt; 0.001) ✓ - Speech reflection: 4.12 vs 3.42 (p \u0026lt; 0.001) ✓ - Synchronization: 4.48 vs 3.60 (p \u0026lt; 0.001) ✓\nRLAnimate A4 vs Motion Capture (two-sided t-tests, α = 0.05): - Human-likeness: 4.33 vs 4.65 (p = 0.083, NS) - Speech reflection: 4.12 vs 4.38 (p = 0.174, NS) - Synchronization: 4.48 vs 4.67 (p = 0.268, NS)\nKey Finding RLAnimate A4 achieves statistical parity with human motion capture while significantly outperforming the state-of-the-art Gesticulator system across all evaluation criteria.\nNavigation  ← Chapter 6 Supplementary Material ← Back to thesis main page Chapter 7 in main thesis  ","date":1737504000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1737504000,"objectID":"d370faf13eef94886084b843b011fa53","permalink":"https://vihanga.github.io/thesis/supplementary/c7/","publishdate":"2025-01-22T00:00:00Z","relpermalink":"/thesis/supplementary/c7/","section":"thesis","summary":"Chapter 7: Portraying Conversational Gestures via Realism Regularisation This page contains supplementary material referenced in Chapter 7, including the video stimuli used in the perceptual evaluation study comparing RLAnimate A4, Gesticulator, and motion capture.\nPerceptual Study Video Stimuli The following videos were used in the perceptual evaluation with 28 participants. Each stimulus set was rendered with three methods: RLAnimate A4, Gesticulator, and Motion Capture (ground truth).\n .video-set { margin-bottom: 50px; background: #f5f5f5; padding: 20px; border-radius: 8px; } .","tags":null,"title":"Chapter 7 Supplementary Material: Beat Gestures","type":"thesis"},{"authors":null,"categories":null,"content":" Examining the effects of a virtual character on learning and engagement in serious games Authors: Vihanga Gamage, Cathy Ennis\nConference: MIG \u0026lsquo;18: Motion, Interaction and Games, November 8–10, 2018, Limassol, Cyprus\nDOI: 10.1145\u0026frasl;3274247.3274499\nAbstract Virtual characters have been employed for many purposes including interacting with players of serious games, with a purpose to increase engagement. These characters are often embodied conversational agents playing diverse roles, such as demonstrators, guides, teachers or interviewers. Recently, much research has been conducted into properties that affect the realism and plausibility of virtual characters, but it is less clear whether the inclusion of interactive agents in serious applications can enhance a user\u0026rsquo;s engagement with the application, or indeed increase efficacy. In a first step towards answering these questions, we conducted a study where a Virtual Learning Environment was used to examine the effect of employing a virtual character to deliver a lesson. In order to investigate whether increased familiarity between the player and the character would help achieve learning outcomes, we allowed participants to customize the physical appearance of the character. We used direct and indirect measures to assess engagement and learning; we measured knowledge retention to ascertain learning via a test at the end of the lesson, and also measured participants\u0026rsquo; perceived engagement with the lesson. Our findings show that a virtual character can be an effective learning aid, causing heightened engagement and retention of knowledge. However, allowing participants to customize character appearance resulted in inhibited engagement, which was contrary to what we expected.\nKey Findings  Enhanced Engagement: Users showed significantly higher engagement when lessons were delivered through a virtual character compared to non-character controls.\n Improved Knowledge Retention: Virtual character delivery resulted in better knowledge retention compared to traditional presentation methods.\n Attention Focus: User attention was predominantly directed at the virtual character, with limited attention to other environmental elements.\n Customization Paradox: Contrary to expectations, character appearance personalization led to significantly lower engagement compared to default appearances.\n  Relevance to Thesis This work laid important groundwork for understanding how virtual characters affect user engagement and learning. While this paper focused on static virtual characters in educational contexts, it motivated the later development of RLAnimate for creating more dynamic, responsive virtual characters that can adapt their behaviors in real-time - addressing some of the limitations identified in this study.\nLinks  Paper PDF (ACM Digital Library) Conference presentation thoughts and feedback MIG\u0026rsquo;18 conference blog post Back to thesis  ","date":1737504000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1737504000,"objectID":"91a78759d0b2980c5906b19f9be44565","permalink":"https://vihanga.github.io/thesis/publications/MIG18/","publishdate":"2025-01-22T00:00:00Z","relpermalink":"/thesis/publications/MIG18/","section":"thesis","summary":"Examining the effects of a virtual character on learning and engagement in serious games Authors: Vihanga Gamage, Cathy Ennis\nConference: MIG \u0026lsquo;18: Motion, Interaction and Games, November 8–10, 2018, Limassol, Cyprus\nDOI: 10.1145\u0026frasl;3274247.3274499\nAbstract Virtual characters have been employed for many purposes including interacting with players of serious games, with a purpose to increase engagement. These characters are often embodied conversational agents playing diverse roles, such as demonstrators, guides, teachers or interviewers.","tags":null,"title":"MIG'18 Paper: Virtual Characters in Serious Games","type":"thesis"},{"authors":["Vihanga Gamage"],"categories":[],"content":"Originally published on RTÉ Brainstorm\nThe solar system is full of debris and rocks floating around. A meteor shower or a shooting star is debris burning up upon entry to the atmosphere. On occasion, pieces of rock make it all the way through. A much larger rock that could make its way through is an asteroid. About 66 million years ago, the impact from an asteroid brought about the Cretaceous extinction. A repetition of this event is theoretically possible - and the same could be said about all-powerful Artificial Intelligence Overlords marginalising the human race.\nSince its inception in the mid 20th century, the field of artificial intelligence has had an interesting ride. Much like the financial markets or Hollywood, there’s been breakthroughs and failures and booms and busts. Driven by advances in deep learning, a great deal of success has been achieved this decade, ushering in the newest AI golden age, along with a frenzy of interest and investment.\nThe current popular views of AI are being shaped primarily by several vocal figures in technology. Microsoft founder Bill Gates has said that AI carries the potential to change society deeply and is both exciting and dangerous. IBM CEO Ginni Rometty has stated that she believes advances in AI will create different jobs rather than no jobs. Facebook founder Mark Zuckerberg has expressed the view that AI will deliver various improvements to human quality of life, while Tesla founder Elon Musk has called AI \u0026ldquo;a fundamental risk to the existence of human civilisation\u0026rdquo;.\nThe future dangers AI could pose to humanity is a frequently occurring subject in conversation, be it to break the ice on a first date, to fill the time before a meeting starts or while waiting for a pint at the pub on a Friday evening. And arguably, Musk\u0026rsquo;s statement carries the most bite. It certainly is a very catchy line that makes for a more topical conversation starter than asteroids or the Cretaceous extinction.\nAs a result, these hypothetical yet sensationalist doomsday opinions are at a point where they dominate the conversation about AI, so much so that the bigger picture on the subject may at the point of being overshadowed and given very little consideration. And that could pose an even greater threat, especially in an age where hype and rhetoric without regard for fact has had a great effect on the fate of the world.\nAdvancements in AI could lead to a superintelligence that could pose a fundamental risk to human existence - and so could climate change or a 5 km-wide space rock colliding with the Earth. But there are many points of difference between these three risks. AI is a long way away advancing to the point of superintelligence and it’s not a certainty that superintelligence would lead to the end of humanity. Climate change is a very real and present danger, with the time to take action to prevent irreversible consequences continuing to decrease. And then there are the chances of an asteroid striking the earth.\nOf course, there is no good that can come from an asteroid strike or global warming, but AI can literally be one of the best things to happen to humans. This is not said enough - and it can’t be said enough. AI can and is being used to as a tool in many incredible ways. For example, it is being used to help improve prevention of cancer, provide effective treatment, and could one day even be part of the solution.\nAI is also being leveraged as a promising way to combat climate change. AI-powered simulations and analysis helps make for sustainable urban planning and ocean preservation. It is possible to make more accurate predictions regarding weather events with the help of AI leading to reduced damage to life and property. It can help power grids to be more energy efficient by predicting peak over of usage allowing for better preparation and enhances the benefits of clean energy methods by deploying in ways such as to incorporate weather and other relevant data to make wind turbines more efficient.\nAnd yet arguments on how AI could hypothetically end humanity as we know it at some point in the future, seem to drown out how AI is already helping humanity’s cause. Such contributions pale in comparison to the very real possibility that AI could be the key to finding the answers for some of humanity’s biggest problems.\nAt a 2018 hearing at the US House of Representatives Committee on Science, Space and Technology, Stanford Professor Fei-Fei Li said that AI is bound to alter the human experience, and not necessarily for the better. After acknowledging this, Li pointed out that ensuring AI will transform the world for the better is very much in our hands. \u0026ldquo;There\u0026rsquo;s nothing artificial about AI\u0026rdquo;, she said.\n \u0026ldquo;It’s inspired by people; it’s created by people, and—most importantly—it impacts people. It is a powerful tool we are only just beginning to understand, and that is a profound responsibility.\u0026rdquo;\n Concern that AI could pose a threat is very much reasonable. But this possibility is just that - a possibility. Let’s not forget AI is a fundamental opportunity to further the cause of the human civilisation, and that it is in our hands to ensure that it impacts us positively, and be optimistic about what it can mean for us. The world can only be the better place for it.\n","date":1566061379,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1566061379,"objectID":"a75ccec893290ac194456b20ef62ffa9","permalink":"https://vihanga.github.io/post/blog/AI-Oppurtunity-not-danger/","publishdate":"2019-08-17T17:02:59Z","relpermalink":"/post/blog/AI-Oppurtunity-not-danger/","section":"post","summary":"Originally published on RTÉ Brainstorm\nThe solar system is full of debris and rocks floating around. A meteor shower or a shooting star is debris burning up upon entry to the atmosphere. On occasion, pieces of rock make it all the way through. A much larger rock that could make its way through is an asteroid. About 66 million years ago, the impact from an asteroid brought about the Cretaceous extinction.","tags":["blog"],"title":"Why AI is an opportunity rather than a danger","type":"post"},{"authors":["Vihanga Gamage"],"categories":[],"content":"\u0026lt;\u0026lt; restricted access to posting \u0026gt;\u0026gt;\n","date":1542474179,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1542474179,"objectID":"8ea6b228c1d9f27f638832de44099fc4","permalink":"https://vihanga.github.io/post/research/MIG18PaperStudy/","publishdate":"2018-11-17T17:02:59Z","relpermalink":"/post/research/MIG18PaperStudy/","section":"post","summary":"\u0026lt;\u0026lt; restricted access to posting \u0026gt;\u0026gt;","tags":["research"],"title":"A pilot study into virtual character application as pedagogical agents","type":"post"},{"authors":["Vihanga Gamage"],"categories":[],"content":"\u0026lt;\u0026lt; restricted access to posting \u0026gt;\u0026gt;\n","date":1542474179,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1542474179,"objectID":"e46c42befa06751c790f5912e50fc00e","permalink":"https://vihanga.github.io/post/blog/MIG18blog/","publishdate":"2018-11-17T17:02:59Z","relpermalink":"/post/blog/MIG18blog/","section":"post","summary":"\u0026lt;\u0026lt; restricted access to posting \u0026gt;\u0026gt;","tags":["blog"],"title":"Motion, Interaction and Games 2018","type":"post"},{"authors":["Vihanga Gamage"],"categories":[],"content":"   After the horrible start to the season I had at the City of Dublin in September, I was desperate to turn my form and results around. I had signed up to play in as much fixtures as I can for the club\u0026rsquo;s second team who plays in the third division of the Leinster Chess Union leagues, and played in all three opening fixtures.\nAnd will you look at that, the results are quite alright.\n  Funnily enough, I seem to have played three consecutive games with the black pieces due to playing on three different boards, and the team did alright especially given our standing on joint second after 2 wins and a narrow loss to one of the highest rated teams in the division.\n  Match 1 : Opening game vs Elm Mount - A very lucky escape. Very lucky indeed. This was a bit of a weird one. I was playing black, on board 5, fairly confident, decent position coming out of the opening, and I was in a mood that day. So, when it looked like there was potential to sacrifice a piece on h3, I was fairly focussed on this.\nAt this, position, I was so preoccupied whether to take h3 with the bishop on c8 or knight on f4, I didn\u0026rsquo;t notice that my opponents last move, Nf1, opens an attack from the bishop on c1 to my knight on f4.\n After a lot of deliberation in my head, I took h3 with the Bishop, and I didn\u0026rsquo;t notice my blunder till my opponent took the piece on f4.\nAfter that blunder, I thought my best chance of recovering was to capitalize on whatever little attacking potential I had in this position, and I forged ahead with my plan to put his castle under pressure. My rationale here was that it was easier for me to get both my rooks involved in an assault on his castle than it was for him to make his rook on a1 active, and maybe I might be able to overwhelm his pieces.\nAnd as luck would have it, that\u0026rsquo;s just what happened.\n Presumably, due to the perceived threat from f4 that would make open the file for my rook on f8 and further weaken his king, he made the move Qd2.\n..Nxg2, he responded with Nf1, again, as what I presume was a hail mary move in the hopes I might overlook a threat on that diagonal, again. But I didn\u0026rsquo;t, he resigned after my next move Nf4+ as the only continuation lead to either losing his queen or checkmate.\n I was glad I got away with the win after that blunder, the team got a 4-2 win in the opening fixture. We went to the pub afterwards, played ping pong while having pints, and watched Man United pull off that awesome 3-2 comeback win against Newcastle.\nOverall, it was an alright day, but it could have just gone very, very, wrong, for myself, just as well as for United and Jose Mourinho.\nMatch 2 : Converting an advantage with surprising efficiency Next up was a game against Lucan, all the other 5 games had been played the week before; my board had been deferred as an old friend was visiting me the day of the match. The match was balanced on 2.5-2.5 with 5 games played, and I was playing their captain on board 4.\nThe opening was nothing special, left a nice evenly balanced position coming into the midgame. \nMy move here was to castle, rather than the engine suggested move of capturing the e4 pawn, but the open file was what lead to my advantage, given my opponent sought to make a rash advance with his rook to apply pressure on my castle.\n But in this position, ...Nc5. Qc3, Ne6. R7xe6,Bxe6. And I was up a rook to a bishop. It had been a recent pattern of mine to play out some fidgety plans leading to drawn positions even when I had the advantage, or even devolve into a loss, but I played through to a winning endgame in what was, on recent form, uncharacteristically surprising efficiency.  It would\u0026rsquo;ve been nicer if I had spotted the forced mating sequence here:\n\u0026hellip;e4+. Kf4, Rgf2+. Kg4, Rf3. Rf7+, Kxf7. Kg5, Rg2+. Kh6, Rh3. c7, Rxh4#\nBut I had less than a minute on the clock, so I was playing on increments, so it\u0026rsquo;s entirely justifiable that I did things the old fashioned way.\n My opponent ran out of time here, when I was winning quite comfortably; and the team scored a 3.5-2.5 win.\nMatch 3 : A fast 15 move draw It was a Monday evening, I was again playing a couple of days after the match, one of two differred boards. I had just flew back to Dublin the evening before from the MIG\u0026rsquo;18 conference in Cyprus.\nI had popped into a pub near the playing venue to get a pint, and opened my Macbook to finish up some emails when I see a window with my Twitter home feed showing that Stan Lee had died.\nI really wasn\u0026rsquo;t in the mood to play a game after seeing this, 10 minutes before the start time. I went in, my oppoennet played the Polish opening, that was a first. And I was more than happy to play a line that lead to a draw.\n Looking at the position, it can be said that I was let off, as my isolated pawn and overall position isn\u0026rsquo;t likely to lead to a whole lot of joy. My move here would\u0026rsquo;ve been Qd6 or Qc7, and it wouldn\u0026rsquo;t have been the hardest to defend here. Qd6 might even allow for some attacking momentum if I can get my knight involved. My opponent seemed to agree, and it seemed he had studied the opening and likely positions pretty thoroughly; also the engine concurred with this analysis.\nBut, as my opponent said, we could\u0026rsquo;ve played for a couple of hours and ended up drawn anyway, or agreed a draw early on that basis and gone home early.\nWe lost the game 2.5-3.5, but given that league standings are based on game points rather than match points, it\u0026rsquo;s not the end of the world. We are tied second on points in the league standings, we seem quite competitive for a newly promoted team with an average rating that is on the latter end in the league.\nIf we can keep this up, come April, the team could be in an interesting position.\n  And, oh hello, a 55 point boost to my FIDE rating. This was a nice surprise, thanks to the K-Factor of 40, as it\u0026rsquo;s less than 30 of my games were FIDE rated.\n  I had initially planned to play in the Kilkenny Weekender, where I played my first competitive chess since moving to Ireland, for the third consecutive year. But with the fatigue from the travel and the conference and with some heavy weeks ahead of me at work, it seemed the right idea to sit this year out.\nI\u0026rsquo;m likely to play at 2 more games in 2018, with our last Ennis shield fixture again Skerries, and a substitute appearance for the first time this season in the Armstrong Cup for the first team.\nMy form seems to be improving after the blunders in the City of Dublin weekender and the less than ideal play early in the first league match. This is a good sign for my potential performance at the 9 round closed all-play-all in early January, that I entered for.\n","date":1542474179,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1542474179,"objectID":"8a6c13decec0c37ebef0cef29eb5a232","permalink":"https://vihanga.github.io/post/chess/2018LeagueEnnisR123/","publishdate":"2018-11-17T17:02:59Z","relpermalink":"/post/chess/2018LeagueEnnisR123/","section":"post","summary":"After the horrible start to the season I had at the City of Dublin in September, I was desperate to turn my form and results around. I had signed up to play in as much fixtures as I can for the club\u0026rsquo;s second team who plays in the third division of the Leinster Chess Union leagues, and played in all three opening fixtures.\nAnd will you look at that, the results are quite alright.","tags":["chess"],"title":"The form awakens* (*seems to stir awake from a 4 month slumber) - 2.5/3 : Rounds 1, 2 \u0026 3 of the Ennis Shield (Leinster League Division 3)","type":"post"},{"authors":["Vihanga Gamage, Cathy Ennis"],"categories":null,"content":"I presented my first conference paper at the 11th Motion, Interactions and Games conference held in November 2018. In this paper, we examined the broad effects a virtual character can have on user experience and performance when incorporated as a pedagogical agent in a serious game. We also explored character appearance personalization as a potential way of heightening engagement with the character.\nOur findings can be summaries as follows: - A user is more engaged with a lesson when delivered through a virtual character compared to a non-character control version. - A user retains knowledge better when a lesson is delivered through a virtual character compared to a non-character control version. - There is a propensity for the majority of user attention to be directed at the virtual character, resulting in little attention being paid to the rest of the game environment. - Surprising, character appearance personalization has an effect opposite to what we expected, resulting in participants being significantly less engaged with a personalized character compare to one with a default appearance.\nSupplementary material in the form of the full questionnaire used in the study and free-form user feedback and some interesting points raised by attendees at the conference following my thoughts can be found here, and a blog post about my experience at the conference and some photographs can be found here.\n","date":1542473248,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1542473248,"objectID":"c6965129ead2fdcac70fc04d9f02f39e","permalink":"https://vihanga.github.io/publication/MIG18/","publishdate":"2018-11-17T16:47:28Z","relpermalink":"/publication/MIG18/","section":"publication","summary":"Virtual characters have been employed for many purposes including interacting with players of serious games, with a purpose to increase engagement. These characters are often embodied conversational agents playing diverse roles, such as demonstrators, guides, teachers or interviewers. Recently, much research has been conducted into properties that affect the realism and plausibility of virtual characters, but it is less clear whether the inclusion of interactive agents in serious applications can enhance a user's engagement with the application, or indeed increase efficacy. In a first step towards answering these questions, we conducted a study where a Virtual Learning Environment was used to examine the effect of employing a virtual character to deliver a lesson. In order to investigate whether increased familiarity between the player and the character would help achieve learning outcomes, we allowed participants to customize the physical appearance of the character. We used direct and indirect measures to assess engagement and learning; we measured knowledge retention to ascertain learning via a test at the end of the lesson, and also measured participants' perceived engagement with the lesson. Our findings show that a virtual character can be an effective learning aid, causing heightened engagement and retention of knowledge. However, allowing participants to customize character appearance resulted in inhibited engagement, which was contrary to what we expected.","tags":[],"title":"Examining the effects of a virtual character on learning and engagement in serious games","type":"publication"},{"authors":["Vihanga Gamage"],"categories":[],"content":"\u0026lt;\u0026lt; restricted access to posting \u0026gt;\u0026gt;\u0026hellip;.\n","date":1542128579,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1542128579,"objectID":"70e8f7b753362d5f38eacdfd83b8feae","permalink":"https://vihanga.github.io/post/blog/StanLee/","publishdate":"2018-11-13T17:02:59Z","relpermalink":"/post/blog/StanLee/","section":"post","summary":"\u0026lt;\u0026lt; restricted access to posting \u0026gt;\u0026gt;\u0026hellip;.","tags":["blog"],"title":"Excelsior!","type":"post"},{"authors":["Vihanga Gamage"],"categories":[],"content":" \u0026ldquo;My soul is painted like the wings of butterflies, fairy tales of yesterday will grow but never die. I can fly, my friends\u0026rdquo;\nThe Show Must Go On, Queen (1991)\u0026rdquo;\n \u0026lt;\u0026lt; restricted access to posting \u0026gt;\u0026gt;\n","date":1540832579,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1540832579,"objectID":"26178ba6ce0c35ef14bd61c03b23aa50","permalink":"https://vihanga.github.io/post/blog/BohemianRhapsody/","publishdate":"2018-10-29T17:02:59Z","relpermalink":"/post/blog/BohemianRhapsody/","section":"post","summary":"\u0026ldquo;My soul is painted like the wings of butterflies, fairy tales of yesterday will grow but never die. I can fly, my friends\u0026rdquo;\nThe Show Must Go On, Queen (1991)\u0026rdquo;\n \u0026lt;\u0026lt; restricted access to posting \u0026gt;\u0026gt;","tags":["blog"],"title":"\"....fairy tales of yesterday will grow but never die.\" , My thoughts on Bohemian Rhapsody, the Queen biopic","type":"post"},{"authors":["Vihanga Gamage"],"categories":[],"content":"  The 2018 / 2019 ICU season kicked off at the City of Dublin weekender held in September. This season was one I approached with as much intent to perform well, as I had done in a while. This is because I will be taking a step back from playing relatively seriously, at the end of the season, for about a year to focus on the last year of my PhD.\nDespite my showing at the Irish Championship Supporting events in August not being as convincing as I had hoped after a few good performances earlier in the year, I was at least happy with the fact that I had consolidated the 180 point rating jump I achieved at the Ennis Open in May.\nI was seeded in the top 10 of the draw in my section. The first round I was playing back against a solid player. After a fairly even middle game phase, I managed to capitalise on my opponent\u0026rsquo;s misstep to go into the end game with a winning position.\nand the stage was set\u0026hellip;\u0026hellip;for a trademark pawn ending screw up from me. This has been happening a lot recently.\nIn my defence, I had studied up on pawn ending theory that I was continually getting wrong, and I had a couple of minutes plus increments on the clock, but this was pretty much inexcusable.\nIn this position, it was my move, and instead of c5, I played Bxc3 leading the game to a dead draw.\n Bxc3, Bxc3 Kxc3, Ke3 Kb3, Kxe4 Kxa3, f4\nand hold on to your hats people because here comes the height of lunacy. Instead of calculating properly and making c4, Kxb4 ??????\nand that was it. First game of the season, arguably the worst game I played since February.\n The next morning, I picked up right where I left off, a toothless game and a dumb theoretical blunder towards the end.\nTwo games, both against players rated about 200 points beyond me, my rating had already dropped by 50.\nIt did not seem like I could get out of the rut I was in, and a tailspin seemed imminent, so to avoid a dropping an even hundred more rating points, I withdrew at this point.\nMy next competitive game is likely going to be the first match round of the new league season. I will be playing the full league campaign for the Dublin University Ennis Shield team; here\u0026rsquo;s hoping I plug the holes in my endgame and mid-game play isn\u0026rsquo;t so flat.\n","date":1539795779,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1539795779,"objectID":"1b2d65a0738b96dd92c771c08112086e","permalink":"https://vihanga.github.io/post/chess/2018CoD/","publishdate":"2018-10-17T17:02:59Z","relpermalink":"/post/chess/2018CoD/","section":"post","summary":"The 2018 / 2019 ICU season kicked off at the City of Dublin weekender held in September. This season was one I approached with as much intent to perform well, as I had done in a while. This is because I will be taking a step back from playing relatively seriously, at the end of the season, for about a year to focus on the last year of my PhD.","tags":["chess"],"title":"A calamitous start to the season - 0/2 at the City of Dublin 2018","type":"post"},{"authors":null,"categories":null,"content":" Vihanga Gamage\u0026rsquo;s Public Calendar  ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"a631103cf606bbead447236a29c6889e","permalink":"https://vihanga.github.io/calendar/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/calendar/","section":"","summary":" Vihanga Gamage\u0026rsquo;s Public Calendar  ","tags":null,"title":"","type":"page"}]