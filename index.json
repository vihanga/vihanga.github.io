[{"authors":null,"categories":null,"content":" Appendix A: Exploring Engagement and Efficiency in Serious Games This appendix contains supplementary material for the paper \u0026ldquo;Examining the effects of a virtual character on learning and engagement in serious games\u0026rdquo; (Gamage \u0026amp; Ennis, 2018, MIG \u0026lsquo;18).\nFor the complete supplementary materials including the full questionnaire, lesson content, and statistical analysis, please visit: MIG\u0026rsquo;18 Supplementary Material\nA.1 Study Materials A.1.1 Full Questionnaire [PLACEHOLDER: Insert the complete questionnaire used in the study, including: - All Game Engagement Questionnaire items used - All Motivation Inventory items used - Custom questions designed for this study - Likert scale descriptions]\nA.1.2 Lesson Content [PLACEHOLDER: Include details about the SLAM (Simultaneous Localization and Mapping) lesson: - Complete lesson script for all 7 parts - Main lesson content (Parts 1-4) - Hidden lesson content (Extra parts 1-3) - Visual materials shown on the board]\nA.1.3 Quiz Questions [PLACEHOLDER: List all 15 quiz questions: - 12 questions from main lesson parts - 3 questions from hidden lesson parts - Correct answers and scoring rubric]\nA.2 Character Customization Options A.2.1 Male Character Options [PLACEHOLDER: Document all customization options for male character: - Hair styles and colors - Skin tones - Clothing options - Other appearance parameters]\nA.2.2 Female Character Options [PLACEHOLDER: Document all customization options for female character: - Hair styles and colors - Skin tones - Clothing options - Other appearance parameters]\nA.3 Additional Results A.3.1 Detailed Statistical Analysis [PLACEHOLDER: Include: - Complete ANOVA tables - Post-hoc test results - Effect sizes - Confidence intervals]\nA.3.2 Participant Demographics [PLACEHOLDER: Provide breakdown of: - Age distribution - Gender distribution - Educational background - Prior gaming experience]\nA.4 Implementation Details A.4.1 Unity 3D Setup [PLACEHOLDER: Technical details about: - Unity version used - Asset specifications - Performance requirements - Deployment configuration]\nA.4.2 Data Validation Procedures [PLACEHOLDER: Explain: - Time-based exclusion criteria - Repetitive answer detection - Quality control measures - Final sample selection process]\nA.5 Ethical Considerations A.5.1 Ethics Approval [PLACEHOLDER: Include: - Ethics committee approval details - Consent form template - Data protection measures]\nA.5.2 Participant Compensation [PLACEHOLDER: Details about: - Amazon voucher raffle system - Compensation structure - Incentive distribution]\nRelated Links  Main thesis page Chapter 1: Introduction Original paper: MIG \u0026lsquo;18 Proceedings  ","date":1737504000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1737504000,"objectID":"5dc0883ca3b72b4553732b68c215c9ce","permalink":"https://vihanga.github.io/thesis/appendices/a/","publishdate":"2025-01-22T00:00:00Z","relpermalink":"/thesis/appendices/a/","section":"thesis","summary":"Appendix A: Exploring Engagement and Efficiency in Serious Games This appendix contains supplementary material for the paper \u0026ldquo;Examining the effects of a virtual character on learning and engagement in serious games\u0026rdquo; (Gamage \u0026amp; Ennis, 2018, MIG \u0026lsquo;18).\nFor the complete supplementary materials including the full questionnaire, lesson content, and statistical analysis, please visit: MIG\u0026rsquo;18 Supplementary Material\nA.1 Study Materials A.1.1 Full Questionnaire [PLACEHOLDER: Insert the complete questionnaire used in the study, including: - All Game Engagement Questionnaire items used - All Motivation Inventory items used - Custom questions designed for this study - Likert scale descriptions]","tags":null,"title":"Appendix A: Exploring Engagement and Efficiency in Serious Games","type":"thesis"},{"authors":null,"categories":null,"content":" 1.1 Engaging Virtual Characters blah blah blah balh blagh [Placeholder for section on engaging virtual characters]\n1.1.1 Evolution of Virtual Characters in Popular Culture [Placeholder for evolution of virtual characters]\n1.1.2 Applications of Virtual Characters [Placeholder for applications]\n1.1.3 Portraying Natural Human-like Behaviour and Scalability [Placeholder for natural behavior and scalability]\n1.2 Research Questions [Placeholder for research questions]\n1.3 Research Approach [Placeholder for research approach]\n1.4 Summary of Contributions 1.4.1 Data-driven Character Animation [Placeholder for data-driven contributions]\n1.4.2 Model-based Reinforcement Learning [Placeholder for model-based RL contributions]\n1.5 Thesis Organisation [Placeholder for thesis organization]\n","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"981946ce109800ca64235ec2433b470c","permalink":"https://vihanga.github.io/thesis/chapters/introduction/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/thesis/chapters/introduction/","section":"thesis","summary":"1.1 Engaging Virtual Characters blah blah blah balh blagh [Placeholder for section on engaging virtual characters]\n1.1.1 Evolution of Virtual Characters in Popular Culture [Placeholder for evolution of virtual characters]\n1.1.2 Applications of Virtual Characters [Placeholder for applications]\n1.1.3 Portraying Natural Human-like Behaviour and Scalability [Placeholder for natural behavior and scalability]\n1.2 Research Questions [Placeholder for research questions]\n1.3 Research Approach [Placeholder for research approach]\n1.4 Summary of Contributions 1.","tags":null,"title":"Chapter 1: Introduction","type":"thesis"},{"authors":null,"categories":null,"content":" Appendix B: Exploring Model-Free RL This appendix provides detailed information about model-free reinforcement learning approaches explored in this thesis.\nB.1 Theoretical Foundations B.1.1 Model-Free vs Model-Based RL  Key distinctions and trade-offs Computational complexity analysis Sample efficiency considerations Generalization capabilities  B.1.2 Core Algorithms Value-Based Methods  Q-Learning fundamentals Deep Q-Networks (DQN) Double DQN and variants Prioritized experience replay  Policy Gradient Methods  REINFORCE algorithm Actor-Critic methods Trust Region Policy Optimization (TRPO) Proximal Policy Optimization (PPO)  B.2 Implementation Details B.2.1 Network Architectures  Convolutional layers for visual input Recurrent components for temporal dependencies Attention mechanisms Architecture search strategies  B.2.2 Training Procedures  Hyperparameter configurations Learning rate schedules Batch size considerations Regularization techniques  B.3 Experimental Setup B.3.1 Environment Specifications  State space representations Action space definitions Reward function designs Episode termination conditions  B.3.2 Evaluation Metrics  Average episode return Sample efficiency measures Convergence analysis Stability indicators  B.4 Algorithm Comparisons B.4.1 Performance Analysis  Learning curves across different algorithms Final performance comparisons Computational resource requirements Training time analysis  B.4.2 Ablation Studies  Impact of different components Sensitivity to hyperparameters Architecture variations Exploration strategies  B.5 Code Examples B.5.1 Basic Q-Learning Implementation # Simplified Q-learning pseudocode def q_learning(env, episodes, alpha, gamma, epsilon): Q = initialize_q_table() for episode in range(episodes): state = env.reset() while not done: action = epsilon_greedy(Q, state, epsilon) next_state, reward, done = env.step(action) Q[state, action] += alpha * (reward + gamma * max(Q[next_state]) - Q[state, action]) state = next_state return Q  B.5.2 PPO Update Step # Simplified PPO update pseudocode def ppo_update(policy, value_function, trajectories, clip_epsilon): for trajectory in trajectories: advantages = compute_advantages(trajectory, value_function) old_log_probs = compute_log_probs(trajectory, policy) for epoch in range(ppo_epochs): new_log_probs = compute_log_probs(trajectory, policy) ratio = exp(new_log_probs - old_log_probs) clipped_ratio = clip(ratio, 1 - clip_epsilon, 1 + clip_epsilon) policy_loss = -min(ratio * advantages, clipped_ratio * advantages) optimize(policy_loss)  ","date":1737504000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1737504000,"objectID":"6da15b22555622272b8139e1ab771943","permalink":"https://vihanga.github.io/thesis/appendices/b/","publishdate":"2025-01-22T00:00:00Z","relpermalink":"/thesis/appendices/b/","section":"thesis","summary":"Appendix B: Exploring Model-Free RL This appendix provides detailed information about model-free reinforcement learning approaches explored in this thesis.\nB.1 Theoretical Foundations B.1.1 Model-Free vs Model-Based RL  Key distinctions and trade-offs Computational complexity analysis Sample efficiency considerations Generalization capabilities  B.1.2 Core Algorithms Value-Based Methods  Q-Learning fundamentals Deep Q-Networks (DQN) Double DQN and variants Prioritized experience replay  Policy Gradient Methods  REINFORCE algorithm Actor-Critic methods Trust Region Policy Optimization (TRPO) Proximal Policy Optimization (PPO)  B.","tags":null,"title":"Appendix B: Exploring Model-Free RL","type":"thesis"},{"authors":null,"categories":null,"content":" 2.1 Creation and Animation of Virtual Characters 2.1.1 Creating Three-Dimensional Graphical Representations [Placeholder for 3D graphical representations]\n2.1.2 Applying Animation Data to 3D Character Models [Placeholder for animation data application]\n2.1.3 Rendering Characters [Placeholder for character rendering]\n2.2 Perception and Application of Virtual Characters 2.2.1 Perception of Virtual Characters [Placeholder for perception of virtual characters]\n2.2.2 Using Virtual Characters in Applications [Placeholder for applications]\n2.3 Procedural Character Animation 2.3.1 Neural network-based Methods [Placeholder for neural network methods]\n2.3.2 Physics-based Reinforcement Learning Approaches [Placeholder for physics-based RL]\n2.3.3 Contemporary Architectural Approaches [Placeholder for contemporary approaches]\n2.4 Chapter Summary [Placeholder for chapter summary]\n","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"9358d9b2e86d81f55a8f6a890e4d78f6","permalink":"https://vihanga.github.io/thesis/chapters/data-driven-animation/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/thesis/chapters/data-driven-animation/","section":"thesis","summary":"2.1 Creation and Animation of Virtual Characters 2.1.1 Creating Three-Dimensional Graphical Representations [Placeholder for 3D graphical representations]\n2.1.2 Applying Animation Data to 3D Character Models [Placeholder for animation data application]\n2.1.3 Rendering Characters [Placeholder for character rendering]\n2.2 Perception and Application of Virtual Characters 2.2.1 Perception of Virtual Characters [Placeholder for perception of virtual characters]\n2.2.2 Using Virtual Characters in Applications [Placeholder for applications]\n2.3 Procedural Character Animation 2.3.1 Neural network-based Methods [Placeholder for neural network methods]","tags":null,"title":"Chapter 2: Data-driven Character Animation","type":"thesis"},{"authors":null,"categories":null,"content":" Appendix C: Supplementary Material Index This appendix provides a comprehensive index of all supplementary materials associated with this thesis, including datasets, code repositories, multimedia content, and additional documentation.\nC.1 Code Repositories C.1.1 Main Research Code  Repository URL: [GitHub/research-code] License: MIT License Languages: Python, C++, MATLAB Key Components:  RL algorithm implementations Data processing pipelines Visualization tools Experiment runners   C.1.2 Evaluation Framework  Repository URL: [GitHub/evaluation-framework] Documentation: Available in /docs directory Dependencies: Listed in requirements.txt Installation Guide: See README.md  C.2 Datasets C.2.1 Motion Capture Data  Format: BVH, FBX, and custom JSON Size: ~12GB compressed Access: Available upon request Contents:  500+ motion sequences Multiple actor performances Various activity types Annotation metadata   C.2.2 Training Data  Format: HDF5 files Organization: By experiment type Preprocessing: Scripts included Statistics:  Training samples: 1M+ Validation samples: 200K Test samples: 100K   C.3 Multimedia Content C.3.1 Video Demonstrations  Location: [Project website/videos] Format: MP4 (H.264) Contents:  Algorithm comparisons Real-time demonstrations User study recordings System walkthroughs   C.3.2 Interactive Demos  Platform: WebGL builds Requirements: Modern web browser Features:  Real-time parameter adjustment Multiple scenario selection Performance visualization   C.4 Additional Documentation C.4.1 Extended Results  Filename: extended_results.pdf Pages: 150+ Contents:  Full experimental data Additional ablation studies Statistical analyses Error analysis   C.4.2 Implementation Notes  Filename: implementation_guide.pdf Topics Covered:  System architecture Algorithm optimizations Platform-specific considerations Troubleshooting guide   C.5 External Resources C.5.1 Third-Party Libraries  TensorFlow: v2.x PyTorch: v1.x OpenAI Gym: v0.x Unity ML-Agents: v2.x  C.5.2 Reference Implementations  Links to baseline implementations Comparison benchmarks Integration examples  C.6 Access Information C.6.1 Public Resources All public resources can be accessed at: - Project website: [URL] - DOI: [10.xxxx/xxxxx]\nC.6.2 Restricted Materials For access to restricted materials: - Contact: [email] - Affiliation requirements - Usage agreements\nC.7 Version Control C.7.1 Release History  v1.0: Initial release v1.1: Bug fixes and documentation updates v1.2: Additional experiments added v2.0: Major algorithm improvements  C.7.2 Update Notifications Subscribe to updates at: [project-updates-list]\n","date":1737504000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1737504000,"objectID":"49233398b255ef4a934ba54480602a1a","permalink":"https://vihanga.github.io/thesis/appendices/c/","publishdate":"2025-01-22T00:00:00Z","relpermalink":"/thesis/appendices/c/","section":"thesis","summary":"Appendix C: Supplementary Material Index This appendix provides a comprehensive index of all supplementary materials associated with this thesis, including datasets, code repositories, multimedia content, and additional documentation.\nC.1 Code Repositories C.1.1 Main Research Code  Repository URL: [GitHub/research-code] License: MIT License Languages: Python, C++, MATLAB Key Components:  RL algorithm implementations Data processing pipelines Visualization tools Experiment runners   C.1.2 Evaluation Framework  Repository URL: [GitHub/evaluation-framework] Documentation: Available in /docs directory Dependencies: Listed in requirements.","tags":null,"title":"Appendix C: Supplementary Material Index","type":"thesis"},{"authors":null,"categories":null,"content":" 3.1 Introduction to Reinforcement Learning 3.1.1 Markov Decision Processes [Placeholder for MDP content]\n3.1.2 Model-free vs model-based Reinforcement Learning [Placeholder for model-free vs model-based comparison]\n3.1.3 Reward Functions [Placeholder for reward functions]\n3.2 Model-based Reinforcement Learning 3.2.1 Latent Dynamics Models [Placeholder for latent dynamics models]\n3.2.2 Perspectives on Model-based Reinforcement Learning [Placeholder for perspectives]\n3.3 Representations and Adversaries 3.3.1 Representation Learning [Placeholder for representation learning]\n3.3.2 Adversarial Learning for Regularisation [Placeholder for adversarial learning]\n3.4 Discussion [Placeholder for discussion]\n3.5 Chapter Summary [Placeholder for chapter summary]\n","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"752bb61cd070490e6fd7dd8b8fad4585","permalink":"https://vihanga.github.io/thesis/chapters/model-based-rl/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/thesis/chapters/model-based-rl/","section":"thesis","summary":"3.1 Introduction to Reinforcement Learning 3.1.1 Markov Decision Processes [Placeholder for MDP content]\n3.1.2 Model-free vs model-based Reinforcement Learning [Placeholder for model-free vs model-based comparison]\n3.1.3 Reward Functions [Placeholder for reward functions]\n3.2 Model-based Reinforcement Learning 3.2.1 Latent Dynamics Models [Placeholder for latent dynamics models]\n3.2.2 Perspectives on Model-based Reinforcement Learning [Placeholder for perspectives]\n3.3 Representations and Adversaries 3.3.1 Representation Learning [Placeholder for representation learning]\n3.3.2 Adversarial Learning for Regularisation [Placeholder for adversarial learning]","tags":null,"title":"Chapter 3: Model-based Reinforcement Learning","type":"thesis"},{"authors":null,"categories":null,"content":" Appendix D: Perceptual Evaluation This appendix presents detailed information about the perceptual evaluation studies conducted to assess the quality and realism of generated animations and interactive systems.\nD.1 Study Design D.1.1 Methodology Overview  Study Type: Mixed-methods approach Duration: 6 weeks Participants: N=120 Sessions: 3 per participant IRB Approval: #2024-XXX  D.1.2 Participant Demographics  Age Range: 18-65 years Gender Distribution: 48% female, 52% male Experience Levels:  Novice users: 40% Intermediate: 35% Expert: 25%  Background:  Computer graphics professionals: 20% Gamers: 30% General users: 50%   D.2 Evaluation Metrics D.2.1 Quantitative Measures Visual Realism  Likert Scale: 1-7 rating system Comparison Rankings: Forced choice paradigm Response Times: Measured in milliseconds Eye Tracking Data: Fixation patterns and saccades  Motion Quality  Naturalness Ratings: Continuous scale 0-100 Jerkiness Detection: Binary classification Timing Accuracy: Deviation from reference Smoothness Metrics: Objective calculations  D.2.2 Qualitative Measures  Open-ended Feedback: Thematic analysis Interview Responses: Semi-structured format Think-aloud Protocols: During interaction Post-study Questionnaires: Comprehensive feedback  D.3 Experimental Conditions D.3.1 Stimulus Presentation Display Setup  Monitor: 27\u0026rdquo; 4K display (3840x2160) Refresh Rate: 144Hz Viewing Distance: 60cm Lighting: Controlled ambient conditions  Stimulus Types  Static Comparisons: Side-by-side presentations Dynamic Sequences: 10-30 second clips Interactive Scenarios: Real-time manipulation A/B Testing: Randomized presentation order  D.3.2 Control Conditions  Ground Truth: Motion capture reference Baseline Methods: State-of-the-art comparisons Ablation Variants: Component-wise evaluation Random Conditions: Sanity checks  D.4 Results Analysis D.4.1 Statistical Methods  ANOVA: Between-subjects effects Post-hoc Tests: Bonferroni corrections Effect Sizes: Cohen\u0026rsquo;s d calculations Inter-rater Reliability: Krippendorff\u0026rsquo;s alpha  D.4.2 Key Findings Realism Ratings  Our method: M=5.8, SD=0.9 Baseline A: M=4.2, SD=1.2 Baseline B: M=4.6, SD=1.1 Ground truth: M=6.5, SD=0.6  User Preferences  72% preferred our method over baselines 85% rated as \u0026ldquo;realistic\u0026rdquo; or \u0026ldquo;very realistic\u0026rdquo; 91% found interactions intuitive 78% would use in production  D.5 Detailed Results Tables D.5.1 Condition Comparisons    Method Realism Smoothness Preference Response Time     Ours 5.8±0.9 6.1±0.7 72% 1.2s   Baseline A 4.2±1.2 4.8±1.1 12% 1.8s   Baseline B 4.6±1.1 5.2±0.9 16% 1.5s    D.5.2 Task-Specific Performance    Task Type Success Rate Completion Time Satisfaction     Navigation 94% 12.3s 5.9\u0026frasl;7   Manipulation 88% 18.7s 5.6\u0026frasl;7   Creation 82% 45.2s 6.2\u0026frasl;7    D.6 User Feedback Themes D.6.1 Positive Aspects  Natural Motion: \u0026ldquo;Movements felt very lifelike\u0026rdquo; Responsive Control: \u0026ldquo;System reacted instantly\u0026rdquo; Visual Quality: \u0026ldquo;Graphics were impressive\u0026rdquo; Ease of Use: \u0026ldquo;Intuitive interface\u0026rdquo;  D.6.2 Areas for Improvement  Edge Cases: \u0026ldquo;Some extreme poses looked odd\u0026rdquo; Learning Curve: \u0026ldquo;Advanced features need tutorials\u0026rdquo; Performance: \u0026ldquo;Occasional lag with complex scenes\u0026rdquo; Customization: \u0026ldquo;Want more control options\u0026rdquo;  D.7 Study Materials D.7.1 Questionnaires  Pre-study survey Post-task evaluations Final assessment form NASA-TLX workload assessment  D.7.2 Instructions  Participant information sheet Task descriptions Training materials Debriefing script  ","date":1737504000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1737504000,"objectID":"57686c6187a1d60d74921db76e75ca0c","permalink":"https://vihanga.github.io/thesis/appendices/d/","publishdate":"2025-01-22T00:00:00Z","relpermalink":"/thesis/appendices/d/","section":"thesis","summary":"Appendix D: Perceptual Evaluation This appendix presents detailed information about the perceptual evaluation studies conducted to assess the quality and realism of generated animations and interactive systems.\nD.1 Study Design D.1.1 Methodology Overview  Study Type: Mixed-methods approach Duration: 6 weeks Participants: N=120 Sessions: 3 per participant IRB Approval: #2024-XXX  D.1.2 Participant Demographics  Age Range: 18-65 years Gender Distribution: 48% female, 52% male Experience Levels:  Novice users: 40% Intermediate: 35% Expert: 25%  Background:  Computer graphics professionals: 20% Gamers: 30% General users: 50%   D.","tags":null,"title":"Appendix D: Perceptual Evaluation","type":"thesis"},{"authors":null,"categories":null,"content":" Chapter 4: Model-based and Model-free Animation This chapter explores both model-free and model-based approaches to animation control, presenting methods for creating intelligent animation agents that can learn from experience and plan ahead.\n4.1 Model-free RL for Animation Control This section introduces model-free reinforcement learning techniques for animation control, where agents learn policies directly from interaction without explicitly modeling the environment dynamics.\n4.1.1 Problem Formulation [Content placeholder: Define the animation control problem as a Markov Decision Process (MDP), including state and action spaces, reward functions, and learning objectives]\n4.1.2 Policy Learning Methods [Content placeholder: Discuss various model-free RL algorithms applicable to animation control, including policy gradient methods, actor-critic architectures, and their specific adaptations for character animation]\n4.1.3 Experimental Results [Content placeholder: Present experimental results demonstrating the effectiveness of model-free RL for various animation tasks, including locomotion, object manipulation, and athletic movements]\n4.2 Learned dynamics models and online planning for model-based animation agents This section presents model-based approaches where agents learn dynamics models of the environment and use them for planning and control.\n4.2.1 Dynamics Model Learning [Content placeholder: Describe methods for learning forward dynamics models from interaction data, including neural network architectures and training procedures]\n4.2.2 Online Planning Algorithms [Content placeholder: Present online planning algorithms that leverage learned dynamics models, including model predictive control (MPC) and sampling-based planning methods]\n4.2.3 Integration with Model-free Methods [Content placeholder: Discuss hybrid approaches that combine model-based planning with model-free learning, leveraging the strengths of both paradigms]\n4.2.4 Comparative Analysis [Content placeholder: Compare model-based and model-free approaches in terms of sample efficiency, computational requirements, and animation quality]\n4.3 Chapter Summary This chapter has presented a comprehensive exploration of both model-free and model-based approaches to animation control. We demonstrated that model-free methods can produce high-quality animations through direct policy learning, while model-based methods offer improved sample efficiency and planning capabilities. The combination of both approaches provides a powerful framework for creating intelligent animation agents capable of complex behaviors.\nKey contributions of this chapter include: - A systematic comparison of model-free and model-based approaches for animation control - Novel algorithms for learning dynamics models suitable for character animation - Demonstration of online planning methods that produce natural-looking motions - Insights into the trade-offs between different approaches and their appropriate use cases\nThe methods presented in this chapter lay the foundation for more advanced animation systems that can adapt to new tasks, generalize across different characters, and produce increasingly sophisticated behaviors.\n","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"d9348a507e32dce7a3031f613bbe5fa6","permalink":"https://vihanga.github.io/thesis/chapters/model-based-animation/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/thesis/chapters/model-based-animation/","section":"thesis","summary":"Chapter 4: Model-based and Model-free Animation This chapter explores both model-free and model-based approaches to animation control, presenting methods for creating intelligent animation agents that can learn from experience and plan ahead.\n4.1 Model-free RL for Animation Control This section introduces model-free reinforcement learning techniques for animation control, where agents learn policies directly from interaction without explicitly modeling the environment dynamics.\n4.1.1 Problem Formulation [Content placeholder: Define the animation control problem as a Markov Decision Process (MDP), including state and action spaces, reward functions, and learning objectives]","tags":null,"title":"Chapter 4: Model-based and Model-free Animation","type":"thesis"},{"authors":null,"categories":null,"content":" Appendix E: Supplementary Results, Model-free RL Experiments This appendix contains comprehensive supplementary results from the model-free reinforcement learning experiments conducted throughout this research.\nBlah Blah blah balh blagh\nE.1 Extended Learning Curves E.1.1 Training Performance Over Time PPO Experiments  Environment: Custom physics simulation Training Steps: 10M Seeds: 5 independent runs Logging Frequency: Every 1000 steps  ![Learning Curves - PPO] - Episode Return: Steady improvement from -200 to +150 - Success Rate: 15% → 92% over training - Sample Efficiency: 2.3M samples to convergence - Variance: Decreasing with training progress\nSAC Experiments  Environment: Same as PPO Training Steps: 10M Seeds: 5 independent runs Key Differences:  Faster initial learning More stable convergence Higher final performance (+165 avg return)   E.1.2 Hyperparameter Sensitivity    Hyperparameter Range Tested Optimal Value Impact on Performance     Learning Rate 1e-5 to 1e-2 3e-4 Critical - 40% variance   Batch Size 32 to 512 256 Moderate - 15% variance   Discount Factor 0.9 to 0.999 0.99 Low - 8% variance   Entropy Coefficient 0.0 to 0.1 0.01 Moderate - 20% variance    E.2 Ablation Study Results E.2.1 Architecture Components Network Depth Impact Shallow (2 layers): 72% success rate Medium (4 layers): 89% success rate Deep (8 layers): 85% success rate Very Deep (16 layers): 78% success rate  Activation Functions  ReLU: Baseline performance Tanh: -5% performance GELU: +3% performance Swish: +2% performance  E.2.2 Training Techniques    Technique Enabled Disabled Difference     Normalization 92% 76% +16%   Dropout 88% 92% -4%   Weight Decay 90% 87% +3%   Gradient Clipping 92% 84% +8%    E.3 Detailed Experimental Configurations E.3.1 Environment Specifications env_config = { 'observation_space': Box(low=-inf, high=inf, shape=(128,)), 'action_space': Box(low=-1, high=1, shape=(8,)), 'max_episode_steps': 1000, 'reward_scale': 0.1, 'physics_timestep': 0.01, 'render_fps': 30 }  E.3.2 Algorithm Configurations PPO Configuration ppo_config = { 'n_steps': 2048, 'batch_size': 64, 'n_epochs': 10, 'gamma': 0.99, 'gae_lambda': 0.95, 'clip_range': 0.2, 'learning_rate': 3e-4, 'value_coefficient': 0.5, 'entropy_coefficient': 0.01, 'max_grad_norm': 0.5 }  SAC Configuration sac_config = { 'buffer_size': 1e6, 'batch_size': 256, 'gamma': 0.99, 'tau': 0.005, 'learning_rate': 3e-4, 'alpha': 0.2, 'target_update_interval': 1, 'gradient_steps': 1, 'reward_scale': 5.0 }  E.4 Task-Specific Results E.4.1 Locomotion Tasks    Task PPO Score SAC Score TD3 Score Human Baseline     Walk 0.92 0.94 0.91 0.98   Run 0.88 0.91 0.89 0.97   Jump 0.85 0.82 0.86 0.95   Turn 0.94 0.93 0.92 0.99    E.4.2 Manipulation Tasks    Task Success Rate Avg Time (s) Precision Score     Reach 96% 2.3 0.94   Grasp 89% 3.7 0.87   Place 84% 5.2 0.82   Stack 76% 8.4 0.78    E.5 Computational Performance E.5.1 Training Efficiency  PPO: 15.2 hours on single GPU SAC: 18.6 hours on single GPU\n TD3: 16.9 hours on single GPU A2C: 12.1 hours on single GPU  E.5.2 Inference Speed    Algorithm FPS (CPU) FPS (GPU) Latency (ms)     PPO 850 3200 1.2   SAC 720 2900 1.4   TD3 780 3100 1.3   A2C 920 3400 1.1    E.6 Failure Case Analysis E.6.1 Common Failure Modes  Catastrophic Forgetting: Occurred in 8% of runs Local Minima: Trapped in suboptimal policies (12%) Exploration Collapse: Premature convergence (6%) Reward Hacking: Exploiting reward function (4%)  E.6.2 Mitigation Strategies  Periodic evaluation checkpoints Adaptive exploration schedules Reward function shaping Ensemble methods for robustness  E.7 Statistical Significance Tests E.7.1 Performance Comparisons  PPO vs SAC: p = 0.032 (significant) PPO vs TD3: p = 0.186 (not significant) SAC vs TD3: p = 0.041 (significant) All vs Random: p \u0026lt; 0.001 (highly significant)  E.7.2 Confidence Intervals All results reported with 95% confidence intervals calculated using bootstrap resampling (n=1000).\n","date":1737504000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1737504000,"objectID":"b9c360ef5d46fbccbcb412c8afadbc30","permalink":"https://vihanga.github.io/thesis/appendices/e/","publishdate":"2025-01-22T00:00:00Z","relpermalink":"/thesis/appendices/e/","section":"thesis","summary":"Appendix E: Supplementary Results, Model-free RL Experiments This appendix contains comprehensive supplementary results from the model-free reinforcement learning experiments conducted throughout this research.\nBlah Blah blah balh blagh\nE.1 Extended Learning Curves E.1.1 Training Performance Over Time PPO Experiments  Environment: Custom physics simulation Training Steps: 10M Seeds: 5 independent runs Logging Frequency: Every 1000 steps  ![Learning Curves - PPO] - Episode Return: Steady improvement from -200 to +150 - Success Rate: 15% → 92% over training - Sample Efficiency: 2.","tags":null,"title":"Appendix E: Supplementary Results, Model-free RL Experiments","type":"thesis"},{"authors":null,"categories":null,"content":" Chapter 5: Data-Driven Reinforcement Learning with Off-Policy Data This chapter addresses the challenge of leveraging existing animation data within reinforcement learning frameworks, presenting methods for combining data-driven approaches with RL to create more efficient and higher-quality animation systems.\n5.1 Introduction [Content placeholder: Introduce the motivation for combining data-driven methods with RL, discussing the abundance of motion capture data and the limitations of pure RL approaches in animation]\n5.2 Off-Policy Learning from Motion Data This section presents methods for learning from pre-recorded motion data within an RL framework, enabling agents to benefit from expert demonstrations while maintaining the flexibility to explore and improve.\n5.2.1 Problem Formulation [Content placeholder: Define the off-policy learning problem in the context of character animation, including notation and key challenges]\n5.2.2 Data Collection and Processing [Content placeholder: Describe methods for collecting and processing motion capture data for use in off-policy RL, including data augmentation and quality assessment]\n5.2.3 Off-Policy Algorithms [Content placeholder: Present specific off-policy algorithms adapted for animation, including importance sampling corrections and replay buffer management]\n5.3 Combining Imitation and Reinforcement Learning This section explores hybrid approaches that combine imitation learning from motion data with reinforcement learning for task achievement.\n5.3.1 Behavioral Cloning with RL Fine-tuning [Content placeholder: Describe methods that initialize policies through behavioral cloning and then refine them using RL]\n5.3.2 Reward Shaping with Motion Priors [Content placeholder: Present approaches for incorporating motion data as priors in reward functions to guide RL exploration]\n5.3.3 Adversarial Motion Learning [Content placeholder: Discuss adversarial methods that learn to distinguish between generated and reference motions, encouraging natural movement patterns]\n5.4 Experimental Evaluation 5.4.1 Benchmark Tasks [Content placeholder: Define a set of benchmark tasks for evaluating data-driven RL methods in animation]\n5.4.2 Quantitative Results [Content placeholder: Present quantitative comparisons of different approaches, including learning curves, final performance, and motion quality metrics]\n5.4.3 Qualitative Analysis [Content placeholder: Provide qualitative analysis of generated animations, including visual comparisons and expert evaluations]\n5.5 Applications 5.5.1 Style Transfer and Motion Editing [Content placeholder: Demonstrate applications to style transfer between different motion styles and interactive motion editing]\n5.5.2 Multi-skill Learning [Content placeholder: Show how data-driven RL can enable learning of multiple skills from diverse motion datasets]\n5.5.3 Adaptive Character Control [Content placeholder: Present applications to adaptive character control that can handle varying environments and tasks]\n5.6 Chapter Summary This chapter has presented methods for effectively combining data-driven approaches with reinforcement learning for character animation. By leveraging existing motion data within an RL framework, we can create animation systems that benefit from both the quality of recorded motions and the adaptability of learned policies.\nKey contributions include: - Novel algorithms for off-policy learning from motion capture data - Hybrid approaches that balance imitation and task-oriented learning - Demonstration of improved sample efficiency and motion quality - Applications to various animation problems including style transfer and multi-skill learning\nThe methods developed in this chapter enable more practical deployment of RL-based animation systems by reducing the need for extensive online exploration while maintaining the flexibility to adapt to new tasks and environments.\n","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"4caa4bfc750979cddab76cb7e35516cf","permalink":"https://vihanga.github.io/thesis/chapters/data-driven-rl/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/thesis/chapters/data-driven-rl/","section":"thesis","summary":"Chapter 5: Data-Driven Reinforcement Learning with Off-Policy Data This chapter addresses the challenge of leveraging existing animation data within reinforcement learning frameworks, presenting methods for combining data-driven approaches with RL to create more efficient and higher-quality animation systems.\n5.1 Introduction [Content placeholder: Introduce the motivation for combining data-driven methods with RL, discussing the abundance of motion capture data and the limitations of pure RL approaches in animation]\n5.2 Off-Policy Learning from Motion Data This section presents methods for learning from pre-recorded motion data within an RL framework, enabling agents to benefit from expert demonstrations while maintaining the flexibility to explore and improve.","tags":null,"title":"Chapter 5: Data-Driven Reinforcement Learning with Off-Policy Data","type":"thesis"},{"authors":null,"categories":null,"content":" Appendix F: Star Jump Ideal Action Calculation This appendix provides detailed mathematical derivations and calculations for determining ideal actions in the star jump motion synthesis task.\nF.1 Problem Formulation F.1.1 Star Jump Motion Definition A star jump consists of the following phases: 1. Preparation Phase: Initial crouching position 2. Launch Phase: Explosive upward movement 3. Aerial Phase: Body forms star shape in mid-air 4. Landing Phase: Controlled descent and stabilization\nF.1.2 State Space Definition The state vector s ∈ ℝⁿ consists of:\ns = [q, q̇, h, φ, τ]  Where: - q ∈ ℝ²¹: Joint angles (7 joints × 3 DOF) - q̇ ∈ ℝ²¹: Joint velocities - h ∈ ℝ: Height of center of mass - φ ∈ ℝ³: Body orientation (Euler angles) - τ ∈ [0,1]: Phase timing parameter\nF.2 Ideal Action Derivation F.2.1 Kinematic Constraints The ideal star jump configuration at peak height satisfies:\nq*_shoulder = [π/2, 0, π/4] # Arms raised laterally q*_hip = [π/4, 0, π/6] # Legs spread wide q*_spine = [0, 0, -π/12] # Slight back arch  F.2.2 Dynamic Optimization The optimal action sequence a(t) minimizes:\nJ = ∫[αE(t) + βD(q,q*) + γS(q̇) + δB(τ)]dt  Where: - E(t): Energy expenditure term - D(q,q*): Deviation from ideal pose - S(q̇): Smoothness penalty - B(τ): Balance maintenance term\nWeights: α=0.1, β=1.0, γ=0.3, δ=0.5\nF.3 Phase-Specific Calculations F.3.1 Preparation Phase (τ ∈ [0, 0.2]) Joint Targets def preparation_targets(τ): crouch_factor = smooth_interpolate(0, 1, τ/0.2) q_target = { 'hip': [-π/3 * crouch_factor, 0, 0], 'knee': [π/2 * crouch_factor, 0, 0], 'ankle': [-π/6 * crouch_factor, 0, 0], 'shoulder': [π/6 * crouch_factor, 0, 0], 'elbow': [-π/4 * crouch_factor, 0, 0] } return q_target  Force Requirements Vertical ground reaction force:\nF_z = m*g + m*a_prep a_prep = -2.5 m/s² (downward acceleration)  F.3.2 Launch Phase (τ ∈ [0.2, 0.35]) Optimal Launch Velocity Using projectile motion equations:\nv₀ = √(2gh_target) h_target = 0.5m (typical jump height) v₀ ≈ 3.13 m/s  Joint Torques Maximum torques during launch:\nτ_max = { 'hip': 250 Nm, 'knee': 180 Nm, 'ankle': 120 Nm, 'shoulder': 60 Nm }  F.3.3 Aerial Phase (τ ∈ [0.35, 0.65]) Star Formation Trajectory def aerial_trajectory(t_flight): # Normalized time in aerial phase t_norm = (t_flight - 0.35) / 0.3 # Smooth transition to star pose spread_factor = sin(π * t_norm) q_aerial = { 'shoulder_abduction': π/2 * spread_factor, 'shoulder_flexion': π/6 * spread_factor, 'hip_abduction': π/3 * spread_factor, 'hip_flexion': π/6 * spread_factor, 'spine_extension': -π/12 * spread_factor } return q_aerial  Angular Momentum Conservation Total angular momentum L = 0 (no rotation):\nL = Σᵢ(rᵢ × mᵢvᵢ) + Iω = 0  F.3.4 Landing Phase (τ ∈ [0.65, 1.0]) Impact Absorption Strategy Joint stiffness schedule:\ndef landing_stiffness(τ_land): # Progressive stiffening k_base = 500 # N·m/rad k_factor = 1 + 2 * (τ_land - 0.65) / 0.35 k_joints = { 'ankle': k_base * k_factor * 1.2, 'knee': k_base * k_factor * 1.0, 'hip': k_base * k_factor * 0.8 } return k_joints  F.4 Control Law Implementation F.4.1 PD Controller with Feedforward def ideal_action(s, τ): # Extract current state q_current, q̇_current = s.joints, s.velocities # Get phase-appropriate targets q_target = get_phase_targets(τ) q̇_target = get_phase_velocities(τ) # PD control law K_p = get_phase_gains_p(τ) K_d = get_phase_gains_d(τ) # Feedforward term τ_ff = get_feedforward_torques(τ) # Combined control τ_control = K_p @ (q_target - q_current) + \\ K_d @ (q̇_target - q̇_current) + \\ τ_ff return clip_torques(τ_control, τ_max)  F.4.2 Phase Transition Conditions def check_phase_transition(s, τ): if τ \u0026lt; 0.2: # Preparation return s.com_velocity[2] \u0026lt; -0.1 elif τ \u0026lt; 0.35: # Launch return s.ground_contact and s.com_velocity[2] \u0026gt; 2.0 elif τ \u0026lt; 0.65: # Aerial return not s.ground_contact else: # Landing return s.ground_contact and abs(s.com_velocity[2]) \u0026lt; 0.1  F.5 Validation Results F.5.1 Simulated Performance    Metric Ideal Achieved Error     Jump Height 0.50m 0.48m 4%   Flight Time 0.64s 0.62s 3%   Star Spread 1.8m 1.75m 2.8%   Landing Stability \u0026lt;5° 3.2° ✓    F.5.2 Energy Efficiency Total energy expenditure:\nE_total = E_kinetic + E_potential + E_internal = 245J + 196J + 87J = 528J  Efficiency compared to simplified model: 78%\nF.6 Implementation Notes F.6.1 Numerical Stability  Use quaternions for orientation to avoid gimbal lock Apply low-pass filtering to joint velocities Implement safety limits on all joint angles  F.6.2 Real-time Considerations  Pre-compute phase trajectories Use lookup tables for trigonometric functions Parallelize joint calculations when possible  F.6.3 Robustness Enhancements  Add compliance terms for unexpected disturbances Implement recovery strategies for off-nominal conditions Include proprioceptive feedback integration  ","date":1737504000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1737504000,"objectID":"ab40f5ed7a3ee4e00cd981489ccb50db","permalink":"https://vihanga.github.io/thesis/appendices/f/","publishdate":"2025-01-22T00:00:00Z","relpermalink":"/thesis/appendices/f/","section":"thesis","summary":"Appendix F: Star Jump Ideal Action Calculation This appendix provides detailed mathematical derivations and calculations for determining ideal actions in the star jump motion synthesis task.\nF.1 Problem Formulation F.1.1 Star Jump Motion Definition A star jump consists of the following phases: 1. Preparation Phase: Initial crouching position 2. Launch Phase: Explosive upward movement 3. Aerial Phase: Body forms star shape in mid-air 4. Landing Phase: Controlled descent and stabilization","tags":null,"title":"Appendix F: Star Jump Ideal Action Calculation","type":"thesis"},{"authors":null,"categories":null,"content":" Chapter 6: Latent Dynamics Models for Character Animation This chapter presents methods for learning compact latent representations of character dynamics, enabling more efficient learning and control of complex animation behaviors through dimensionality reduction and structured latent spaces.\n6.1 Introduction [Content placeholder: Introduce the concept of latent dynamics models and their benefits for character animation, including improved generalization and computational efficiency]\n6.2 Learning Latent Representations This section describes methods for learning meaningful latent representations of character states and dynamics.\n6.2.1 Variational Autoencoders for Motion [Content placeholder: Present VAE-based approaches for learning latent representations of character poses and motions]\n6.2.2 Dynamics in Latent Space [Content placeholder: Describe methods for learning dynamics models that operate in the learned latent space rather than the full state space]\n6.2.3 Structured Latent Spaces [Content placeholder: Discuss techniques for imposing structure on latent spaces to improve interpretability and control]\n6.3 Control in Latent Space This section presents methods for character control that operate in the learned latent representations.\n6.3.1 Latent Policy Learning [Content placeholder: Describe reinforcement learning algorithms that learn policies in latent space]\n6.3.2 Hierarchical Control Architectures [Content placeholder: Present hierarchical control methods that use latent representations at different levels of abstraction]\n6.3.3 Transfer and Generalization [Content placeholder: Discuss how latent representations enable better transfer learning and generalization across different characters and tasks]\n6.4 Applications to Complex Behaviors 6.4.1 Multi-character Coordination [Content placeholder: Demonstrate applications to coordinating multiple characters using shared latent representations]\n6.4.2 Long-horizon Planning [Content placeholder: Show how latent dynamics models enable efficient long-horizon planning for complex animation sequences]\n6.4.3 Interactive Character Control [Content placeholder: Present applications to real-time interactive control using latent space representations]\n6.5 Experimental Results 6.5.1 Representation Quality [Content placeholder: Evaluate the quality of learned latent representations through reconstruction accuracy and semantic meaningfulness]\n6.5.2 Control Performance [Content placeholder: Compare control performance using latent dynamics models versus full-state approaches]\n6.5.3 Computational Efficiency [Content placeholder: Analyze the computational benefits of operating in latent space for both learning and inference]\n6.6 Chapter Summary This chapter has presented a comprehensive framework for learning and utilizing latent dynamics models in character animation. By operating in learned latent spaces, we can achieve more efficient learning, better generalization, and more intuitive control of complex character behaviors.\nKey contributions of this chapter include: - Novel architectures for learning structured latent representations of character dynamics - Efficient control algorithms that operate in latent space - Demonstration of improved sample efficiency and generalization - Applications to challenging animation problems including multi-character coordination and long-horizon planning\nThe latent dynamics approach developed in this chapter provides a foundation for scaling character animation systems to more complex behaviors and environments while maintaining computational tractability and control quality.\n","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"b010e1faa4d04184493b3d5814db0f5e","permalink":"https://vihanga.github.io/thesis/chapters/latent-dynamics/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/thesis/chapters/latent-dynamics/","section":"thesis","summary":"Chapter 6: Latent Dynamics Models for Character Animation This chapter presents methods for learning compact latent representations of character dynamics, enabling more efficient learning and control of complex animation behaviors through dimensionality reduction and structured latent spaces.\n6.1 Introduction [Content placeholder: Introduce the concept of latent dynamics models and their benefits for character animation, including improved generalization and computational efficiency]\n6.2 Learning Latent Representations This section describes methods for learning meaningful latent representations of character states and dynamics.","tags":null,"title":"Chapter 6: Latent Dynamics Models for Character Animation","type":"thesis"},{"authors":null,"categories":null,"content":" Appendix G: RLAnimate Directory This appendix provides a comprehensive directory of the RLAnimate framework, including its architecture, components, API reference, and usage examples.\nG.1 System Architecture G.1.1 Overview RLAnimate is a modular framework for physics-based character animation using reinforcement learning. The system consists of:\nRLAnimate/ ├── core/ │ ├── agents/ │ ├── environments/ │ ├── physics/ │ └── utils/ ├── models/ │ ├── policies/ │ ├── value_functions/ │ └── networks/ ├── training/ │ ├── algorithms/ │ ├── replay_buffers/ │ └── schedulers/ ├── evaluation/ │ ├── metrics/ │ ├── visualization/ │ └── benchmarks/ └── examples/ ├── tutorials/ ├── experiments/ └── demos/  G.1.2 Core Components Physics Engine Interface class PhysicsInterface: \u0026quot;\u0026quot;\u0026quot;Abstract interface for physics simulation backends\u0026quot;\u0026quot;\u0026quot; def step(self, actions: np.ndarray) -\u0026gt; Tuple[State, float, bool, dict]: \u0026quot;\u0026quot;\u0026quot;Advance simulation by one timestep\u0026quot;\u0026quot;\u0026quot; def reset(self) -\u0026gt; State: \u0026quot;\u0026quot;\u0026quot;Reset simulation to initial state\u0026quot;\u0026quot;\u0026quot; def get_state(self) -\u0026gt; State: \u0026quot;\u0026quot;\u0026quot;Get current simulation state\u0026quot;\u0026quot;\u0026quot; def set_state(self, state: State) -\u0026gt; None: \u0026quot;\u0026quot;\u0026quot;Set simulation state\u0026quot;\u0026quot;\u0026quot;  Character Model class Character: \u0026quot;\u0026quot;\u0026quot;Articulated character representation\u0026quot;\u0026quot;\u0026quot; properties = { 'num_joints': 21, 'num_dof': 63, 'mass': 70.0, # kg 'height': 1.75 # m } joints = [ 'pelvis', 'spine', 'chest', 'neck', 'head', 'left_shoulder', 'left_elbow', 'left_wrist', 'right_shoulder', 'right_elbow', 'right_wrist', 'left_hip', 'left_knee', 'left_ankle', 'right_hip', 'right_knee', 'right_ankle' ]  G.2 API Reference G.2.1 Environment API class RLAnimateEnv(gym.Env): \u0026quot;\u0026quot;\u0026quot;Base environment for character animation tasks\u0026quot;\u0026quot;\u0026quot; def __init__(self, config: Dict[str, Any]): \u0026quot;\u0026quot;\u0026quot; Args: config: Environment configuration dictionary \u0026quot;\u0026quot;\u0026quot; def step(self, action: np.ndarray) -\u0026gt; Tuple[np.ndarray, float, bool, dict]: \u0026quot;\u0026quot;\u0026quot; Execute action and return step information Args: action: Joint torques or target positions Returns: observation: Current state observation reward: Step reward done: Episode termination flag info: Additional information \u0026quot;\u0026quot;\u0026quot; def reset(self) -\u0026gt; np.ndarray: \u0026quot;\u0026quot;\u0026quot;Reset environment to initial state\u0026quot;\u0026quot;\u0026quot; def render(self, mode: str = 'human') -\u0026gt; Optional[np.ndarray]: \u0026quot;\u0026quot;\u0026quot;Render current state\u0026quot;\u0026quot;\u0026quot;  G.2.2 Policy API class Policy(nn.Module): \u0026quot;\u0026quot;\u0026quot;Base policy network class\u0026quot;\u0026quot;\u0026quot; def forward(self, obs: torch.Tensor) -\u0026gt; Distribution: \u0026quot;\u0026quot;\u0026quot; Compute action distribution Args: obs: Observation tensor Returns: Action distribution (Normal or Categorical) \u0026quot;\u0026quot;\u0026quot; def get_action(self, obs: torch.Tensor, deterministic: bool = False) -\u0026gt; torch.Tensor: \u0026quot;\u0026quot;\u0026quot;Sample action from policy\u0026quot;\u0026quot;\u0026quot; def evaluate_actions(self, obs: torch.Tensor, actions: torch.Tensor) -\u0026gt; Tuple[torch.Tensor, torch.Tensor]: \u0026quot;\u0026quot;\u0026quot;Evaluate log probability and entropy of actions\u0026quot;\u0026quot;\u0026quot;  G.2.3 Training API class Trainer: \u0026quot;\u0026quot;\u0026quot;Main training orchestrator\u0026quot;\u0026quot;\u0026quot; def __init__(self, env: RLAnimateEnv, policy: Policy, algorithm: str = 'ppo', config: Dict[str, Any] = None): \u0026quot;\u0026quot;\u0026quot;Initialize trainer with environment and policy\u0026quot;\u0026quot;\u0026quot; def train(self, total_timesteps: int, callback: Optional[Callable] = None) -\u0026gt; Policy: \u0026quot;\u0026quot;\u0026quot; Train policy for specified timesteps Returns: Trained policy \u0026quot;\u0026quot;\u0026quot; def save(self, path: str) -\u0026gt; None: \u0026quot;\u0026quot;\u0026quot;Save trained model\u0026quot;\u0026quot;\u0026quot; def load(self, path: str) -\u0026gt; None: \u0026quot;\u0026quot;\u0026quot;Load trained model\u0026quot;\u0026quot;\u0026quot;  G.3 Configuration System G.3.1 Environment Configuration # config/env/locomotion.yaml env: name: \u0026quot;Locomotion-v1\u0026quot; physics_backend: \u0026quot;mujoco\u0026quot; timestep: 0.01 max_episode_steps: 1000 observation: include_velocities: true include_accelerations: false history_length: 3 reward: weights: task: 1.0 energy: 0.1 smoothness: 0.05 balance: 0.2 termination: fall_threshold: 0.3 max_joint_error: 45 # degrees  G.3.2 Training Configuration # config/train/ppo_default.yaml algorithm: \u0026quot;ppo\u0026quot; policy: network_type: \u0026quot;mlp\u0026quot; hidden_sizes: [256, 256] activation: \u0026quot;tanh\u0026quot; training: learning_rate: 3e-4 n_steps: 2048 batch_size: 64 n_epochs: 10 gamma: 0.99 gae_lambda: 0.95 clip_range: 0.2 schedule: learning_rate: \u0026quot;linear\u0026quot; clip_range: \u0026quot;constant\u0026quot;  G.4 Usage Examples G.4.1 Basic Training Script import rlanimate as rla # Create environment env = rla.make_env('Locomotion-v1') # Create policy policy = rla.Policy( observation_space=env.observation_space, action_space=env.action_space, hidden_sizes=[256, 256] ) # Create trainer trainer = rla.Trainer( env=env, policy=policy, algorithm='ppo', config={'learning_rate': 3e-4} ) # Train policy = trainer.train(total_timesteps=1_000_000) # Save model trainer.save('models/locomotion_ppo.zip') # Evaluate mean_reward = rla.evaluate(env, policy, n_episodes=100) print(f\u0026quot;Mean reward: {mean_reward}\u0026quot;)  G.4.2 Custom Task Definition class JumpingTask(rla.Task): \u0026quot;\u0026quot;\u0026quot;Custom jumping task\u0026quot;\u0026quot;\u0026quot; def __init__(self, target_height: float = 0.5): self.target_height = target_height def compute_reward(self, state: State, action: np.ndarray) -\u0026gt; float: # Height reward height_reward = np.exp(-abs(state.com_height - self.target_height)) # Posture reward posture_reward = self.compute_posture_reward(state) # Energy penalty energy_penalty = -0.001 * np.sum(action**2) return height_reward + 0.5 * posture_reward + energy_penalty def is_success(self, state: State) -\u0026gt; bool: return state.max_com_height \u0026gt;= self.target_height * 0.95  G.4.3 Visualization Script import rlanimate.visualization as viz # Load trained model env = rla.make_env('Locomotion-v1') policy = rla.load_policy('models/locomotion_ppo.zip') # Create visualizer visualizer = viz.Visualizer(env, policy) # Interactive visualization visualizer.run_interactive() # Record video visualizer.record_video( 'output/locomotion_demo.mp4', n_episodes=5, fps=30 ) # Generate trajectory plots trajectories = visualizer.collect_trajectories(n_episodes=10) viz.plot_trajectories(trajectories, save_path='output/trajectories.png')  G.5 Extension Guide G.5.1 Adding New Environments # rlanimate/envs/custom_env.py class CustomEnv(RLAnimateEnv): \u0026quot;\u0026quot;\u0026quot;Template for custom environment\u0026quot;\u0026quot;\u0026quot; def __init__(self, config): super().__init__(config) # Initialize custom components def _compute_reward(self, state, action): # Implement reward function pass def _get_observation(self, state): # Implement observation extraction pass # Register environment from gym.envs.registration import register register( id='CustomEnv-v1', entry_point='rlanimate.envs:CustomEnv', )  G.5.2 Adding New Algorithms # rlanimate/algorithms/custom_algo.py class CustomAlgorithm(BaseAlgorithm): \u0026quot;\u0026quot;\u0026quot;Template for custom RL algorithm\u0026quot;\u0026quot;\u0026quot; def train_step(self, batch): # Implement training step pass def update_policy(self, trajectories): # Implement policy update pass  G.6 Performance Benchmarks G.6.1 Training Speed    Algorithm Steps/Second GPU Memory Training Time (1M steps)     PPO 15,000 2.1 GB 1.1 hours   SAC 12,000 2.8 GB 1.4 hours   TD3 13,500 2.5 GB 1.2 hours    G.6.2 Task Performance    Task PPO SAC TD3 Human Demo     Walk 0.92 0.94 0.91 0.98   Run 0.88 0.91 0.89 0.97   Jump 0.85 0.82 0.86 0.95    G.7 Troubleshooting G.7.1 Common Issues  ImportError: Ensure all dependencies are installed CUDA Error: Check GPU compatibility and drivers Memory Error: Reduce batch size or model size Convergence Issues: Adjust learning rate or exploration  G.7.2 Debug Mode # Enable debug logging import logging logging.basicConfig(level=logging.DEBUG) # Run with debug flags env = rla.make_env('Locomotion-v1', debug=True) trainer = rla.Trainer(env, policy, debug=True)  G.8 License and Citation G.8.1 License RLAnimate is released under the MIT License.\nG.8.2 Citation @software{rlanimate2024, title={RLAnimate: A Framework for Physics-based Character Animation}, author={[Author Names]}, year={2024}, url={https://github.com/[username]/rlanimate} }  ","date":1737504000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1737504000,"objectID":"df16b1e5ec5509fe58e851f187bbc6b9","permalink":"https://vihanga.github.io/thesis/appendices/g/","publishdate":"2025-01-22T00:00:00Z","relpermalink":"/thesis/appendices/g/","section":"thesis","summary":"Appendix G: RLAnimate Directory This appendix provides a comprehensive directory of the RLAnimate framework, including its architecture, components, API reference, and usage examples.\nG.1 System Architecture G.1.1 Overview RLAnimate is a modular framework for physics-based character animation using reinforcement learning. The system consists of:\nRLAnimate/ ├── core/ │ ├── agents/ │ ├── environments/ │ ├── physics/ │ └── utils/ ├── models/ │ ├── policies/ │ ├── value_functions/ │ └── networks/ ├── training/ │ ├── algorithms/ │ ├── replay_buffers/ │ └── schedulers/ ├── evaluation/ │ ├── metrics/ │ ├── visualization/ │ └── benchmarks/ └── examples/ ├── tutorials/ ├── experiments/ └── demos/  G.","tags":null,"title":"Appendix G: RLAnimate Directory","type":"thesis"},{"authors":null,"categories":null,"content":" Chapter 7: Conversational Gesture Generation This chapter addresses the specific challenge of generating natural conversational gestures that accompany speech, presenting methods for creating embodied conversational agents with realistic nonverbal communication behaviors.\n7.1 Introduction [Content placeholder: Introduce the importance of conversational gestures in human communication and the challenges of automatic gesture generation for virtual agents]\n7.2 Understanding Conversational Gestures This section provides background on the nature and function of conversational gestures.\n7.2.1 Types of Conversational Gestures [Content placeholder: Categorize different types of gestures including beat gestures, iconic gestures, metaphoric gestures, and deictic gestures]\n7.2.2 Speech-Gesture Synchrony [Content placeholder: Discuss the temporal relationships between speech and gesture, including principles of synchronization]\n7.2.3 Individual and Cultural Variations [Content placeholder: Address variations in gesture patterns across individuals and cultures]\n7.3 Data-Driven Gesture Generation This section presents methods for learning gesture generation models from human data.\n7.3.1 Multimodal Data Collection [Content placeholder: Describe methods for collecting synchronized speech and motion data for training gesture generation models]\n7.3.2 Feature Extraction and Representation [Content placeholder: Present techniques for extracting relevant features from speech (prosody, semantics) and motion data]\n7.3.3 Sequence-to-Sequence Models [Content placeholder: Describe neural architectures for mapping from speech features to gesture sequences]\n7.4 Reinforcement Learning for Gesture Adaptation This section explores how RL can be used to adapt and improve gesture generation based on interaction feedback.\n7.4.1 Interactive Learning Framework [Content placeholder: Present an RL framework for learning gesture policies through interaction with human users]\n7.4.2 Reward Design for Natural Gestures [Content placeholder: Discuss reward functions that capture naturalness, expressiveness, and communicative effectiveness]\n7.4.3 Online Adaptation [Content placeholder: Describe methods for online adaptation of gesture generation to individual users and contexts]\n7.5 Evaluation Methods 7.5.1 Objective Metrics [Content placeholder: Define quantitative metrics for evaluating gesture generation including synchrony, diversity, and appropriateness]\n7.5.2 User Studies [Content placeholder: Present user study methodologies for evaluating the perceived naturalness and effectiveness of generated gestures]\n7.5.3 Comparative Analysis [Content placeholder: Compare different approaches including rule-based, data-driven, and RL-based methods]\n7.6 Applications and Case Studies 7.6.1 Virtual Assistants [Content placeholder: Demonstrate applications to embodied virtual assistants with natural gesture behaviors]\n7.6.2 Social Robots [Content placeholder: Show applications to social robotics where appropriate gesture generation enhances human-robot interaction]\n7.6.3 Virtual Reality Avatars [Content placeholder: Present applications to VR avatars that need to generate gestures in real-time during conversation]\n7.7 Chapter Summary This chapter has presented a comprehensive approach to conversational gesture generation, combining insights from human communication research with advanced machine learning techniques. The methods developed enable virtual agents to communicate more naturally and effectively through appropriate nonverbal behaviors.\nKey contributions of this chapter include: - A framework for learning gesture generation models from multimodal human data - Novel RL approaches for adapting gesture generation based on interaction feedback - Comprehensive evaluation methods for assessing gesture quality and effectiveness - Demonstration of applications across various domains including virtual assistants and social robotics\nThe work presented in this chapter advances the state of the art in embodied conversational agents, bringing us closer to virtual characters that can engage in natural, multimodal communication with humans.\n","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"b23e04332de28844bae789171e90c6e3","permalink":"https://vihanga.github.io/thesis/chapters/conversational-gestures/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/thesis/chapters/conversational-gestures/","section":"thesis","summary":"Chapter 7: Conversational Gesture Generation This chapter addresses the specific challenge of generating natural conversational gestures that accompany speech, presenting methods for creating embodied conversational agents with realistic nonverbal communication behaviors.\n7.1 Introduction [Content placeholder: Introduce the importance of conversational gestures in human communication and the challenges of automatic gesture generation for virtual agents]\n7.2 Understanding Conversational Gestures This section provides background on the nature and function of conversational gestures.","tags":null,"title":"Chapter 7: Conversational Gesture Generation","type":"thesis"},{"authors":null,"categories":null,"content":" Chapter 8: Conclusion This chapter summarizes the contributions of this thesis, discusses their implications for the field of character animation and reinforcement learning, and outlines directions for future research.\n8.1 Summary of Contributions This thesis has presented a comprehensive exploration of reinforcement learning methods for character animation, addressing fundamental challenges and proposing novel solutions across multiple domains.\n8.1.1 Technical Contributions [Content placeholder: Summarize the main technical contributions including: - Novel RL algorithms for physics-based animation - Model-based and model-free approaches for animation control - Methods for leveraging existing motion data in RL frameworks - Latent dynamics models for efficient learning and control - Gesture generation techniques for conversational agents]\n8.1.2 Theoretical Insights [Content placeholder: Discuss theoretical insights gained about: - The relationship between physics-based simulation and learning - Trade-offs between model-based and model-free approaches - The role of data priors in animation learning - Connections between human motion principles and RL objectives]\n8.1.3 Practical Applications [Content placeholder: Highlight practical applications demonstrated including: - Interactive character control systems - Animation tools for content creation - Embodied conversational agents - Motion synthesis for games and virtual environments]\n8.2 Impact and Significance 8.2.1 Advancing the State of the Art [Content placeholder: Discuss how this work advances the state of the art in character animation, comparing with previous approaches and highlighting improvements]\n8.2.2 Bridging Communities [Content placeholder: Describe how this work bridges the computer graphics and machine learning communities, fostering cross-disciplinary collaboration]\n8.2.3 Enabling New Applications [Content placeholder: Discuss new applications enabled by the methods developed in this thesis, from entertainment to robotics]\n8.3 Limitations and Challenges 8.3.1 Computational Requirements [Content placeholder: Acknowledge computational limitations and discuss trade-offs between quality and efficiency]\n8.3.2 Generalization Boundaries [Content placeholder: Discuss limitations in generalization across different characters, environments, and tasks]\n8.3.3 Evaluation Challenges [Content placeholder: Address the ongoing challenge of objectively evaluating animation quality and naturalness]\n8.4 Future Directions 8.4.1 Scaling to More Complex Behaviors [Content placeholder: Outline research directions for scaling the methods to more complex behaviors and longer time horizons]\n8.4.2 Multi-agent and Social Interactions [Content placeholder: Discuss extensions to multi-agent scenarios and social interaction modeling]\n8.4.3 Integration with Large Language Models [Content placeholder: Explore potential integration with LLMs for high-level behavior specification and control]\n8.4.4 Real-world Deployment [Content placeholder: Discuss pathways for deploying these methods in real-world applications, from games to robotics]\n8.5 Closing Remarks The work presented in this thesis represents a significant step forward in creating intelligent, adaptive character animation systems through reinforcement learning. By addressing fundamental challenges in physics-based animation, data integration, and behavioral modeling, we have demonstrated that RL can be a powerful tool for creating lifelike virtual characters.\nThe methods and insights developed here open new possibilities for interactive entertainment, virtual reality, robotics, and human-computer interaction. As we continue to push the boundaries of what is possible with learned animation systems, the vision of truly intelligent virtual characters that can adapt, learn, and interact naturally with humans comes ever closer to reality.\nThe journey of bringing artificial characters to life through learning is far from complete, but the foundations laid in this thesis provide a solid basis for future innovations. It is my hope that this work will inspire continued research at the intersection of animation and machine learning, ultimately leading to virtual characters that are indistinguishable from their human counterparts in their ability to move, gesture, and express themselves naturally.\n","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"8c2b9df60fa3dd5ba941a75a5fd50439","permalink":"https://vihanga.github.io/thesis/chapters/conclusion/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/thesis/chapters/conclusion/","section":"thesis","summary":"Chapter 8: Conclusion This chapter summarizes the contributions of this thesis, discusses their implications for the field of character animation and reinforcement learning, and outlines directions for future research.\n8.1 Summary of Contributions This thesis has presented a comprehensive exploration of reinforcement learning methods for character animation, addressing fundamental challenges and proposing novel solutions across multiple domains.\n8.1.1 Technical Contributions [Content placeholder: Summarize the main technical contributions including: - Novel RL algorithms for physics-based animation - Model-based and model-free approaches for animation control - Methods for leveraging existing motion data in RL frameworks - Latent dynamics models for efficient learning and control - Gesture generation techniques for conversational agents]","tags":null,"title":"Chapter 8: Conclusion","type":"thesis"},{"authors":null,"categories":null,"content":" Video Demonstrations Motion Synthesis Examples  Basic Locomotion: Demonstrations of walking, running, and transitioning between different gaits Complex Behaviors: Examples of jumping, turning, and obstacle avoidance Multi-character Interactions: Synchronized movements and collision avoidance  Training Progress Visualization  Evolution of motion quality over training iterations Comparison between different reward function configurations Ablation study results in video format  Additional Results Quantitative Metrics Extended evaluation metrics not included in the main chapter: - Frame-by-frame motion quality assessment - Computational performance benchmarks - Memory usage analysis during training and inference\nQualitative Comparisons  Side-by-side comparisons with baseline methods User study results and feedback Analysis of failure cases and edge conditions  Code Examples Training Configuration # Example configuration for motion synthesis training config = { 'learning_rate': 3e-4, 'batch_size': 256, 'hidden_dims': [512, 512, 256], 'action_space': 'continuous', 'reward_weights': { 'motion_quality': 0.7, 'energy_efficiency': 0.2, 'goal_reaching': 0.1 } }  Custom Reward Function Implementation def compute_reward(state, action, next_state, reference_motion): \u0026quot;\u0026quot;\u0026quot; Compute reward based on motion quality and task objectives \u0026quot;\u0026quot;\u0026quot; # Motion matching component pose_similarity = compute_pose_similarity(next_state, reference_motion) # Energy efficiency component energy_penalty = compute_energy_penalty(action) # Task-specific objectives task_reward = compute_task_reward(state, next_state) return pose_similarity - energy_penalty + task_reward  Motion Preprocessing Pipeline # Data preprocessing for motion capture sequences def preprocess_motion_data(raw_mocap_data): # Normalize joint positions normalized_data = normalize_skeleton(raw_mocap_data) # Extract motion features velocities = compute_joint_velocities(normalized_data) accelerations = compute_joint_accelerations(velocities) # Create training sequences sequences = create_overlapping_windows(normalized_data, window_size=64) return sequences, velocities, accelerations  Dataset Information Motion Capture Database  Source: CMU Motion Capture Database, custom recordings Total Sequences: 2,847 motion clips Duration: ~15 hours of motion data Subjects: 45 different performers Motion Categories:  Locomotion (40%) Sports movements (25%) Dance sequences (20%) Daily activities (15%)   Data Preprocessing Details  Sampling rate: 120 Hz downsampled to 30 Hz Coordinate system: Y-up, right-handed Joint representation: 22-joint skeleton model Data augmentation: Mirroring, time warping, noise injection  Training/Validation Split  Training set: 80% (2,278 sequences) Validation set: 10% (285 sequences) Test set: 10% (284 sequences)  Download Links  Preprocessed dataset available upon request Raw motion capture files: [Contact for access] Trained model checkpoints: [Available on project page]  Computational Resources Hardware Requirements  Training: NVIDIA V100 GPU (32GB VRAM) Inference: GTX 1080 Ti or better Memory: 16GB RAM minimum Storage: 50GB for full dataset  Training Time  Full model: ~48 hours Ablation experiments: 12-24 hours each Hyperparameter search: ~200 GPU hours total  Supplementary Figures Additional figures and visualizations that support the main chapter: - Extended ablation study results - Detailed architecture diagrams - Motion trajectory comparisons - Error distribution analyses\nFor questions or additional materials, please contact the author.\n","date":1737504000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1737504000,"objectID":"be2d5c51703a1556b5dcc0c8d7a08c43","permalink":"https://vihanga.github.io/thesis/supplementary/c4/","publishdate":"2025-01-22T00:00:00Z","relpermalink":"/thesis/supplementary/c4/","section":"thesis","summary":"Video Demonstrations Motion Synthesis Examples  Basic Locomotion: Demonstrations of walking, running, and transitioning between different gaits Complex Behaviors: Examples of jumping, turning, and obstacle avoidance Multi-character Interactions: Synchronized movements and collision avoidance  Training Progress Visualization  Evolution of motion quality over training iterations Comparison between different reward function configurations Ablation study results in video format  Additional Results Quantitative Metrics Extended evaluation metrics not included in the main chapter: - Frame-by-frame motion quality assessment - Computational performance benchmarks - Memory usage analysis during training and inference","tags":null,"title":"Chapter 4: Supplementary Material","type":"thesis"},{"authors":null,"categories":null,"content":" Chapter 5 Supplementary Material - Part A Model-free Output Sequences This page contains supplementary material referenced in Chapter 5, Section 5.2 (Model-free RL experiments).\nVideo Demonstrations Star Jump Task - Model-free Agents [PLACEHOLDER: Video grid showing: - DDPG agent performing Male Star Jump (MSJ) - DDPG agent performing Female Star Jump (FSJ) - SAC agent performing Male Star Jump (MSJ) - SAC agent performing Female Star Jump (FSJ) - Side-by-side comparison of best performing agents]\nPointing Task - Model-free Agents [PLACEHOLDER: Video grid showing: - DDPG agent performing pointing tasks - SAC agent performing pointing tasks - Comparison of successful vs failed pointing attempts - Different reward configurations (R1, R2, R3)]\nPerformance Data Training Curves [PLACEHOLDER: Interactive plots showing: - Training progression for DDPG vs SAC on Star Jump - Training progression for DDPG vs SAC on Pointing - Episode reward over training steps - Success rate over training episodes]\nNumerical Results [PLACEHOLDER: Detailed tables with: - Final performance scores for all agent configurations - Standard deviations and confidence intervals - Training time and sample efficiency metrics - Hyperparameter configurations used]\nAgent Artifacts Analysis [PLACEHOLDER: Video analysis showing: - Common failure modes in model-free agents - Jittery movement patterns - Unnatural joint configurations - Comparison with human motion capture data]\nImplementation Details Hyperparameters [PLACEHOLDER: Complete listing of: - Network architectures for DDPG and SAC - Learning rates and optimization settings - Reward function parameters - State and action space definitions]\nCode Snippets [PLACEHOLDER: Key implementation details: - Reward function implementations - State observation configurations - Action space definitions - Training loop structure]\nNavigation  Chapter 5 Part B: Model-based Output Sequences → ← Back to thesis main page Chapter 5 in main thesis  ","date":1737504000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1737504000,"objectID":"684c8c6eb974cdedac7413b0677c73bb","permalink":"https://vihanga.github.io/thesis/supplementary/c5a/","publishdate":"2025-01-22T00:00:00Z","relpermalink":"/thesis/supplementary/c5a/","section":"thesis","summary":"Chapter 5 Supplementary Material - Part A Model-free Output Sequences This page contains supplementary material referenced in Chapter 5, Section 5.2 (Model-free RL experiments).\nVideo Demonstrations Star Jump Task - Model-free Agents [PLACEHOLDER: Video grid showing: - DDPG agent performing Male Star Jump (MSJ) - DDPG agent performing Female Star Jump (FSJ) - SAC agent performing Male Star Jump (MSJ) - SAC agent performing Female Star Jump (FSJ) - Side-by-side comparison of best performing agents]","tags":null,"title":"Chapter 5 Supplementary Material - Part A: Model-free Output Sequences","type":"thesis"},{"authors":null,"categories":null,"content":" Chapter 5 Supplementary Material - Part B Model-based Output Sequences This page contains supplementary material referenced in Chapter 5, Section 5.3 (Model-based RL experiments).\nVideo Demonstrations Star Jump Task - Model-based Agents [PLACEHOLDER: Video grid showing: - Model-based agent performing Male Star Jump (R1, R2, R3 configurations) - Model-based agent performing Female Star Jump (R1, R2, R3 configurations) - Comparison with model-free agents - Improved smoothness and natural motion]\nPointing Task - Model-based Agents [PLACEHOLDER: Video grid showing: - Model-based pointing with different state configurations (S1, S2, S3) - Successful pointing sequences - Multi-target pointing demonstrations - Dynamic target tracking]\nDynamics Model Visualization [PLACEHOLDER: Interactive visualizations showing: - Learned latent dynamics representations - State transition predictions - Planning trajectories - Uncertainty estimates]\nPerformance Improvements Quantitative Results [PLACEHOLDER: Detailed comparison tables: - Model-based vs Model-free performance metrics - Sample efficiency improvements - Final reward scores - Success rates across different configurations]\nArtifact Reduction [PLACEHOLDER: Video comparisons showing: - Reduced jitter in model-based agents - More natural motion trajectories - Better temporal consistency - Smoother joint movements]\nAlternative Training Approaches Section 5.3.2: Action Output Model [PLACEHOLDER: Results from the alternative approach: - Performance without conventional reward function - Direct action supervision results - Comparison with standard model-based approach - Training efficiency metrics]\nImplementation Details Model Architecture [PLACEHOLDER: Detailed architecture diagrams: - Dynamics model structure - Planning algorithm flowchart - Network layer specifications - Hyperparameter configurations]\nTraining Protocol [PLACEHOLDER: Training details including: - Data collection procedures - Model update frequencies - Planning horizon settings - Computational requirements]\nNavigation  ← Chapter 5 Part A: Model-free Output Sequences Chapter 6 Supplementary Material → ← Back to thesis main page Chapter 5 in main thesis  ","date":1737504000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1737504000,"objectID":"4533a70804b1d00a48ea76985216082e","permalink":"https://vihanga.github.io/thesis/supplementary/c5b/","publishdate":"2025-01-22T00:00:00Z","relpermalink":"/thesis/supplementary/c5b/","section":"thesis","summary":"Chapter 5 Supplementary Material - Part B Model-based Output Sequences This page contains supplementary material referenced in Chapter 5, Section 5.3 (Model-based RL experiments).\nVideo Demonstrations Star Jump Task - Model-based Agents [PLACEHOLDER: Video grid showing: - Model-based agent performing Male Star Jump (R1, R2, R3 configurations) - Model-based agent performing Female Star Jump (R1, R2, R3 configurations) - Comparison with model-free agents - Improved smoothness and natural motion]","tags":null,"title":"Chapter 5 Supplementary Material - Part B: Model-based Output Sequences","type":"thesis"},{"authors":null,"categories":null,"content":" Chapter 6 Supplementary Material A1 and A2 Agent Demonstrations This page contains supplementary material referenced in Chapter 6 of the thesis.\nA1 Waving and Pointing Behaviors Waving Demonstrations [PLACEHOLDER: Video grid showing: - A1 agent performing various waving gestures - Different exaggeration levels (0.0 to 1.0) - Left hand vs right hand waving - Comparison with motion capture reference]\nPointing Demonstrations [PLACEHOLDER: Video grid showing: - A1 agent pointing to various 3D targets - Dynamic target tracking - Smooth transitions between targets - Accuracy visualization with target indicators]\nCombined Behaviors [PLACEHOLDER: Videos demonstrating: - Simultaneous waving and pointing - Behavior switching and blending - Multi-objective control scenarios]\nA2 Agent Improvements Enhanced Waving Behaviors [PLACEHOLDER: Comparison videos showing: - A1 vs A2 waving quality - Reduced artifacts in A2 - More natural motion trajectories - Better temporal consistency]\nEnhanced Pointing Behaviors [PLACEHOLDER: Comparison videos showing: - A1 vs A2 pointing accuracy - Smoother target transitions - Improved elbow positioning - Natural arm configurations]\nMotion Capture Data [PLACEHOLDER: Visualization of: - Original motion capture clips used for training - Data preprocessing pipeline - Training data distribution - Behavior labeling process]\nTraining Details Loss Function Evolution [PLACEHOLDER: Plots showing: - L1 (Description reconstruction) loss over training - L2 (KL divergence) loss progression - L3 (Animation reconstruction) loss - Total loss convergence]\nLatent Space Analysis [PLACEHOLDER: Visualizations of: - Learned portrayal dynamics - Behavior dynamics representations - Latent space interpolations - Clustering of different behaviors]\nAblation Studies State Configuration Comparisons [PLACEHOLDER: Results showing: - Performance with different state inputs - Impact of individual state components - Optimal configuration analysis]\nModel Component Ablations [PLACEHOLDER: Videos and metrics for: - Agents without portrayal model - Agents without behavior model - Single dynamics vs dual dynamics - Impact of stochastic components]\nA2 Agent Applied to Finger Animation Preliminary Results [PLACEHOLDER: Videos showing: - A2 agent attempting finger control - Artifacts and instabilities observed - Comparison with body-only animation - Analysis of failure modes]\nThis section provides context for the improvements developed in Chapter 7.\nNavigation  ← Chapter 5 Part B: Model-based Output Sequences Chapter 7 Supplementary Material → ← Back to thesis main page Chapter 6 in main thesis  ","date":1737504000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1737504000,"objectID":"10c1b5d05179a4d74b11d745a8d44096","permalink":"https://vihanga.github.io/thesis/supplementary/c6/","publishdate":"2025-01-22T00:00:00Z","relpermalink":"/thesis/supplementary/c6/","section":"thesis","summary":"Chapter 6 Supplementary Material A1 and A2 Agent Demonstrations This page contains supplementary material referenced in Chapter 6 of the thesis.\nA1 Waving and Pointing Behaviors Waving Demonstrations [PLACEHOLDER: Video grid showing: - A1 agent performing various waving gestures - Different exaggeration levels (0.0 to 1.0) - Left hand vs right hand waving - Comparison with motion capture reference]\nPointing Demonstrations [PLACEHOLDER: Video grid showing: - A1 agent pointing to various 3D targets - Dynamic target tracking - Smooth transitions between targets - Accuracy visualization with target indicators]","tags":null,"title":"Chapter 6 Supplementary Material: RLAnimate Waving and Pointing","type":"thesis"},{"authors":null,"categories":null,"content":" Chapter 7 Supplementary Material Beat Gesture Generation Results This page contains supplementary material referenced in Chapter 7 of the thesis, including video demonstrations of RLAnimate A4 generating beat gestures from speech input.\nA3 Agent Demonstrations Quaternion-based Waving and Pointing with Fingers [PLACEHOLDER: Video grid showing: - A3 agent with quaternion rotations - Waving behaviors with finger movement - Pointing behaviors with finger articulation - Comparison with A2 Euler-based approach]\nAnimation Dynamics Model Results [PLACEHOLDER: Videos demonstrating: - Improved stability with animation dynamics - Reduced finger artifacts - Smoother transitions - Natural finger configurations]\nA4 Agent - Beat Gesture Generation Speech-driven Beat Gestures [PLACEHOLDER: Video grid showing: - A4 agent generating beat gestures from various speech inputs - Different speaker voices (male/female) - Various speech tempos and styles - Emotional speech variations]\nRealism Regularization Components [PLACEHOLDER: Visualizations showing: - Human-likeness regularization effects - Physics-based constraints in action - Smoothness enforcement results - Adversarial training impact]\nComparison with Baselines RLAnimate A4 vs Gesticulator [PLACEHOLDER: Side-by-side videos showing: - Same speech input, different outputs - A4\u0026rsquo;s superior naturalness - Gesticulator\u0026rsquo;s mechanical movements - Synchronization quality comparison]\nRLAnimate A4 vs Motion Capture [PLACEHOLDER: Comparison videos showing: - A4 output alongside ground truth motion capture - Statistical similarity in movement patterns - Variability and naturalness comparison - Perceptual quality assessment]\nTechnical Evaluation Metrics Acceleration Analysis [PLACEHOLDER: Plots and visualizations showing: - Joint acceleration profiles - Rate of change of acceleration - Comparison across methods - Statistical distributions]\nSmoothness Metrics [PLACEHOLDER: Graphs displaying: - Smoothness scores over time - Per-joint smoothness analysis - Comparison with motion capture baseline - Impact of smoothness regularization]\nPerceptual Evaluation Materials Study Setup [PLACEHOLDER: Information about: - Video clips used in the study - Randomization procedure - Question presentation format - Participant interface screenshots]\nDetailed Results [PLACEHOLDER: Extended results including: - Full statistical analysis - Per-clip ratings breakdown - Participant comments - Demographic correlations]\nAblation Studies Realism Regularization Components [PLACEHOLDER: Videos showing A4 performance: - Without human-likeness (H only) - Without physics (P only) - Without smoothness (S only) - Without adversarial (A only) - Various combinations]\nArchitecture Ablations [PLACEHOLDER: Comparisons of: - A4 with different hidden dimensions - Impact of recurrent vs feedforward realism model - Effect of different loss weightings - Training stability analysis]\nSpeech Feature Analysis HuBERT Representations [PLACEHOLDER: Visualizations of: - Raw HuBERT features - Temporal patterns in speech - Correlation with gesture timing - Feature importance analysis]\nGeneralization Tests Novel Speech Inputs [PLACEHOLDER: Videos showing A4 performance on: - Different languages - Non-speech vocalizations - Singing and rhythmic speech - Extreme speech rates]\nStyle Transfer [PLACEHOLDER: Demonstrations of: - Gesture style variations - Character-specific adaptations - Cultural gesture differences - Personality-driven modifications]\nImplementation Resources Code Architecture [PLACEHOLDER: Diagrams and explanations of: - Complete A4 architecture - Training pipeline - Inference optimization - Real-time implementation details]\nTraining Data [PLACEHOLDER: Information about: - Motion capture dataset used - Speech-gesture alignment process - Data augmentation techniques - Training/validation splits]\nNavigation  ← Chapter 6 Supplementary Material ← Back to thesis main page Chapter 7 in main thesis  ","date":1737504000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1737504000,"objectID":"d370faf13eef94886084b843b011fa53","permalink":"https://vihanga.github.io/thesis/supplementary/c7/","publishdate":"2025-01-22T00:00:00Z","relpermalink":"/thesis/supplementary/c7/","section":"thesis","summary":"Chapter 7 Supplementary Material Beat Gesture Generation Results This page contains supplementary material referenced in Chapter 7 of the thesis, including video demonstrations of RLAnimate A4 generating beat gestures from speech input.\nA3 Agent Demonstrations Quaternion-based Waving and Pointing with Fingers [PLACEHOLDER: Video grid showing: - A3 agent with quaternion rotations - Waving behaviors with finger movement - Pointing behaviors with finger articulation - Comparison with A2 Euler-based approach]","tags":null,"title":"Chapter 7 Supplementary Material: Beat Gestures","type":"thesis"},{"authors":null,"categories":null,"content":" Examining the effects of a virtual character on learning and engagement in serious games Authors: Vihanga Gamage, Cathy Ennis\nConference: MIG \u0026lsquo;18: Motion, Interaction and Games, November 8–10, 2018, Limassol, Cyprus\nDOI: 10.1145\u0026frasl;3274247.3274499\nAbstract Virtual characters have been employed for many purposes including interacting with players of serious games, with a purpose to increase engagement. These characters are often embodied conversational agents playing diverse roles, such as demonstrators, guides, teachers or interviewers. Recently, much research has been conducted into properties that affect the realism and plausibility of virtual characters, but it is less clear whether the inclusion of interactive agents in serious applications can enhance a user\u0026rsquo;s engagement with the application, or indeed increase efficacy. In a first step towards answering these questions, we conducted a study where a Virtual Learning Environment was used to examine the effect of employing a virtual character to deliver a lesson. In order to investigate whether increased familiarity between the player and the character would help achieve learning outcomes, we allowed participants to customize the physical appearance of the character. We used direct and indirect measures to assess engagement and learning; we measured knowledge retention to ascertain learning via a test at the end of the lesson, and also measured participants\u0026rsquo; perceived engagement with the lesson. Our findings show that a virtual character can be an effective learning aid, causing heightened engagement and retention of knowledge. However, allowing participants to customize character appearance resulted in inhibited engagement, which was contrary to what we expected.\nKey Findings  A user is more engaged with a lesson when delivered through a virtual character compared to a non-character control version A user retains knowledge better when a lesson is delivered through a virtual character compared to a non-character control version There is a propensity for the majority of user attention to be directed at the virtual character, resulting in little attention being paid to the rest of the game environment Surprisingly, character appearance personalization has an effect opposite to what we expected, resulting in participants being significantly less engaged with a personalized character compared to one with a default appearance  Supplementary Material Full Questionnaire The complete questionnaire used in the study included items from: - Game Engagement Questionnaire (Brockmyer et al., 2009) - Motivation Inventory (McAuley et al., 1989) - Custom questions designed for this study\n[PLACEHOLDER: Insert full questionnaire document here]\nLesson Content - SLAM Tutorial The lesson covered Simultaneous Localization and Mapping (SLAM) with the following structure:\nMain Lessons: 1. Introduction to SLAM 2. SLAM Approaches 3. How SLAM works? 4. EKF SLAM\nHidden Lessons (triggered by interaction): 1. Corner Detectors 2. Proactive vs Lazy SLAM 3. Maximum Likelihood Estimators\n[PLACEHOLDER: Insert complete lesson scripts and visual materials]\nQuiz Questions The knowledge retention test included: - 12 questions on main lesson content - 3 questions on hidden lesson content\n[PLACEHOLDER: Insert all quiz questions with correct answers]\nCharacter Customization Options Screenshots and descriptions of all customization options for: - Male character variations - Female character variations - Hair styles and colors - Skin tones - Clothing options\n[PLACEHOLDER: Insert customization interface screenshots and option descriptions]\nStatistical Analysis Detailed statistical results including: - ANOVA tables for test scores - Kruskal-Wallis H test results for questionnaire data - Post-hoc test results (Tukey-Kramer and Mann-Whitney U) - Effect sizes and confidence intervals\n[PLACEHOLDER: Insert complete statistical analysis tables]\nFree-form User Feedback Selected comments from participants:\n[PLACEHOLDER: Insert anonymized participant comments and feedback]\nUnity Implementation Details Technical specifications: - Unity version and configuration - Character assets from RenderPeople - Performance requirements - Deployment as web application\n[PLACEHOLDER: Insert technical implementation details]\nLinks  Paper PDF (ACM Digital Library) Preprint PDF Conference presentation thoughts and feedback MIG\u0026rsquo;18 conference blog post Back to thesis Appendix A in thesis  ","date":1737504000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1737504000,"objectID":"91a78759d0b2980c5906b19f9be44565","permalink":"https://vihanga.github.io/thesis/publications/MIG18/","publishdate":"2025-01-22T00:00:00Z","relpermalink":"/thesis/publications/MIG18/","section":"thesis","summary":"Examining the effects of a virtual character on learning and engagement in serious games Authors: Vihanga Gamage, Cathy Ennis\nConference: MIG \u0026lsquo;18: Motion, Interaction and Games, November 8–10, 2018, Limassol, Cyprus\nDOI: 10.1145\u0026frasl;3274247.3274499\nAbstract Virtual characters have been employed for many purposes including interacting with players of serious games, with a purpose to increase engagement. These characters are often embodied conversational agents playing diverse roles, such as demonstrators, guides, teachers or interviewers.","tags":null,"title":"MIG'18 Supplementary Material","type":"thesis"},{"authors":["Vihanga Gamage"],"categories":[],"content":"Originally published on RTÉ Brainstorm\nThe solar system is full of debris and rocks floating around. A meteor shower or a shooting star is debris burning up upon entry to the atmosphere. On occasion, pieces of rock make it all the way through. A much larger rock that could make its way through is an asteroid. About 66 million years ago, the impact from an asteroid brought about the Cretaceous extinction. A repetition of this event is theoretically possible - and the same could be said about all-powerful Artificial Intelligence Overlords marginalising the human race.\nSince its inception in the mid 20th century, the field of artificial intelligence has had an interesting ride. Much like the financial markets or Hollywood, there’s been breakthroughs and failures and booms and busts. Driven by advances in deep learning, a great deal of success has been achieved this decade, ushering in the newest AI golden age, along with a frenzy of interest and investment.\nThe current popular views of AI are being shaped primarily by several vocal figures in technology. Microsoft founder Bill Gates has said that AI carries the potential to change society deeply and is both exciting and dangerous. IBM CEO Ginni Rometty has stated that she believes advances in AI will create different jobs rather than no jobs. Facebook founder Mark Zuckerberg has expressed the view that AI will deliver various improvements to human quality of life, while Tesla founder Elon Musk has called AI \u0026ldquo;a fundamental risk to the existence of human civilisation\u0026rdquo;.\nThe future dangers AI could pose to humanity is a frequently occurring subject in conversation, be it to break the ice on a first date, to fill the time before a meeting starts or while waiting for a pint at the pub on a Friday evening. And arguably, Musk\u0026rsquo;s statement carries the most bite. It certainly is a very catchy line that makes for a more topical conversation starter than asteroids or the Cretaceous extinction.\nAs a result, these hypothetical yet sensationalist doomsday opinions are at a point where they dominate the conversation about AI, so much so that the bigger picture on the subject may at the point of being overshadowed and given very little consideration. And that could pose an even greater threat, especially in an age where hype and rhetoric without regard for fact has had a great effect on the fate of the world.\nAdvancements in AI could lead to a superintelligence that could pose a fundamental risk to human existence - and so could climate change or a 5 km-wide space rock colliding with the Earth. But there are many points of difference between these three risks. AI is a long way away advancing to the point of superintelligence and it’s not a certainty that superintelligence would lead to the end of humanity. Climate change is a very real and present danger, with the time to take action to prevent irreversible consequences continuing to decrease. And then there are the chances of an asteroid striking the earth.\nOf course, there is no good that can come from an asteroid strike or global warming, but AI can literally be one of the best things to happen to humans. This is not said enough - and it can’t be said enough. AI can and is being used to as a tool in many incredible ways. For example, it is being used to help improve prevention of cancer, provide effective treatment, and could one day even be part of the solution.\nAI is also being leveraged as a promising way to combat climate change. AI-powered simulations and analysis helps make for sustainable urban planning and ocean preservation. It is possible to make more accurate predictions regarding weather events with the help of AI leading to reduced damage to life and property. It can help power grids to be more energy efficient by predicting peak over of usage allowing for better preparation and enhances the benefits of clean energy methods by deploying in ways such as to incorporate weather and other relevant data to make wind turbines more efficient.\nAnd yet arguments on how AI could hypothetically end humanity as we know it at some point in the future, seem to drown out how AI is already helping humanity’s cause. Such contributions pale in comparison to the very real possibility that AI could be the key to finding the answers for some of humanity’s biggest problems.\nAt a 2018 hearing at the US House of Representatives Committee on Science, Space and Technology, Stanford Professor Fei-Fei Li said that AI is bound to alter the human experience, and not necessarily for the better. After acknowledging this, Li pointed out that ensuring AI will transform the world for the better is very much in our hands. \u0026ldquo;There\u0026rsquo;s nothing artificial about AI\u0026rdquo;, she said.\n \u0026ldquo;It’s inspired by people; it’s created by people, and—most importantly—it impacts people. It is a powerful tool we are only just beginning to understand, and that is a profound responsibility.\u0026rdquo;\n Concern that AI could pose a threat is very much reasonable. But this possibility is just that - a possibility. Let’s not forget AI is a fundamental opportunity to further the cause of the human civilisation, and that it is in our hands to ensure that it impacts us positively, and be optimistic about what it can mean for us. The world can only be the better place for it.\n","date":1566061379,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1566061379,"objectID":"a75ccec893290ac194456b20ef62ffa9","permalink":"https://vihanga.github.io/post/blog/AI-Oppurtunity-not-danger/","publishdate":"2019-08-17T17:02:59Z","relpermalink":"/post/blog/AI-Oppurtunity-not-danger/","section":"post","summary":"Originally published on RTÉ Brainstorm\nThe solar system is full of debris and rocks floating around. A meteor shower or a shooting star is debris burning up upon entry to the atmosphere. On occasion, pieces of rock make it all the way through. A much larger rock that could make its way through is an asteroid. About 66 million years ago, the impact from an asteroid brought about the Cretaceous extinction.","tags":["blog"],"title":"Why AI is an opportunity rather than a danger","type":"post"},{"authors":["Vihanga Gamage"],"categories":[],"content":"\u0026lt;\u0026lt; restricted access to posting \u0026gt;\u0026gt;\n","date":1542474179,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1542474179,"objectID":"8ea6b228c1d9f27f638832de44099fc4","permalink":"https://vihanga.github.io/post/research/MIG18PaperStudy/","publishdate":"2018-11-17T17:02:59Z","relpermalink":"/post/research/MIG18PaperStudy/","section":"post","summary":"\u0026lt;\u0026lt; restricted access to posting \u0026gt;\u0026gt;","tags":["research"],"title":"A pilot study into virtual character application as pedagogical agents","type":"post"},{"authors":["Vihanga Gamage"],"categories":[],"content":"\u0026lt;\u0026lt; restricted access to posting \u0026gt;\u0026gt;\n","date":1542474179,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1542474179,"objectID":"e46c42befa06751c790f5912e50fc00e","permalink":"https://vihanga.github.io/post/blog/MIG18blog/","publishdate":"2018-11-17T17:02:59Z","relpermalink":"/post/blog/MIG18blog/","section":"post","summary":"\u0026lt;\u0026lt; restricted access to posting \u0026gt;\u0026gt;","tags":["blog"],"title":"Motion, Interaction and Games 2018","type":"post"},{"authors":["Vihanga Gamage"],"categories":[],"content":"   After the horrible start to the season I had at the City of Dublin in September, I was desperate to turn my form and results around. I had signed up to play in as much fixtures as I can for the club\u0026rsquo;s second team who plays in the third division of the Leinster Chess Union leagues, and played in all three opening fixtures.\nAnd will you look at that, the results are quite alright.\n  Funnily enough, I seem to have played three consecutive games with the black pieces due to playing on three different boards, and the team did alright especially given our standing on joint second after 2 wins and a narrow loss to one of the highest rated teams in the division.\n  Match 1 : Opening game vs Elm Mount - A very lucky escape. Very lucky indeed. This was a bit of a weird one. I was playing black, on board 5, fairly confident, decent position coming out of the opening, and I was in a mood that day. So, when it looked like there was potential to sacrifice a piece on h3, I was fairly focussed on this.\nAt this, position, I was so preoccupied whether to take h3 with the bishop on c8 or knight on f4, I didn\u0026rsquo;t notice that my opponents last move, Nf1, opens an attack from the bishop on c1 to my knight on f4.\n After a lot of deliberation in my head, I took h3 with the Bishop, and I didn\u0026rsquo;t notice my blunder till my opponent took the piece on f4.\nAfter that blunder, I thought my best chance of recovering was to capitalize on whatever little attacking potential I had in this position, and I forged ahead with my plan to put his castle under pressure. My rationale here was that it was easier for me to get both my rooks involved in an assault on his castle than it was for him to make his rook on a1 active, and maybe I might be able to overwhelm his pieces.\nAnd as luck would have it, that\u0026rsquo;s just what happened.\n Presumably, due to the perceived threat from f4 that would make open the file for my rook on f8 and further weaken his king, he made the move Qd2.\n..Nxg2, he responded with Nf1, again, as what I presume was a hail mary move in the hopes I might overlook a threat on that diagonal, again. But I didn\u0026rsquo;t, he resigned after my next move Nf4+ as the only continuation lead to either losing his queen or checkmate.\n I was glad I got away with the win after that blunder, the team got a 4-2 win in the opening fixture. We went to the pub afterwards, played ping pong while having pints, and watched Man United pull off that awesome 3-2 comeback win against Newcastle.\nOverall, it was an alright day, but it could have just gone very, very, wrong, for myself, just as well as for United and Jose Mourinho.\nMatch 2 : Converting an advantage with surprising efficiency Next up was a game against Lucan, all the other 5 games had been played the week before; my board had been deferred as an old friend was visiting me the day of the match. The match was balanced on 2.5-2.5 with 5 games played, and I was playing their captain on board 4.\nThe opening was nothing special, left a nice evenly balanced position coming into the midgame. \nMy move here was to castle, rather than the engine suggested move of capturing the e4 pawn, but the open file was what lead to my advantage, given my opponent sought to make a rash advance with his rook to apply pressure on my castle.\n But in this position, ...Nc5. Qc3, Ne6. R7xe6,Bxe6. And I was up a rook to a bishop. It had been a recent pattern of mine to play out some fidgety plans leading to drawn positions even when I had the advantage, or even devolve into a loss, but I played through to a winning endgame in what was, on recent form, uncharacteristically surprising efficiency.  It would\u0026rsquo;ve been nicer if I had spotted the forced mating sequence here:\n\u0026hellip;e4+. Kf4, Rgf2+. Kg4, Rf3. Rf7+, Kxf7. Kg5, Rg2+. Kh6, Rh3. c7, Rxh4#\nBut I had less than a minute on the clock, so I was playing on increments, so it\u0026rsquo;s entirely justifiable that I did things the old fashioned way.\n My opponent ran out of time here, when I was winning quite comfortably; and the team scored a 3.5-2.5 win.\nMatch 3 : A fast 15 move draw It was a Monday evening, I was again playing a couple of days after the match, one of two differred boards. I had just flew back to Dublin the evening before from the MIG\u0026rsquo;18 conference in Cyprus.\nI had popped into a pub near the playing venue to get a pint, and opened my Macbook to finish up some emails when I see a window with my Twitter home feed showing that Stan Lee had died.\nI really wasn\u0026rsquo;t in the mood to play a game after seeing this, 10 minutes before the start time. I went in, my oppoennet played the Polish opening, that was a first. And I was more than happy to play a line that lead to a draw.\n Looking at the position, it can be said that I was let off, as my isolated pawn and overall position isn\u0026rsquo;t likely to lead to a whole lot of joy. My move here would\u0026rsquo;ve been Qd6 or Qc7, and it wouldn\u0026rsquo;t have been the hardest to defend here. Qd6 might even allow for some attacking momentum if I can get my knight involved. My opponent seemed to agree, and it seemed he had studied the opening and likely positions pretty thoroughly; also the engine concurred with this analysis.\nBut, as my opponent said, we could\u0026rsquo;ve played for a couple of hours and ended up drawn anyway, or agreed a draw early on that basis and gone home early.\nWe lost the game 2.5-3.5, but given that league standings are based on game points rather than match points, it\u0026rsquo;s not the end of the world. We are tied second on points in the league standings, we seem quite competitive for a newly promoted team with an average rating that is on the latter end in the league.\nIf we can keep this up, come April, the team could be in an interesting position.\n  And, oh hello, a 55 point boost to my FIDE rating. This was a nice surprise, thanks to the K-Factor of 40, as it\u0026rsquo;s less than 30 of my games were FIDE rated.\n  I had initially planned to play in the Kilkenny Weekender, where I played my first competitive chess since moving to Ireland, for the third consecutive year. But with the fatigue from the travel and the conference and with some heavy weeks ahead of me at work, it seemed the right idea to sit this year out.\nI\u0026rsquo;m likely to play at 2 more games in 2018, with our last Ennis shield fixture again Skerries, and a substitute appearance for the first time this season in the Armstrong Cup for the first team.\nMy form seems to be improving after the blunders in the City of Dublin weekender and the less than ideal play early in the first league match. This is a good sign for my potential performance at the 9 round closed all-play-all in early January, that I entered for.\n","date":1542474179,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1542474179,"objectID":"8a6c13decec0c37ebef0cef29eb5a232","permalink":"https://vihanga.github.io/post/chess/2018LeagueEnnisR123/","publishdate":"2018-11-17T17:02:59Z","relpermalink":"/post/chess/2018LeagueEnnisR123/","section":"post","summary":"After the horrible start to the season I had at the City of Dublin in September, I was desperate to turn my form and results around. I had signed up to play in as much fixtures as I can for the club\u0026rsquo;s second team who plays in the third division of the Leinster Chess Union leagues, and played in all three opening fixtures.\nAnd will you look at that, the results are quite alright.","tags":["chess"],"title":"The form awakens* (*seems to stir awake from a 4 month slumber) - 2.5/3 : Rounds 1, 2 \u0026 3 of the Ennis Shield (Leinster League Division 3)","type":"post"},{"authors":["Vihanga Gamage, Cathy Ennis"],"categories":null,"content":"I presented my first conference paper at the 11th Motion, Interactions and Games conference held in November 2018. In this paper, we examined the broad effects a virtual character can have on user experience and performance when incorporated as a pedagogical agent in a serious game. We also explored character appearance personalization as a potential way of heightening engagement with the character.\nOur findings can be summaries as follows: - A user is more engaged with a lesson when delivered through a virtual character compared to a non-character control version. - A user retains knowledge better when a lesson is delivered through a virtual character compared to a non-character control version. - There is a propensity for the majority of user attention to be directed at the virtual character, resulting in little attention being paid to the rest of the game environment. - Surprising, character appearance personalization has an effect opposite to what we expected, resulting in participants being significantly less engaged with a personalized character compare to one with a default appearance.\nSupplementary material in the form of the full questionnaire used in the study and free-form user feedback and some interesting points raised by attendees at the conference following my thoughts can be found here, and a blog post about my experience at the conference and some photographs can be found here.\n","date":1542473248,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1542473248,"objectID":"c6965129ead2fdcac70fc04d9f02f39e","permalink":"https://vihanga.github.io/publication/MIG18/","publishdate":"2018-11-17T16:47:28Z","relpermalink":"/publication/MIG18/","section":"publication","summary":"Virtual characters have been employed for many purposes including interacting with players of serious games, with a purpose to increase engagement. These characters are often embodied conversational agents playing diverse roles, such as demonstrators, guides, teachers or interviewers. Recently, much research has been conducted into properties that affect the realism and plausibility of virtual characters, but it is less clear whether the inclusion of interactive agents in serious applications can enhance a user's engagement with the application, or indeed increase efficacy. In a first step towards answering these questions, we conducted a study where a Virtual Learning Environment was used to examine the effect of employing a virtual character to deliver a lesson. In order to investigate whether increased familiarity between the player and the character would help achieve learning outcomes, we allowed participants to customize the physical appearance of the character. We used direct and indirect measures to assess engagement and learning; we measured knowledge retention to ascertain learning via a test at the end of the lesson, and also measured participants' perceived engagement with the lesson. Our findings show that a virtual character can be an effective learning aid, causing heightened engagement and retention of knowledge. However, allowing participants to customize character appearance resulted in inhibited engagement, which was contrary to what we expected.","tags":[],"title":"Examining the effects of a virtual character on learning and engagement in serious games","type":"publication"},{"authors":["Vihanga Gamage"],"categories":[],"content":"\u0026lt;\u0026lt; restricted access to posting \u0026gt;\u0026gt;\u0026hellip;.\n","date":1542128579,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1542128579,"objectID":"70e8f7b753362d5f38eacdfd83b8feae","permalink":"https://vihanga.github.io/post/blog/StanLee/","publishdate":"2018-11-13T17:02:59Z","relpermalink":"/post/blog/StanLee/","section":"post","summary":"\u0026lt;\u0026lt; restricted access to posting \u0026gt;\u0026gt;\u0026hellip;.","tags":["blog"],"title":"Excelsior!","type":"post"},{"authors":["Vihanga Gamage"],"categories":[],"content":" \u0026ldquo;My soul is painted like the wings of butterflies, fairy tales of yesterday will grow but never die. I can fly, my friends\u0026rdquo;\nThe Show Must Go On, Queen (1991)\u0026rdquo;\n \u0026lt;\u0026lt; restricted access to posting \u0026gt;\u0026gt;\n","date":1540832579,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1540832579,"objectID":"26178ba6ce0c35ef14bd61c03b23aa50","permalink":"https://vihanga.github.io/post/blog/BohemianRhapsody/","publishdate":"2018-10-29T17:02:59Z","relpermalink":"/post/blog/BohemianRhapsody/","section":"post","summary":"\u0026ldquo;My soul is painted like the wings of butterflies, fairy tales of yesterday will grow but never die. I can fly, my friends\u0026rdquo;\nThe Show Must Go On, Queen (1991)\u0026rdquo;\n \u0026lt;\u0026lt; restricted access to posting \u0026gt;\u0026gt;","tags":["blog"],"title":"\"....fairy tales of yesterday will grow but never die.\" , My thoughts on Bohemian Rhapsody, the Queen biopic","type":"post"},{"authors":["Vihanga Gamage"],"categories":[],"content":"  The 2018 / 2019 ICU season kicked off at the City of Dublin weekender held in September. This season was one I approached with as much intent to perform well, as I had done in a while. This is because I will be taking a step back from playing relatively seriously, at the end of the season, for about a year to focus on the last year of my PhD.\nDespite my showing at the Irish Championship Supporting events in August not being as convincing as I had hoped after a few good performances earlier in the year, I was at least happy with the fact that I had consolidated the 180 point rating jump I achieved at the Ennis Open in May.\nI was seeded in the top 10 of the draw in my section. The first round I was playing back against a solid player. After a fairly even middle game phase, I managed to capitalise on my opponent\u0026rsquo;s misstep to go into the end game with a winning position.\nand the stage was set\u0026hellip;\u0026hellip;for a trademark pawn ending screw up from me. This has been happening a lot recently.\nIn my defence, I had studied up on pawn ending theory that I was continually getting wrong, and I had a couple of minutes plus increments on the clock, but this was pretty much inexcusable.\nIn this position, it was my move, and instead of c5, I played Bxc3 leading the game to a dead draw.\n Bxc3, Bxc3 Kxc3, Ke3 Kb3, Kxe4 Kxa3, f4\nand hold on to your hats people because here comes the height of lunacy. Instead of calculating properly and making c4, Kxb4 ??????\nand that was it. First game of the season, arguably the worst game I played since February.\n The next morning, I picked up right where I left off, a toothless game and a dumb theoretical blunder towards the end.\nTwo games, both against players rated about 200 points beyond me, my rating had already dropped by 50.\nIt did not seem like I could get out of the rut I was in, and a tailspin seemed imminent, so to avoid a dropping an even hundred more rating points, I withdrew at this point.\nMy next competitive game is likely going to be the first match round of the new league season. I will be playing the full league campaign for the Dublin University Ennis Shield team; here\u0026rsquo;s hoping I plug the holes in my endgame and mid-game play isn\u0026rsquo;t so flat.\n","date":1539795779,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1539795779,"objectID":"1b2d65a0738b96dd92c771c08112086e","permalink":"https://vihanga.github.io/post/chess/2018CoD/","publishdate":"2018-10-17T17:02:59Z","relpermalink":"/post/chess/2018CoD/","section":"post","summary":"The 2018 / 2019 ICU season kicked off at the City of Dublin weekender held in September. This season was one I approached with as much intent to perform well, as I had done in a while. This is because I will be taking a step back from playing relatively seriously, at the end of the season, for about a year to focus on the last year of my PhD.","tags":["chess"],"title":"A calamitous start to the season - 0/2 at the City of Dublin 2018","type":"post"},{"authors":null,"categories":null,"content":" Vihanga Gamage\u0026rsquo;s Public Calendar  ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"a631103cf606bbead447236a29c6889e","permalink":"https://vihanga.github.io/calendar/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/calendar/","section":"","summary":" Vihanga Gamage\u0026rsquo;s Public Calendar  ","tags":null,"title":"","type":"page"}]