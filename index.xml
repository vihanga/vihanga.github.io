<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Vihanga Gamage&#39;s Corner of the World Wide Web on Vihanga Gamage&#39;s Corner of the World Wide Web</title>
    <link>https://vihanga.github.io/</link>
    <description>Recent content in Vihanga Gamage&#39;s Corner of the World Wide Web on Vihanga Gamage&#39;s Corner of the World Wide Web</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>&amp;copy; 2018</copyright>
    <lastBuildDate>Wed, 22 Jan 2025 00:00:00 +0000</lastBuildDate>
    <atom:link href="/" rel="self" type="application/rss+xml" />
    
    <item>
      <title>Appendix A: Exploring Engagement and Efficiency in Serious Games</title>
      <link>https://vihanga.github.io/thesis/appendices/a/</link>
      <pubDate>Wed, 22 Jan 2025 00:00:00 +0000</pubDate>
      
      <guid>https://vihanga.github.io/thesis/appendices/a/</guid>
      <description>

&lt;h1 id=&#34;appendix-a-exploring-engagement-and-efficiency-in-serious-games&#34;&gt;Appendix A: Exploring Engagement and Efficiency in Serious Games&lt;/h1&gt;

&lt;p&gt;This appendix provides supplementary material related to the exploration of engagement and efficiency in serious games, as discussed in the main thesis.&lt;/p&gt;

&lt;h2 id=&#34;a-1-game-design-principles&#34;&gt;A.1 Game Design Principles&lt;/h2&gt;

&lt;h3 id=&#34;a-1-1-engagement-metrics&#34;&gt;A.1.1 Engagement Metrics&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;Player retention rates&lt;/li&gt;
&lt;li&gt;Time-on-task measurements&lt;/li&gt;
&lt;li&gt;User satisfaction surveys&lt;/li&gt;
&lt;li&gt;Learning outcome assessments&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;a-1-2-efficiency-considerations&#34;&gt;A.1.2 Efficiency Considerations&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;Computational resource usage&lt;/li&gt;
&lt;li&gt;Response time optimization&lt;/li&gt;
&lt;li&gt;Scalability factors&lt;/li&gt;
&lt;li&gt;Performance benchmarks&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;a-2-case-studies&#34;&gt;A.2 Case Studies&lt;/h2&gt;

&lt;h3 id=&#34;a-2-1-educational-games&#34;&gt;A.2.1 Educational Games&lt;/h3&gt;

&lt;p&gt;Analysis of engagement patterns in educational serious games, including:
- Mathematics learning applications
- Language acquisition platforms
- Science simulation environments&lt;/p&gt;

&lt;h3 id=&#34;a-2-2-training-simulations&#34;&gt;A.2.2 Training Simulations&lt;/h3&gt;

&lt;p&gt;Efficiency metrics from professional training applications:
- Medical procedure simulators
- Military training systems
- Industrial safety programs&lt;/p&gt;

&lt;h2 id=&#34;a-3-experimental-data&#34;&gt;A.3 Experimental Data&lt;/h2&gt;

&lt;h3 id=&#34;a-3-1-user-study-results&#34;&gt;A.3.1 User Study Results&lt;/h3&gt;

&lt;p&gt;Detailed results from user studies examining:
- Engagement levels across different game mechanics
- Efficiency trade-offs in game design decisions
- Comparative analysis of different approaches&lt;/p&gt;

&lt;h3 id=&#34;a-3-2-performance-metrics&#34;&gt;A.3.2 Performance Metrics&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;Frame rate analysis&lt;/li&gt;
&lt;li&gt;Memory usage patterns&lt;/li&gt;
&lt;li&gt;Network latency measurements&lt;/li&gt;
&lt;li&gt;User interface responsiveness&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;a-4-design-guidelines&#34;&gt;A.4 Design Guidelines&lt;/h2&gt;

&lt;h3 id=&#34;a-4-1-balancing-engagement-and-efficiency&#34;&gt;A.4.1 Balancing Engagement and Efficiency&lt;/h3&gt;

&lt;p&gt;Best practices for optimizing both user engagement and system efficiency:
- Progressive complexity scaling
- Adaptive difficulty systems
- Resource-aware design patterns&lt;/p&gt;

&lt;h3 id=&#34;a-4-2-implementation-recommendations&#34;&gt;A.4.2 Implementation Recommendations&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;Architecture considerations&lt;/li&gt;
&lt;li&gt;Technology stack selections&lt;/li&gt;
&lt;li&gt;Optimization strategies&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Chapter 1: Introduction</title>
      <link>https://vihanga.github.io/thesis/chapters/introduction/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://vihanga.github.io/thesis/chapters/introduction/</guid>
      <description>

&lt;h2 id=&#34;1-1-engaging-virtual-characters&#34;&gt;1.1 Engaging Virtual Characters&lt;/h2&gt;

&lt;p&gt;[Placeholder for section on engaging virtual characters]&lt;/p&gt;

&lt;h3 id=&#34;1-1-1-evolution-of-virtual-characters-in-popular-culture&#34;&gt;1.1.1 Evolution of Virtual Characters in Popular Culture&lt;/h3&gt;

&lt;p&gt;[Placeholder for evolution of virtual characters]&lt;/p&gt;

&lt;h3 id=&#34;1-1-2-applications-of-virtual-characters&#34;&gt;1.1.2 Applications of Virtual Characters&lt;/h3&gt;

&lt;p&gt;[Placeholder for applications]&lt;/p&gt;

&lt;h3 id=&#34;1-1-3-portraying-natural-human-like-behaviour-and-scalability&#34;&gt;1.1.3 Portraying Natural Human-like Behaviour and Scalability&lt;/h3&gt;

&lt;p&gt;[Placeholder for natural behavior and scalability]&lt;/p&gt;

&lt;h2 id=&#34;1-2-research-questions&#34;&gt;1.2 Research Questions&lt;/h2&gt;

&lt;p&gt;[Placeholder for research questions]&lt;/p&gt;

&lt;h2 id=&#34;1-3-research-approach&#34;&gt;1.3 Research Approach&lt;/h2&gt;

&lt;p&gt;[Placeholder for research approach]&lt;/p&gt;

&lt;h2 id=&#34;1-4-summary-of-contributions&#34;&gt;1.4 Summary of Contributions&lt;/h2&gt;

&lt;h3 id=&#34;1-4-1-data-driven-character-animation&#34;&gt;1.4.1 Data-driven Character Animation&lt;/h3&gt;

&lt;p&gt;[Placeholder for data-driven contributions]&lt;/p&gt;

&lt;h3 id=&#34;1-4-2-model-based-reinforcement-learning&#34;&gt;1.4.2 Model-based Reinforcement Learning&lt;/h3&gt;

&lt;p&gt;[Placeholder for model-based RL contributions]&lt;/p&gt;

&lt;h2 id=&#34;1-5-thesis-organisation&#34;&gt;1.5 Thesis Organisation&lt;/h2&gt;

&lt;p&gt;[Placeholder for thesis organization]&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Appendix B: Exploring Model-Free RL</title>
      <link>https://vihanga.github.io/thesis/appendices/b/</link>
      <pubDate>Wed, 22 Jan 2025 00:00:00 +0000</pubDate>
      
      <guid>https://vihanga.github.io/thesis/appendices/b/</guid>
      <description>

&lt;h1 id=&#34;appendix-b-exploring-model-free-rl&#34;&gt;Appendix B: Exploring Model-Free RL&lt;/h1&gt;

&lt;p&gt;This appendix provides detailed information about model-free reinforcement learning approaches explored in this thesis.&lt;/p&gt;

&lt;h2 id=&#34;b-1-theoretical-foundations&#34;&gt;B.1 Theoretical Foundations&lt;/h2&gt;

&lt;h3 id=&#34;b-1-1-model-free-vs-model-based-rl&#34;&gt;B.1.1 Model-Free vs Model-Based RL&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;Key distinctions and trade-offs&lt;/li&gt;
&lt;li&gt;Computational complexity analysis&lt;/li&gt;
&lt;li&gt;Sample efficiency considerations&lt;/li&gt;
&lt;li&gt;Generalization capabilities&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;b-1-2-core-algorithms&#34;&gt;B.1.2 Core Algorithms&lt;/h3&gt;

&lt;h4 id=&#34;value-based-methods&#34;&gt;Value-Based Methods&lt;/h4&gt;

&lt;ul&gt;
&lt;li&gt;Q-Learning fundamentals&lt;/li&gt;
&lt;li&gt;Deep Q-Networks (DQN)&lt;/li&gt;
&lt;li&gt;Double DQN and variants&lt;/li&gt;
&lt;li&gt;Prioritized experience replay&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&#34;policy-gradient-methods&#34;&gt;Policy Gradient Methods&lt;/h4&gt;

&lt;ul&gt;
&lt;li&gt;REINFORCE algorithm&lt;/li&gt;
&lt;li&gt;Actor-Critic methods&lt;/li&gt;
&lt;li&gt;Trust Region Policy Optimization (TRPO)&lt;/li&gt;
&lt;li&gt;Proximal Policy Optimization (PPO)&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;b-2-implementation-details&#34;&gt;B.2 Implementation Details&lt;/h2&gt;

&lt;h3 id=&#34;b-2-1-network-architectures&#34;&gt;B.2.1 Network Architectures&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;Convolutional layers for visual input&lt;/li&gt;
&lt;li&gt;Recurrent components for temporal dependencies&lt;/li&gt;
&lt;li&gt;Attention mechanisms&lt;/li&gt;
&lt;li&gt;Architecture search strategies&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;b-2-2-training-procedures&#34;&gt;B.2.2 Training Procedures&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;Hyperparameter configurations&lt;/li&gt;
&lt;li&gt;Learning rate schedules&lt;/li&gt;
&lt;li&gt;Batch size considerations&lt;/li&gt;
&lt;li&gt;Regularization techniques&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;b-3-experimental-setup&#34;&gt;B.3 Experimental Setup&lt;/h2&gt;

&lt;h3 id=&#34;b-3-1-environment-specifications&#34;&gt;B.3.1 Environment Specifications&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;State space representations&lt;/li&gt;
&lt;li&gt;Action space definitions&lt;/li&gt;
&lt;li&gt;Reward function designs&lt;/li&gt;
&lt;li&gt;Episode termination conditions&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;b-3-2-evaluation-metrics&#34;&gt;B.3.2 Evaluation Metrics&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;Average episode return&lt;/li&gt;
&lt;li&gt;Sample efficiency measures&lt;/li&gt;
&lt;li&gt;Convergence analysis&lt;/li&gt;
&lt;li&gt;Stability indicators&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;b-4-algorithm-comparisons&#34;&gt;B.4 Algorithm Comparisons&lt;/h2&gt;

&lt;h3 id=&#34;b-4-1-performance-analysis&#34;&gt;B.4.1 Performance Analysis&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;Learning curves across different algorithms&lt;/li&gt;
&lt;li&gt;Final performance comparisons&lt;/li&gt;
&lt;li&gt;Computational resource requirements&lt;/li&gt;
&lt;li&gt;Training time analysis&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;b-4-2-ablation-studies&#34;&gt;B.4.2 Ablation Studies&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;Impact of different components&lt;/li&gt;
&lt;li&gt;Sensitivity to hyperparameters&lt;/li&gt;
&lt;li&gt;Architecture variations&lt;/li&gt;
&lt;li&gt;Exploration strategies&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;b-5-code-examples&#34;&gt;B.5 Code Examples&lt;/h2&gt;

&lt;h3 id=&#34;b-5-1-basic-q-learning-implementation&#34;&gt;B.5.1 Basic Q-Learning Implementation&lt;/h3&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Simplified Q-learning pseudocode
def q_learning(env, episodes, alpha, gamma, epsilon):
    Q = initialize_q_table()
    for episode in range(episodes):
        state = env.reset()
        while not done:
            action = epsilon_greedy(Q, state, epsilon)
            next_state, reward, done = env.step(action)
            Q[state, action] += alpha * (reward + gamma * max(Q[next_state]) - Q[state, action])
            state = next_state
    return Q
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;b-5-2-ppo-update-step&#34;&gt;B.5.2 PPO Update Step&lt;/h3&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Simplified PPO update pseudocode
def ppo_update(policy, value_function, trajectories, clip_epsilon):
    for trajectory in trajectories:
        advantages = compute_advantages(trajectory, value_function)
        old_log_probs = compute_log_probs(trajectory, policy)
        
        for epoch in range(ppo_epochs):
            new_log_probs = compute_log_probs(trajectory, policy)
            ratio = exp(new_log_probs - old_log_probs)
            clipped_ratio = clip(ratio, 1 - clip_epsilon, 1 + clip_epsilon)
            policy_loss = -min(ratio * advantages, clipped_ratio * advantages)
            optimize(policy_loss)
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>Chapter 2: Data-driven Character Animation</title>
      <link>https://vihanga.github.io/thesis/chapters/data-driven-animation/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://vihanga.github.io/thesis/chapters/data-driven-animation/</guid>
      <description>

&lt;h2 id=&#34;2-1-creation-and-animation-of-virtual-characters&#34;&gt;2.1 Creation and Animation of Virtual Characters&lt;/h2&gt;

&lt;h3 id=&#34;2-1-1-creating-three-dimensional-graphical-representations&#34;&gt;2.1.1 Creating Three-Dimensional Graphical Representations&lt;/h3&gt;

&lt;p&gt;[Placeholder for 3D graphical representations]&lt;/p&gt;

&lt;h3 id=&#34;2-1-2-applying-animation-data-to-3d-character-models&#34;&gt;2.1.2 Applying Animation Data to 3D Character Models&lt;/h3&gt;

&lt;p&gt;[Placeholder for animation data application]&lt;/p&gt;

&lt;h3 id=&#34;2-1-3-rendering-characters&#34;&gt;2.1.3 Rendering Characters&lt;/h3&gt;

&lt;p&gt;[Placeholder for character rendering]&lt;/p&gt;

&lt;h2 id=&#34;2-2-perception-and-application-of-virtual-characters&#34;&gt;2.2 Perception and Application of Virtual Characters&lt;/h2&gt;

&lt;h3 id=&#34;2-2-1-perception-of-virtual-characters&#34;&gt;2.2.1 Perception of Virtual Characters&lt;/h3&gt;

&lt;p&gt;[Placeholder for perception of virtual characters]&lt;/p&gt;

&lt;h3 id=&#34;2-2-2-using-virtual-characters-in-applications&#34;&gt;2.2.2 Using Virtual Characters in Applications&lt;/h3&gt;

&lt;p&gt;[Placeholder for applications]&lt;/p&gt;

&lt;h2 id=&#34;2-3-procedural-character-animation&#34;&gt;2.3 Procedural Character Animation&lt;/h2&gt;

&lt;h3 id=&#34;2-3-1-neural-network-based-methods&#34;&gt;2.3.1 Neural network-based Methods&lt;/h3&gt;

&lt;p&gt;[Placeholder for neural network methods]&lt;/p&gt;

&lt;h3 id=&#34;2-3-2-physics-based-reinforcement-learning-approaches&#34;&gt;2.3.2 Physics-based Reinforcement Learning Approaches&lt;/h3&gt;

&lt;p&gt;[Placeholder for physics-based RL]&lt;/p&gt;

&lt;h3 id=&#34;2-3-3-contemporary-architectural-approaches&#34;&gt;2.3.3 Contemporary Architectural Approaches&lt;/h3&gt;

&lt;p&gt;[Placeholder for contemporary approaches]&lt;/p&gt;

&lt;h2 id=&#34;2-4-chapter-summary&#34;&gt;2.4 Chapter Summary&lt;/h2&gt;

&lt;p&gt;[Placeholder for chapter summary]&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Appendix C: Supplementary Material Index</title>
      <link>https://vihanga.github.io/thesis/appendices/c5a/</link>
      <pubDate>Wed, 22 Jan 2025 00:00:00 +0000</pubDate>
      
      <guid>https://vihanga.github.io/thesis/appendices/c5a/</guid>
      <description>

&lt;h1 id=&#34;appendix-c-supplementary-material-index&#34;&gt;Appendix C: Supplementary Material Index&lt;/h1&gt;

&lt;p&gt;This appendix provides a comprehensive index of all supplementary materials associated with this thesis, including datasets, code repositories, multimedia content, and additional documentation.&lt;/p&gt;

&lt;h2 id=&#34;c-1-code-repositories&#34;&gt;C.1 Code Repositories&lt;/h2&gt;

&lt;h3 id=&#34;c-1-1-main-research-code&#34;&gt;C.1.1 Main Research Code&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Repository URL&lt;/strong&gt;: [GitHub/research-code]&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;License&lt;/strong&gt;: MIT License&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Languages&lt;/strong&gt;: Python, C++, MATLAB&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Key Components&lt;/strong&gt;:

&lt;ul&gt;
&lt;li&gt;RL algorithm implementations&lt;/li&gt;
&lt;li&gt;Data processing pipelines&lt;/li&gt;
&lt;li&gt;Visualization tools&lt;/li&gt;
&lt;li&gt;Experiment runners&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;c-1-2-evaluation-framework&#34;&gt;C.1.2 Evaluation Framework&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Repository URL&lt;/strong&gt;: [GitHub/evaluation-framework]&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Documentation&lt;/strong&gt;: Available in &lt;code&gt;/docs&lt;/code&gt; directory&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Dependencies&lt;/strong&gt;: Listed in &lt;code&gt;requirements.txt&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Installation Guide&lt;/strong&gt;: See &lt;code&gt;README.md&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;c-2-datasets&#34;&gt;C.2 Datasets&lt;/h2&gt;

&lt;h3 id=&#34;c-2-1-motion-capture-data&#34;&gt;C.2.1 Motion Capture Data&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Format&lt;/strong&gt;: BVH, FBX, and custom JSON&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Size&lt;/strong&gt;: ~12GB compressed&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Access&lt;/strong&gt;: Available upon request&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Contents&lt;/strong&gt;:

&lt;ul&gt;
&lt;li&gt;500+ motion sequences&lt;/li&gt;
&lt;li&gt;Multiple actor performances&lt;/li&gt;
&lt;li&gt;Various activity types&lt;/li&gt;
&lt;li&gt;Annotation metadata&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;c-2-2-training-data&#34;&gt;C.2.2 Training Data&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Format&lt;/strong&gt;: HDF5 files&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Organization&lt;/strong&gt;: By experiment type&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Preprocessing&lt;/strong&gt;: Scripts included&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Statistics&lt;/strong&gt;:

&lt;ul&gt;
&lt;li&gt;Training samples: 1M+&lt;/li&gt;
&lt;li&gt;Validation samples: 200K&lt;/li&gt;
&lt;li&gt;Test samples: 100K&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;c-3-multimedia-content&#34;&gt;C.3 Multimedia Content&lt;/h2&gt;

&lt;h3 id=&#34;c-3-1-video-demonstrations&#34;&gt;C.3.1 Video Demonstrations&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Location&lt;/strong&gt;: [Project website/videos]&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Format&lt;/strong&gt;: MP4 (H.264)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Contents&lt;/strong&gt;:

&lt;ul&gt;
&lt;li&gt;Algorithm comparisons&lt;/li&gt;
&lt;li&gt;Real-time demonstrations&lt;/li&gt;
&lt;li&gt;User study recordings&lt;/li&gt;
&lt;li&gt;System walkthroughs&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;c-3-2-interactive-demos&#34;&gt;C.3.2 Interactive Demos&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Platform&lt;/strong&gt;: WebGL builds&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Requirements&lt;/strong&gt;: Modern web browser&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Features&lt;/strong&gt;:

&lt;ul&gt;
&lt;li&gt;Real-time parameter adjustment&lt;/li&gt;
&lt;li&gt;Multiple scenario selection&lt;/li&gt;
&lt;li&gt;Performance visualization&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;c-4-additional-documentation&#34;&gt;C.4 Additional Documentation&lt;/h2&gt;

&lt;h3 id=&#34;c-4-1-extended-results&#34;&gt;C.4.1 Extended Results&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Filename&lt;/strong&gt;: &lt;code&gt;extended_results.pdf&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Pages&lt;/strong&gt;: 150+&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Contents&lt;/strong&gt;:

&lt;ul&gt;
&lt;li&gt;Full experimental data&lt;/li&gt;
&lt;li&gt;Additional ablation studies&lt;/li&gt;
&lt;li&gt;Statistical analyses&lt;/li&gt;
&lt;li&gt;Error analysis&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;c-4-2-implementation-notes&#34;&gt;C.4.2 Implementation Notes&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Filename&lt;/strong&gt;: &lt;code&gt;implementation_guide.pdf&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Topics Covered&lt;/strong&gt;:

&lt;ul&gt;
&lt;li&gt;System architecture&lt;/li&gt;
&lt;li&gt;Algorithm optimizations&lt;/li&gt;
&lt;li&gt;Platform-specific considerations&lt;/li&gt;
&lt;li&gt;Troubleshooting guide&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;c-5-external-resources&#34;&gt;C.5 External Resources&lt;/h2&gt;

&lt;h3 id=&#34;c-5-1-third-party-libraries&#34;&gt;C.5.1 Third-Party Libraries&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;TensorFlow&lt;/strong&gt;: v2.x&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;PyTorch&lt;/strong&gt;: v1.x&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;OpenAI Gym&lt;/strong&gt;: v0.x&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Unity ML-Agents&lt;/strong&gt;: v2.x&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;c-5-2-reference-implementations&#34;&gt;C.5.2 Reference Implementations&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;Links to baseline implementations&lt;/li&gt;
&lt;li&gt;Comparison benchmarks&lt;/li&gt;
&lt;li&gt;Integration examples&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;c-6-access-information&#34;&gt;C.6 Access Information&lt;/h2&gt;

&lt;h3 id=&#34;c-6-1-public-resources&#34;&gt;C.6.1 Public Resources&lt;/h3&gt;

&lt;p&gt;All public resources can be accessed at:
- Project website: [URL]
- DOI: [10.xxxx/xxxxx]&lt;/p&gt;

&lt;h3 id=&#34;c-6-2-restricted-materials&#34;&gt;C.6.2 Restricted Materials&lt;/h3&gt;

&lt;p&gt;For access to restricted materials:
- Contact: [email]
- Affiliation requirements
- Usage agreements&lt;/p&gt;

&lt;h2 id=&#34;c-7-version-control&#34;&gt;C.7 Version Control&lt;/h2&gt;

&lt;h3 id=&#34;c-7-1-release-history&#34;&gt;C.7.1 Release History&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;v1.0: Initial release&lt;/li&gt;
&lt;li&gt;v1.1: Bug fixes and documentation updates&lt;/li&gt;
&lt;li&gt;v1.2: Additional experiments added&lt;/li&gt;
&lt;li&gt;v2.0: Major algorithm improvements&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;c-7-2-update-notifications&#34;&gt;C.7.2 Update Notifications&lt;/h3&gt;

&lt;p&gt;Subscribe to updates at: [project-updates-list]&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Chapter 3: Model-based Reinforcement Learning</title>
      <link>https://vihanga.github.io/thesis/chapters/model-based-rl/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://vihanga.github.io/thesis/chapters/model-based-rl/</guid>
      <description>

&lt;h2 id=&#34;3-1-introduction-to-reinforcement-learning&#34;&gt;3.1 Introduction to Reinforcement Learning&lt;/h2&gt;

&lt;h3 id=&#34;3-1-1-markov-decision-processes&#34;&gt;3.1.1 Markov Decision Processes&lt;/h3&gt;

&lt;p&gt;[Placeholder for MDP content]&lt;/p&gt;

&lt;h3 id=&#34;3-1-2-model-free-vs-model-based-reinforcement-learning&#34;&gt;3.1.2 Model-free vs model-based Reinforcement Learning&lt;/h3&gt;

&lt;p&gt;[Placeholder for model-free vs model-based comparison]&lt;/p&gt;

&lt;h3 id=&#34;3-1-3-reward-functions&#34;&gt;3.1.3 Reward Functions&lt;/h3&gt;

&lt;p&gt;[Placeholder for reward functions]&lt;/p&gt;

&lt;h2 id=&#34;3-2-model-based-reinforcement-learning&#34;&gt;3.2 Model-based Reinforcement Learning&lt;/h2&gt;

&lt;h3 id=&#34;3-2-1-latent-dynamics-models&#34;&gt;3.2.1 Latent Dynamics Models&lt;/h3&gt;

&lt;p&gt;[Placeholder for latent dynamics models]&lt;/p&gt;

&lt;h3 id=&#34;3-2-2-perspectives-on-model-based-reinforcement-learning&#34;&gt;3.2.2 Perspectives on Model-based Reinforcement Learning&lt;/h3&gt;

&lt;p&gt;[Placeholder for perspectives]&lt;/p&gt;

&lt;h2 id=&#34;3-3-representations-and-adversaries&#34;&gt;3.3 Representations and Adversaries&lt;/h2&gt;

&lt;h3 id=&#34;3-3-1-representation-learning&#34;&gt;3.3.1 Representation Learning&lt;/h3&gt;

&lt;p&gt;[Placeholder for representation learning]&lt;/p&gt;

&lt;h3 id=&#34;3-3-2-adversarial-learning-for-regularisation&#34;&gt;3.3.2 Adversarial Learning for Regularisation&lt;/h3&gt;

&lt;p&gt;[Placeholder for adversarial learning]&lt;/p&gt;

&lt;h2 id=&#34;3-4-discussion&#34;&gt;3.4 Discussion&lt;/h2&gt;

&lt;p&gt;[Placeholder for discussion]&lt;/p&gt;

&lt;h2 id=&#34;3-5-chapter-summary&#34;&gt;3.5 Chapter Summary&lt;/h2&gt;

&lt;p&gt;[Placeholder for chapter summary]&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Appendix D: Perceptual Evaluation</title>
      <link>https://vihanga.github.io/thesis/appendices/d/</link>
      <pubDate>Wed, 22 Jan 2025 00:00:00 +0000</pubDate>
      
      <guid>https://vihanga.github.io/thesis/appendices/d/</guid>
      <description>

&lt;h1 id=&#34;appendix-d-perceptual-evaluation&#34;&gt;Appendix D: Perceptual Evaluation&lt;/h1&gt;

&lt;p&gt;This appendix presents detailed information about the perceptual evaluation studies conducted to assess the quality and realism of generated animations and interactive systems.&lt;/p&gt;

&lt;h2 id=&#34;d-1-study-design&#34;&gt;D.1 Study Design&lt;/h2&gt;

&lt;h3 id=&#34;d-1-1-methodology-overview&#34;&gt;D.1.1 Methodology Overview&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Study Type&lt;/strong&gt;: Mixed-methods approach&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Duration&lt;/strong&gt;: 6 weeks&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Participants&lt;/strong&gt;: N=120&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Sessions&lt;/strong&gt;: 3 per participant&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;IRB Approval&lt;/strong&gt;: #2024-XXX&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;d-1-2-participant-demographics&#34;&gt;D.1.2 Participant Demographics&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Age Range&lt;/strong&gt;: 18-65 years&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Gender Distribution&lt;/strong&gt;: 48% female, 52% male&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Experience Levels&lt;/strong&gt;:

&lt;ul&gt;
&lt;li&gt;Novice users: 40%&lt;/li&gt;
&lt;li&gt;Intermediate: 35%&lt;/li&gt;
&lt;li&gt;Expert: 25%&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Background&lt;/strong&gt;:

&lt;ul&gt;
&lt;li&gt;Computer graphics professionals: 20%&lt;/li&gt;
&lt;li&gt;Gamers: 30%&lt;/li&gt;
&lt;li&gt;General users: 50%&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;d-2-evaluation-metrics&#34;&gt;D.2 Evaluation Metrics&lt;/h2&gt;

&lt;h3 id=&#34;d-2-1-quantitative-measures&#34;&gt;D.2.1 Quantitative Measures&lt;/h3&gt;

&lt;h4 id=&#34;visual-realism&#34;&gt;Visual Realism&lt;/h4&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Likert Scale&lt;/strong&gt;: 1-7 rating system&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Comparison Rankings&lt;/strong&gt;: Forced choice paradigm&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Response Times&lt;/strong&gt;: Measured in milliseconds&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Eye Tracking Data&lt;/strong&gt;: Fixation patterns and saccades&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&#34;motion-quality&#34;&gt;Motion Quality&lt;/h4&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Naturalness Ratings&lt;/strong&gt;: Continuous scale 0-100&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Jerkiness Detection&lt;/strong&gt;: Binary classification&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Timing Accuracy&lt;/strong&gt;: Deviation from reference&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Smoothness Metrics&lt;/strong&gt;: Objective calculations&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;d-2-2-qualitative-measures&#34;&gt;D.2.2 Qualitative Measures&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Open-ended Feedback&lt;/strong&gt;: Thematic analysis&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Interview Responses&lt;/strong&gt;: Semi-structured format&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Think-aloud Protocols&lt;/strong&gt;: During interaction&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Post-study Questionnaires&lt;/strong&gt;: Comprehensive feedback&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;d-3-experimental-conditions&#34;&gt;D.3 Experimental Conditions&lt;/h2&gt;

&lt;h3 id=&#34;d-3-1-stimulus-presentation&#34;&gt;D.3.1 Stimulus Presentation&lt;/h3&gt;

&lt;h4 id=&#34;display-setup&#34;&gt;Display Setup&lt;/h4&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Monitor&lt;/strong&gt;: 27&amp;rdquo; 4K display (3840x2160)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Refresh Rate&lt;/strong&gt;: 144Hz&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Viewing Distance&lt;/strong&gt;: 60cm&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Lighting&lt;/strong&gt;: Controlled ambient conditions&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&#34;stimulus-types&#34;&gt;Stimulus Types&lt;/h4&gt;

&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Static Comparisons&lt;/strong&gt;: Side-by-side presentations&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Dynamic Sequences&lt;/strong&gt;: 10-30 second clips&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Interactive Scenarios&lt;/strong&gt;: Real-time manipulation&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;A/B Testing&lt;/strong&gt;: Randomized presentation order&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&#34;d-3-2-control-conditions&#34;&gt;D.3.2 Control Conditions&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Ground Truth&lt;/strong&gt;: Motion capture reference&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Baseline Methods&lt;/strong&gt;: State-of-the-art comparisons&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Ablation Variants&lt;/strong&gt;: Component-wise evaluation&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Random Conditions&lt;/strong&gt;: Sanity checks&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;d-4-results-analysis&#34;&gt;D.4 Results Analysis&lt;/h2&gt;

&lt;h3 id=&#34;d-4-1-statistical-methods&#34;&gt;D.4.1 Statistical Methods&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;ANOVA&lt;/strong&gt;: Between-subjects effects&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Post-hoc Tests&lt;/strong&gt;: Bonferroni corrections&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Effect Sizes&lt;/strong&gt;: Cohen&amp;rsquo;s d calculations&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Inter-rater Reliability&lt;/strong&gt;: Krippendorff&amp;rsquo;s alpha&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;d-4-2-key-findings&#34;&gt;D.4.2 Key Findings&lt;/h3&gt;

&lt;h4 id=&#34;realism-ratings&#34;&gt;Realism Ratings&lt;/h4&gt;

&lt;ul&gt;
&lt;li&gt;Our method: M=5.8, SD=0.9&lt;/li&gt;
&lt;li&gt;Baseline A: M=4.2, SD=1.2&lt;/li&gt;
&lt;li&gt;Baseline B: M=4.6, SD=1.1&lt;/li&gt;
&lt;li&gt;Ground truth: M=6.5, SD=0.6&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&#34;user-preferences&#34;&gt;User Preferences&lt;/h4&gt;

&lt;ul&gt;
&lt;li&gt;72% preferred our method over baselines&lt;/li&gt;
&lt;li&gt;85% rated as &amp;ldquo;realistic&amp;rdquo; or &amp;ldquo;very realistic&amp;rdquo;&lt;/li&gt;
&lt;li&gt;91% found interactions intuitive&lt;/li&gt;
&lt;li&gt;78% would use in production&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;d-5-detailed-results-tables&#34;&gt;D.5 Detailed Results Tables&lt;/h2&gt;

&lt;h3 id=&#34;d-5-1-condition-comparisons&#34;&gt;D.5.1 Condition Comparisons&lt;/h3&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Method&lt;/th&gt;
&lt;th&gt;Realism&lt;/th&gt;
&lt;th&gt;Smoothness&lt;/th&gt;
&lt;th&gt;Preference&lt;/th&gt;
&lt;th&gt;Response Time&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Ours&lt;/td&gt;
&lt;td&gt;5.8±0.9&lt;/td&gt;
&lt;td&gt;6.1±0.7&lt;/td&gt;
&lt;td&gt;72%&lt;/td&gt;
&lt;td&gt;1.2s&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;Baseline A&lt;/td&gt;
&lt;td&gt;4.2±1.2&lt;/td&gt;
&lt;td&gt;4.8±1.1&lt;/td&gt;
&lt;td&gt;12%&lt;/td&gt;
&lt;td&gt;1.8s&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;Baseline B&lt;/td&gt;
&lt;td&gt;4.6±1.1&lt;/td&gt;
&lt;td&gt;5.2±0.9&lt;/td&gt;
&lt;td&gt;16%&lt;/td&gt;
&lt;td&gt;1.5s&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;h3 id=&#34;d-5-2-task-specific-performance&#34;&gt;D.5.2 Task-Specific Performance&lt;/h3&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Task Type&lt;/th&gt;
&lt;th&gt;Success Rate&lt;/th&gt;
&lt;th&gt;Completion Time&lt;/th&gt;
&lt;th&gt;Satisfaction&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Navigation&lt;/td&gt;
&lt;td&gt;94%&lt;/td&gt;
&lt;td&gt;12.3s&lt;/td&gt;
&lt;td&gt;5.&lt;sup&gt;9&lt;/sup&gt;&amp;frasl;&lt;sub&gt;7&lt;/sub&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;Manipulation&lt;/td&gt;
&lt;td&gt;88%&lt;/td&gt;
&lt;td&gt;18.7s&lt;/td&gt;
&lt;td&gt;5.&lt;sup&gt;6&lt;/sup&gt;&amp;frasl;&lt;sub&gt;7&lt;/sub&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;Creation&lt;/td&gt;
&lt;td&gt;82%&lt;/td&gt;
&lt;td&gt;45.2s&lt;/td&gt;
&lt;td&gt;6.&lt;sup&gt;2&lt;/sup&gt;&amp;frasl;&lt;sub&gt;7&lt;/sub&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;h2 id=&#34;d-6-user-feedback-themes&#34;&gt;D.6 User Feedback Themes&lt;/h2&gt;

&lt;h3 id=&#34;d-6-1-positive-aspects&#34;&gt;D.6.1 Positive Aspects&lt;/h3&gt;

&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Natural Motion&lt;/strong&gt;: &amp;ldquo;Movements felt very lifelike&amp;rdquo;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Responsive Control&lt;/strong&gt;: &amp;ldquo;System reacted instantly&amp;rdquo;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Visual Quality&lt;/strong&gt;: &amp;ldquo;Graphics were impressive&amp;rdquo;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Ease of Use&lt;/strong&gt;: &amp;ldquo;Intuitive interface&amp;rdquo;&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&#34;d-6-2-areas-for-improvement&#34;&gt;D.6.2 Areas for Improvement&lt;/h3&gt;

&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Edge Cases&lt;/strong&gt;: &amp;ldquo;Some extreme poses looked odd&amp;rdquo;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Learning Curve&lt;/strong&gt;: &amp;ldquo;Advanced features need tutorials&amp;rdquo;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Performance&lt;/strong&gt;: &amp;ldquo;Occasional lag with complex scenes&amp;rdquo;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Customization&lt;/strong&gt;: &amp;ldquo;Want more control options&amp;rdquo;&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&#34;d-7-study-materials&#34;&gt;D.7 Study Materials&lt;/h2&gt;

&lt;h3 id=&#34;d-7-1-questionnaires&#34;&gt;D.7.1 Questionnaires&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;Pre-study survey&lt;/li&gt;
&lt;li&gt;Post-task evaluations&lt;/li&gt;
&lt;li&gt;Final assessment form&lt;/li&gt;
&lt;li&gt;NASA-TLX workload assessment&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;d-7-2-instructions&#34;&gt;D.7.2 Instructions&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;Participant information sheet&lt;/li&gt;
&lt;li&gt;Task descriptions&lt;/li&gt;
&lt;li&gt;Training materials&lt;/li&gt;
&lt;li&gt;Debriefing script&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Chapter 4: Model-based and Model-free Animation</title>
      <link>https://vihanga.github.io/thesis/chapters/model-based-animation/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://vihanga.github.io/thesis/chapters/model-based-animation/</guid>
      <description>

&lt;h1 id=&#34;chapter-4-model-based-and-model-free-animation&#34;&gt;Chapter 4: Model-based and Model-free Animation&lt;/h1&gt;

&lt;p&gt;This chapter explores both model-free and model-based approaches to animation control, presenting methods for creating intelligent animation agents that can learn from experience and plan ahead.&lt;/p&gt;

&lt;h2 id=&#34;4-1-model-free-rl-for-animation-control&#34;&gt;4.1 Model-free RL for Animation Control&lt;/h2&gt;

&lt;p&gt;This section introduces model-free reinforcement learning techniques for animation control, where agents learn policies directly from interaction without explicitly modeling the environment dynamics.&lt;/p&gt;

&lt;h3 id=&#34;4-1-1-problem-formulation&#34;&gt;4.1.1 Problem Formulation&lt;/h3&gt;

&lt;p&gt;[Content placeholder: Define the animation control problem as a Markov Decision Process (MDP), including state and action spaces, reward functions, and learning objectives]&lt;/p&gt;

&lt;h3 id=&#34;4-1-2-policy-learning-methods&#34;&gt;4.1.2 Policy Learning Methods&lt;/h3&gt;

&lt;p&gt;[Content placeholder: Discuss various model-free RL algorithms applicable to animation control, including policy gradient methods, actor-critic architectures, and their specific adaptations for character animation]&lt;/p&gt;

&lt;h3 id=&#34;4-1-3-experimental-results&#34;&gt;4.1.3 Experimental Results&lt;/h3&gt;

&lt;p&gt;[Content placeholder: Present experimental results demonstrating the effectiveness of model-free RL for various animation tasks, including locomotion, object manipulation, and athletic movements]&lt;/p&gt;

&lt;h2 id=&#34;4-2-learned-dynamics-models-and-online-planning-for-model-based-animation-agents&#34;&gt;4.2 Learned dynamics models and online planning for model-based animation agents&lt;/h2&gt;

&lt;p&gt;This section presents model-based approaches where agents learn dynamics models of the environment and use them for planning and control.&lt;/p&gt;

&lt;h3 id=&#34;4-2-1-dynamics-model-learning&#34;&gt;4.2.1 Dynamics Model Learning&lt;/h3&gt;

&lt;p&gt;[Content placeholder: Describe methods for learning forward dynamics models from interaction data, including neural network architectures and training procedures]&lt;/p&gt;

&lt;h3 id=&#34;4-2-2-online-planning-algorithms&#34;&gt;4.2.2 Online Planning Algorithms&lt;/h3&gt;

&lt;p&gt;[Content placeholder: Present online planning algorithms that leverage learned dynamics models, including model predictive control (MPC) and sampling-based planning methods]&lt;/p&gt;

&lt;h3 id=&#34;4-2-3-integration-with-model-free-methods&#34;&gt;4.2.3 Integration with Model-free Methods&lt;/h3&gt;

&lt;p&gt;[Content placeholder: Discuss hybrid approaches that combine model-based planning with model-free learning, leveraging the strengths of both paradigms]&lt;/p&gt;

&lt;h3 id=&#34;4-2-4-comparative-analysis&#34;&gt;4.2.4 Comparative Analysis&lt;/h3&gt;

&lt;p&gt;[Content placeholder: Compare model-based and model-free approaches in terms of sample efficiency, computational requirements, and animation quality]&lt;/p&gt;

&lt;h2 id=&#34;4-3-chapter-summary&#34;&gt;4.3 Chapter Summary&lt;/h2&gt;

&lt;p&gt;This chapter has presented a comprehensive exploration of both model-free and model-based approaches to animation control. We demonstrated that model-free methods can produce high-quality animations through direct policy learning, while model-based methods offer improved sample efficiency and planning capabilities. The combination of both approaches provides a powerful framework for creating intelligent animation agents capable of complex behaviors.&lt;/p&gt;

&lt;p&gt;Key contributions of this chapter include:
- A systematic comparison of model-free and model-based approaches for animation control
- Novel algorithms for learning dynamics models suitable for character animation
- Demonstration of online planning methods that produce natural-looking motions
- Insights into the trade-offs between different approaches and their appropriate use cases&lt;/p&gt;

&lt;p&gt;The methods presented in this chapter lay the foundation for more advanced animation systems that can adapt to new tasks, generalize across different characters, and produce increasingly sophisticated behaviors.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Appendix E: Supplementary Results, Model-free RL Experiments</title>
      <link>https://vihanga.github.io/thesis/appendices/e/</link>
      <pubDate>Wed, 22 Jan 2025 00:00:00 +0000</pubDate>
      
      <guid>https://vihanga.github.io/thesis/appendices/e/</guid>
      <description>

&lt;h1 id=&#34;appendix-e-supplementary-results-model-free-rl-experiments&#34;&gt;Appendix E: Supplementary Results, Model-free RL Experiments&lt;/h1&gt;

&lt;p&gt;This appendix contains comprehensive supplementary results from the model-free reinforcement learning experiments conducted throughout this research.&lt;/p&gt;

&lt;h2 id=&#34;e-1-extended-learning-curves&#34;&gt;E.1 Extended Learning Curves&lt;/h2&gt;

&lt;h3 id=&#34;e-1-1-training-performance-over-time&#34;&gt;E.1.1 Training Performance Over Time&lt;/h3&gt;

&lt;h4 id=&#34;ppo-experiments&#34;&gt;PPO Experiments&lt;/h4&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Environment&lt;/strong&gt;: Custom physics simulation&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Training Steps&lt;/strong&gt;: 10M&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Seeds&lt;/strong&gt;: 5 independent runs&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Logging Frequency&lt;/strong&gt;: Every 1000 steps&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;![Learning Curves - PPO]
- Episode Return: Steady improvement from -200 to +150
- Success Rate: 15% → 92% over training
- Sample Efficiency: 2.3M samples to convergence
- Variance: Decreasing with training progress&lt;/p&gt;

&lt;h4 id=&#34;sac-experiments&#34;&gt;SAC Experiments&lt;/h4&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Environment&lt;/strong&gt;: Same as PPO&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Training Steps&lt;/strong&gt;: 10M&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Seeds&lt;/strong&gt;: 5 independent runs&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Key Differences&lt;/strong&gt;:

&lt;ul&gt;
&lt;li&gt;Faster initial learning&lt;/li&gt;
&lt;li&gt;More stable convergence&lt;/li&gt;
&lt;li&gt;Higher final performance (+165 avg return)&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;e-1-2-hyperparameter-sensitivity&#34;&gt;E.1.2 Hyperparameter Sensitivity&lt;/h3&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Hyperparameter&lt;/th&gt;
&lt;th&gt;Range Tested&lt;/th&gt;
&lt;th&gt;Optimal Value&lt;/th&gt;
&lt;th&gt;Impact on Performance&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Learning Rate&lt;/td&gt;
&lt;td&gt;1e-5 to 1e-2&lt;/td&gt;
&lt;td&gt;3e-4&lt;/td&gt;
&lt;td&gt;Critical - 40% variance&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;Batch Size&lt;/td&gt;
&lt;td&gt;32 to 512&lt;/td&gt;
&lt;td&gt;256&lt;/td&gt;
&lt;td&gt;Moderate - 15% variance&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;Discount Factor&lt;/td&gt;
&lt;td&gt;0.9 to 0.999&lt;/td&gt;
&lt;td&gt;0.99&lt;/td&gt;
&lt;td&gt;Low - 8% variance&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;Entropy Coefficient&lt;/td&gt;
&lt;td&gt;0.0 to 0.1&lt;/td&gt;
&lt;td&gt;0.01&lt;/td&gt;
&lt;td&gt;Moderate - 20% variance&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;h2 id=&#34;e-2-ablation-study-results&#34;&gt;E.2 Ablation Study Results&lt;/h2&gt;

&lt;h3 id=&#34;e-2-1-architecture-components&#34;&gt;E.2.1 Architecture Components&lt;/h3&gt;

&lt;h4 id=&#34;network-depth-impact&#34;&gt;Network Depth Impact&lt;/h4&gt;

&lt;pre&gt;&lt;code&gt;Shallow (2 layers): 72% success rate
Medium (4 layers): 89% success rate  
Deep (8 layers): 85% success rate
Very Deep (16 layers): 78% success rate
&lt;/code&gt;&lt;/pre&gt;

&lt;h4 id=&#34;activation-functions&#34;&gt;Activation Functions&lt;/h4&gt;

&lt;ul&gt;
&lt;li&gt;ReLU: Baseline performance&lt;/li&gt;
&lt;li&gt;Tanh: -5% performance&lt;/li&gt;
&lt;li&gt;GELU: +3% performance&lt;/li&gt;
&lt;li&gt;Swish: +2% performance&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;e-2-2-training-techniques&#34;&gt;E.2.2 Training Techniques&lt;/h3&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Technique&lt;/th&gt;
&lt;th&gt;Enabled&lt;/th&gt;
&lt;th&gt;Disabled&lt;/th&gt;
&lt;th&gt;Difference&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Normalization&lt;/td&gt;
&lt;td&gt;92%&lt;/td&gt;
&lt;td&gt;76%&lt;/td&gt;
&lt;td&gt;+16%&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;Dropout&lt;/td&gt;
&lt;td&gt;88%&lt;/td&gt;
&lt;td&gt;92%&lt;/td&gt;
&lt;td&gt;-4%&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;Weight Decay&lt;/td&gt;
&lt;td&gt;90%&lt;/td&gt;
&lt;td&gt;87%&lt;/td&gt;
&lt;td&gt;+3%&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;Gradient Clipping&lt;/td&gt;
&lt;td&gt;92%&lt;/td&gt;
&lt;td&gt;84%&lt;/td&gt;
&lt;td&gt;+8%&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;h2 id=&#34;e-3-detailed-experimental-configurations&#34;&gt;E.3 Detailed Experimental Configurations&lt;/h2&gt;

&lt;h3 id=&#34;e-3-1-environment-specifications&#34;&gt;E.3.1 Environment Specifications&lt;/h3&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;env_config = {
    &#39;observation_space&#39;: Box(low=-inf, high=inf, shape=(128,)),
    &#39;action_space&#39;: Box(low=-1, high=1, shape=(8,)),
    &#39;max_episode_steps&#39;: 1000,
    &#39;reward_scale&#39;: 0.1,
    &#39;physics_timestep&#39;: 0.01,
    &#39;render_fps&#39;: 30
}
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;e-3-2-algorithm-configurations&#34;&gt;E.3.2 Algorithm Configurations&lt;/h3&gt;

&lt;h4 id=&#34;ppo-configuration&#34;&gt;PPO Configuration&lt;/h4&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;ppo_config = {
    &#39;n_steps&#39;: 2048,
    &#39;batch_size&#39;: 64,
    &#39;n_epochs&#39;: 10,
    &#39;gamma&#39;: 0.99,
    &#39;gae_lambda&#39;: 0.95,
    &#39;clip_range&#39;: 0.2,
    &#39;learning_rate&#39;: 3e-4,
    &#39;value_coefficient&#39;: 0.5,
    &#39;entropy_coefficient&#39;: 0.01,
    &#39;max_grad_norm&#39;: 0.5
}
&lt;/code&gt;&lt;/pre&gt;

&lt;h4 id=&#34;sac-configuration&#34;&gt;SAC Configuration&lt;/h4&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;sac_config = {
    &#39;buffer_size&#39;: 1e6,
    &#39;batch_size&#39;: 256,
    &#39;gamma&#39;: 0.99,
    &#39;tau&#39;: 0.005,
    &#39;learning_rate&#39;: 3e-4,
    &#39;alpha&#39;: 0.2,
    &#39;target_update_interval&#39;: 1,
    &#39;gradient_steps&#39;: 1,
    &#39;reward_scale&#39;: 5.0
}
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;e-4-task-specific-results&#34;&gt;E.4 Task-Specific Results&lt;/h2&gt;

&lt;h3 id=&#34;e-4-1-locomotion-tasks&#34;&gt;E.4.1 Locomotion Tasks&lt;/h3&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Task&lt;/th&gt;
&lt;th&gt;PPO Score&lt;/th&gt;
&lt;th&gt;SAC Score&lt;/th&gt;
&lt;th&gt;TD3 Score&lt;/th&gt;
&lt;th&gt;Human Baseline&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Walk&lt;/td&gt;
&lt;td&gt;0.92&lt;/td&gt;
&lt;td&gt;0.94&lt;/td&gt;
&lt;td&gt;0.91&lt;/td&gt;
&lt;td&gt;0.98&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;Run&lt;/td&gt;
&lt;td&gt;0.88&lt;/td&gt;
&lt;td&gt;0.91&lt;/td&gt;
&lt;td&gt;0.89&lt;/td&gt;
&lt;td&gt;0.97&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;Jump&lt;/td&gt;
&lt;td&gt;0.85&lt;/td&gt;
&lt;td&gt;0.82&lt;/td&gt;
&lt;td&gt;0.86&lt;/td&gt;
&lt;td&gt;0.95&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;Turn&lt;/td&gt;
&lt;td&gt;0.94&lt;/td&gt;
&lt;td&gt;0.93&lt;/td&gt;
&lt;td&gt;0.92&lt;/td&gt;
&lt;td&gt;0.99&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;h3 id=&#34;e-4-2-manipulation-tasks&#34;&gt;E.4.2 Manipulation Tasks&lt;/h3&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Task&lt;/th&gt;
&lt;th&gt;Success Rate&lt;/th&gt;
&lt;th&gt;Avg Time (s)&lt;/th&gt;
&lt;th&gt;Precision Score&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Reach&lt;/td&gt;
&lt;td&gt;96%&lt;/td&gt;
&lt;td&gt;2.3&lt;/td&gt;
&lt;td&gt;0.94&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;Grasp&lt;/td&gt;
&lt;td&gt;89%&lt;/td&gt;
&lt;td&gt;3.7&lt;/td&gt;
&lt;td&gt;0.87&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;Place&lt;/td&gt;
&lt;td&gt;84%&lt;/td&gt;
&lt;td&gt;5.2&lt;/td&gt;
&lt;td&gt;0.82&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;Stack&lt;/td&gt;
&lt;td&gt;76%&lt;/td&gt;
&lt;td&gt;8.4&lt;/td&gt;
&lt;td&gt;0.78&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;h2 id=&#34;e-5-computational-performance&#34;&gt;E.5 Computational Performance&lt;/h2&gt;

&lt;h3 id=&#34;e-5-1-training-efficiency&#34;&gt;E.5.1 Training Efficiency&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;PPO&lt;/strong&gt;: 15.2 hours on single GPU&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;SAC&lt;/strong&gt;: 18.6 hours on single GPU&lt;br /&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;TD3&lt;/strong&gt;: 16.9 hours on single GPU&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;A2C&lt;/strong&gt;: 12.1 hours on single GPU&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;e-5-2-inference-speed&#34;&gt;E.5.2 Inference Speed&lt;/h3&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Algorithm&lt;/th&gt;
&lt;th&gt;FPS (CPU)&lt;/th&gt;
&lt;th&gt;FPS (GPU)&lt;/th&gt;
&lt;th&gt;Latency (ms)&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;PPO&lt;/td&gt;
&lt;td&gt;850&lt;/td&gt;
&lt;td&gt;3200&lt;/td&gt;
&lt;td&gt;1.2&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;SAC&lt;/td&gt;
&lt;td&gt;720&lt;/td&gt;
&lt;td&gt;2900&lt;/td&gt;
&lt;td&gt;1.4&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;TD3&lt;/td&gt;
&lt;td&gt;780&lt;/td&gt;
&lt;td&gt;3100&lt;/td&gt;
&lt;td&gt;1.3&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;A2C&lt;/td&gt;
&lt;td&gt;920&lt;/td&gt;
&lt;td&gt;3400&lt;/td&gt;
&lt;td&gt;1.1&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;h2 id=&#34;e-6-failure-case-analysis&#34;&gt;E.6 Failure Case Analysis&lt;/h2&gt;

&lt;h3 id=&#34;e-6-1-common-failure-modes&#34;&gt;E.6.1 Common Failure Modes&lt;/h3&gt;

&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Catastrophic Forgetting&lt;/strong&gt;: Occurred in 8% of runs&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Local Minima&lt;/strong&gt;: Trapped in suboptimal policies (12%)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Exploration Collapse&lt;/strong&gt;: Premature convergence (6%)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Reward Hacking&lt;/strong&gt;: Exploiting reward function (4%)&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&#34;e-6-2-mitigation-strategies&#34;&gt;E.6.2 Mitigation Strategies&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;Periodic evaluation checkpoints&lt;/li&gt;
&lt;li&gt;Adaptive exploration schedules&lt;/li&gt;
&lt;li&gt;Reward function shaping&lt;/li&gt;
&lt;li&gt;Ensemble methods for robustness&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;e-7-statistical-significance-tests&#34;&gt;E.7 Statistical Significance Tests&lt;/h2&gt;

&lt;h3 id=&#34;e-7-1-performance-comparisons&#34;&gt;E.7.1 Performance Comparisons&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;PPO vs SAC: p = 0.032 (significant)&lt;/li&gt;
&lt;li&gt;PPO vs TD3: p = 0.186 (not significant)&lt;/li&gt;
&lt;li&gt;SAC vs TD3: p = 0.041 (significant)&lt;/li&gt;
&lt;li&gt;All vs Random: p &amp;lt; 0.001 (highly significant)&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;e-7-2-confidence-intervals&#34;&gt;E.7.2 Confidence Intervals&lt;/h3&gt;

&lt;p&gt;All results reported with 95% confidence intervals calculated using bootstrap resampling (n=1000).&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Chapter 5: Data-Driven Reinforcement Learning with Off-Policy Data</title>
      <link>https://vihanga.github.io/thesis/chapters/data-driven-rl/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://vihanga.github.io/thesis/chapters/data-driven-rl/</guid>
      <description>

&lt;h1 id=&#34;chapter-5-data-driven-reinforcement-learning-with-off-policy-data&#34;&gt;Chapter 5: Data-Driven Reinforcement Learning with Off-Policy Data&lt;/h1&gt;

&lt;p&gt;This chapter addresses the challenge of leveraging existing animation data within reinforcement learning frameworks, presenting methods for combining data-driven approaches with RL to create more efficient and higher-quality animation systems.&lt;/p&gt;

&lt;h2 id=&#34;5-1-introduction&#34;&gt;5.1 Introduction&lt;/h2&gt;

&lt;p&gt;[Content placeholder: Introduce the motivation for combining data-driven methods with RL, discussing the abundance of motion capture data and the limitations of pure RL approaches in animation]&lt;/p&gt;

&lt;h2 id=&#34;5-2-off-policy-learning-from-motion-data&#34;&gt;5.2 Off-Policy Learning from Motion Data&lt;/h2&gt;

&lt;p&gt;This section presents methods for learning from pre-recorded motion data within an RL framework, enabling agents to benefit from expert demonstrations while maintaining the flexibility to explore and improve.&lt;/p&gt;

&lt;h3 id=&#34;5-2-1-problem-formulation&#34;&gt;5.2.1 Problem Formulation&lt;/h3&gt;

&lt;p&gt;[Content placeholder: Define the off-policy learning problem in the context of character animation, including notation and key challenges]&lt;/p&gt;

&lt;h3 id=&#34;5-2-2-data-collection-and-processing&#34;&gt;5.2.2 Data Collection and Processing&lt;/h3&gt;

&lt;p&gt;[Content placeholder: Describe methods for collecting and processing motion capture data for use in off-policy RL, including data augmentation and quality assessment]&lt;/p&gt;

&lt;h3 id=&#34;5-2-3-off-policy-algorithms&#34;&gt;5.2.3 Off-Policy Algorithms&lt;/h3&gt;

&lt;p&gt;[Content placeholder: Present specific off-policy algorithms adapted for animation, including importance sampling corrections and replay buffer management]&lt;/p&gt;

&lt;h2 id=&#34;5-3-combining-imitation-and-reinforcement-learning&#34;&gt;5.3 Combining Imitation and Reinforcement Learning&lt;/h2&gt;

&lt;p&gt;This section explores hybrid approaches that combine imitation learning from motion data with reinforcement learning for task achievement.&lt;/p&gt;

&lt;h3 id=&#34;5-3-1-behavioral-cloning-with-rl-fine-tuning&#34;&gt;5.3.1 Behavioral Cloning with RL Fine-tuning&lt;/h3&gt;

&lt;p&gt;[Content placeholder: Describe methods that initialize policies through behavioral cloning and then refine them using RL]&lt;/p&gt;

&lt;h3 id=&#34;5-3-2-reward-shaping-with-motion-priors&#34;&gt;5.3.2 Reward Shaping with Motion Priors&lt;/h3&gt;

&lt;p&gt;[Content placeholder: Present approaches for incorporating motion data as priors in reward functions to guide RL exploration]&lt;/p&gt;

&lt;h3 id=&#34;5-3-3-adversarial-motion-learning&#34;&gt;5.3.3 Adversarial Motion Learning&lt;/h3&gt;

&lt;p&gt;[Content placeholder: Discuss adversarial methods that learn to distinguish between generated and reference motions, encouraging natural movement patterns]&lt;/p&gt;

&lt;h2 id=&#34;5-4-experimental-evaluation&#34;&gt;5.4 Experimental Evaluation&lt;/h2&gt;

&lt;h3 id=&#34;5-4-1-benchmark-tasks&#34;&gt;5.4.1 Benchmark Tasks&lt;/h3&gt;

&lt;p&gt;[Content placeholder: Define a set of benchmark tasks for evaluating data-driven RL methods in animation]&lt;/p&gt;

&lt;h3 id=&#34;5-4-2-quantitative-results&#34;&gt;5.4.2 Quantitative Results&lt;/h3&gt;

&lt;p&gt;[Content placeholder: Present quantitative comparisons of different approaches, including learning curves, final performance, and motion quality metrics]&lt;/p&gt;

&lt;h3 id=&#34;5-4-3-qualitative-analysis&#34;&gt;5.4.3 Qualitative Analysis&lt;/h3&gt;

&lt;p&gt;[Content placeholder: Provide qualitative analysis of generated animations, including visual comparisons and expert evaluations]&lt;/p&gt;

&lt;h2 id=&#34;5-5-applications&#34;&gt;5.5 Applications&lt;/h2&gt;

&lt;h3 id=&#34;5-5-1-style-transfer-and-motion-editing&#34;&gt;5.5.1 Style Transfer and Motion Editing&lt;/h3&gt;

&lt;p&gt;[Content placeholder: Demonstrate applications to style transfer between different motion styles and interactive motion editing]&lt;/p&gt;

&lt;h3 id=&#34;5-5-2-multi-skill-learning&#34;&gt;5.5.2 Multi-skill Learning&lt;/h3&gt;

&lt;p&gt;[Content placeholder: Show how data-driven RL can enable learning of multiple skills from diverse motion datasets]&lt;/p&gt;

&lt;h3 id=&#34;5-5-3-adaptive-character-control&#34;&gt;5.5.3 Adaptive Character Control&lt;/h3&gt;

&lt;p&gt;[Content placeholder: Present applications to adaptive character control that can handle varying environments and tasks]&lt;/p&gt;

&lt;h2 id=&#34;5-6-chapter-summary&#34;&gt;5.6 Chapter Summary&lt;/h2&gt;

&lt;p&gt;This chapter has presented methods for effectively combining data-driven approaches with reinforcement learning for character animation. By leveraging existing motion data within an RL framework, we can create animation systems that benefit from both the quality of recorded motions and the adaptability of learned policies.&lt;/p&gt;

&lt;p&gt;Key contributions include:
- Novel algorithms for off-policy learning from motion capture data
- Hybrid approaches that balance imitation and task-oriented learning
- Demonstration of improved sample efficiency and motion quality
- Applications to various animation problems including style transfer and multi-skill learning&lt;/p&gt;

&lt;p&gt;The methods developed in this chapter enable more practical deployment of RL-based animation systems by reducing the need for extensive online exploration while maintaining the flexibility to adapt to new tasks and environments.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Appendix F: Star Jump Ideal Action Calculation</title>
      <link>https://vihanga.github.io/thesis/appendices/f/</link>
      <pubDate>Wed, 22 Jan 2025 00:00:00 +0000</pubDate>
      
      <guid>https://vihanga.github.io/thesis/appendices/f/</guid>
      <description>

&lt;h1 id=&#34;appendix-f-star-jump-ideal-action-calculation&#34;&gt;Appendix F: Star Jump Ideal Action Calculation&lt;/h1&gt;

&lt;p&gt;This appendix provides detailed mathematical derivations and calculations for determining ideal actions in the star jump motion synthesis task.&lt;/p&gt;

&lt;h2 id=&#34;f-1-problem-formulation&#34;&gt;F.1 Problem Formulation&lt;/h2&gt;

&lt;h3 id=&#34;f-1-1-star-jump-motion-definition&#34;&gt;F.1.1 Star Jump Motion Definition&lt;/h3&gt;

&lt;p&gt;A star jump consists of the following phases:
1. &lt;strong&gt;Preparation Phase&lt;/strong&gt;: Initial crouching position
2. &lt;strong&gt;Launch Phase&lt;/strong&gt;: Explosive upward movement
3. &lt;strong&gt;Aerial Phase&lt;/strong&gt;: Body forms star shape in mid-air
4. &lt;strong&gt;Landing Phase&lt;/strong&gt;: Controlled descent and stabilization&lt;/p&gt;

&lt;h3 id=&#34;f-1-2-state-space-definition&#34;&gt;F.1.2 State Space Definition&lt;/h3&gt;

&lt;p&gt;The state vector &lt;strong&gt;s&lt;/strong&gt; ∈ ℝⁿ consists of:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;s = [q, q̇, h, φ, τ]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Where:
- &lt;strong&gt;q&lt;/strong&gt; ∈ ℝ²¹: Joint angles (7 joints × 3 DOF)
- &lt;strong&gt;q̇&lt;/strong&gt; ∈ ℝ²¹: Joint velocities
- &lt;strong&gt;h&lt;/strong&gt; ∈ ℝ: Height of center of mass
- &lt;strong&gt;φ&lt;/strong&gt; ∈ ℝ³: Body orientation (Euler angles)
- &lt;strong&gt;τ&lt;/strong&gt; ∈ [0,1]: Phase timing parameter&lt;/p&gt;

&lt;h2 id=&#34;f-2-ideal-action-derivation&#34;&gt;F.2 Ideal Action Derivation&lt;/h2&gt;

&lt;h3 id=&#34;f-2-1-kinematic-constraints&#34;&gt;F.2.1 Kinematic Constraints&lt;/h3&gt;

&lt;p&gt;The ideal star jump configuration at peak height satisfies:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;q*_shoulder = [π/2, 0, π/4]  # Arms raised laterally
q*_hip = [π/4, 0, π/6]       # Legs spread wide
q*_spine = [0, 0, -π/12]     # Slight back arch
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;f-2-2-dynamic-optimization&#34;&gt;F.2.2 Dynamic Optimization&lt;/h3&gt;

&lt;p&gt;The optimal action sequence &lt;strong&gt;a&lt;/strong&gt;(t) minimizes:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;J = ∫[αE(t) + βD(q,q*) + γS(q̇) + δB(τ)]dt
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Where:
- &lt;strong&gt;E(t)&lt;/strong&gt;: Energy expenditure term
- &lt;strong&gt;D(q,q*)&lt;/strong&gt;: Deviation from ideal pose
- &lt;strong&gt;S(q̇)&lt;/strong&gt;: Smoothness penalty
- &lt;strong&gt;B(τ)&lt;/strong&gt;: Balance maintenance term&lt;/p&gt;

&lt;p&gt;Weights: α=0.1, β=1.0, γ=0.3, δ=0.5&lt;/p&gt;

&lt;h2 id=&#34;f-3-phase-specific-calculations&#34;&gt;F.3 Phase-Specific Calculations&lt;/h2&gt;

&lt;h3 id=&#34;f-3-1-preparation-phase-τ-0-0-2&#34;&gt;F.3.1 Preparation Phase (τ ∈ [0, 0.2])&lt;/h3&gt;

&lt;h4 id=&#34;joint-targets&#34;&gt;Joint Targets&lt;/h4&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def preparation_targets(τ):
    crouch_factor = smooth_interpolate(0, 1, τ/0.2)
    q_target = {
        &#39;hip&#39;: [-π/3 * crouch_factor, 0, 0],
        &#39;knee&#39;: [π/2 * crouch_factor, 0, 0],
        &#39;ankle&#39;: [-π/6 * crouch_factor, 0, 0],
        &#39;shoulder&#39;: [π/6 * crouch_factor, 0, 0],
        &#39;elbow&#39;: [-π/4 * crouch_factor, 0, 0]
    }
    return q_target
&lt;/code&gt;&lt;/pre&gt;

&lt;h4 id=&#34;force-requirements&#34;&gt;Force Requirements&lt;/h4&gt;

&lt;p&gt;Vertical ground reaction force:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;F_z = m*g + m*a_prep
a_prep = -2.5 m/s² (downward acceleration)
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;f-3-2-launch-phase-τ-0-2-0-35&#34;&gt;F.3.2 Launch Phase (τ ∈ [0.2, 0.35])&lt;/h3&gt;

&lt;h4 id=&#34;optimal-launch-velocity&#34;&gt;Optimal Launch Velocity&lt;/h4&gt;

&lt;p&gt;Using projectile motion equations:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;v₀ = √(2gh_target)
h_target = 0.5m (typical jump height)
v₀ ≈ 3.13 m/s
&lt;/code&gt;&lt;/pre&gt;

&lt;h4 id=&#34;joint-torques&#34;&gt;Joint Torques&lt;/h4&gt;

&lt;p&gt;Maximum torques during launch:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;τ_max = {
    &#39;hip&#39;: 250 Nm,
    &#39;knee&#39;: 180 Nm,
    &#39;ankle&#39;: 120 Nm,
    &#39;shoulder&#39;: 60 Nm
}
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;f-3-3-aerial-phase-τ-0-35-0-65&#34;&gt;F.3.3 Aerial Phase (τ ∈ [0.35, 0.65])&lt;/h3&gt;

&lt;h4 id=&#34;star-formation-trajectory&#34;&gt;Star Formation Trajectory&lt;/h4&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def aerial_trajectory(t_flight):
    # Normalized time in aerial phase
    t_norm = (t_flight - 0.35) / 0.3
    
    # Smooth transition to star pose
    spread_factor = sin(π * t_norm)
    
    q_aerial = {
        &#39;shoulder_abduction&#39;: π/2 * spread_factor,
        &#39;shoulder_flexion&#39;: π/6 * spread_factor,
        &#39;hip_abduction&#39;: π/3 * spread_factor,
        &#39;hip_flexion&#39;: π/6 * spread_factor,
        &#39;spine_extension&#39;: -π/12 * spread_factor
    }
    return q_aerial
&lt;/code&gt;&lt;/pre&gt;

&lt;h4 id=&#34;angular-momentum-conservation&#34;&gt;Angular Momentum Conservation&lt;/h4&gt;

&lt;p&gt;Total angular momentum &lt;strong&gt;L&lt;/strong&gt; = 0 (no rotation):&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;L = Σᵢ(rᵢ × mᵢvᵢ) + Iω = 0
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;f-3-4-landing-phase-τ-0-65-1-0&#34;&gt;F.3.4 Landing Phase (τ ∈ [0.65, 1.0])&lt;/h3&gt;

&lt;h4 id=&#34;impact-absorption-strategy&#34;&gt;Impact Absorption Strategy&lt;/h4&gt;

&lt;p&gt;Joint stiffness schedule:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def landing_stiffness(τ_land):
    # Progressive stiffening
    k_base = 500  # N·m/rad
    k_factor = 1 + 2 * (τ_land - 0.65) / 0.35
    
    k_joints = {
        &#39;ankle&#39;: k_base * k_factor * 1.2,
        &#39;knee&#39;: k_base * k_factor * 1.0,
        &#39;hip&#39;: k_base * k_factor * 0.8
    }
    return k_joints
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;f-4-control-law-implementation&#34;&gt;F.4 Control Law Implementation&lt;/h2&gt;

&lt;h3 id=&#34;f-4-1-pd-controller-with-feedforward&#34;&gt;F.4.1 PD Controller with Feedforward&lt;/h3&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def ideal_action(s, τ):
    # Extract current state
    q_current, q̇_current = s.joints, s.velocities
    
    # Get phase-appropriate targets
    q_target = get_phase_targets(τ)
    q̇_target = get_phase_velocities(τ)
    
    # PD control law
    K_p = get_phase_gains_p(τ)
    K_d = get_phase_gains_d(τ)
    
    # Feedforward term
    τ_ff = get_feedforward_torques(τ)
    
    # Combined control
    τ_control = K_p @ (q_target - q_current) + \
                K_d @ (q̇_target - q̇_current) + \
                τ_ff
                
    return clip_torques(τ_control, τ_max)
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;f-4-2-phase-transition-conditions&#34;&gt;F.4.2 Phase Transition Conditions&lt;/h3&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def check_phase_transition(s, τ):
    if τ &amp;lt; 0.2:  # Preparation
        return s.com_velocity[2] &amp;lt; -0.1
    elif τ &amp;lt; 0.35:  # Launch
        return s.ground_contact and s.com_velocity[2] &amp;gt; 2.0
    elif τ &amp;lt; 0.65:  # Aerial
        return not s.ground_contact
    else:  # Landing
        return s.ground_contact and abs(s.com_velocity[2]) &amp;lt; 0.1
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;f-5-validation-results&#34;&gt;F.5 Validation Results&lt;/h2&gt;

&lt;h3 id=&#34;f-5-1-simulated-performance&#34;&gt;F.5.1 Simulated Performance&lt;/h3&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Metric&lt;/th&gt;
&lt;th&gt;Ideal&lt;/th&gt;
&lt;th&gt;Achieved&lt;/th&gt;
&lt;th&gt;Error&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Jump Height&lt;/td&gt;
&lt;td&gt;0.50m&lt;/td&gt;
&lt;td&gt;0.48m&lt;/td&gt;
&lt;td&gt;4%&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;Flight Time&lt;/td&gt;
&lt;td&gt;0.64s&lt;/td&gt;
&lt;td&gt;0.62s&lt;/td&gt;
&lt;td&gt;3%&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;Star Spread&lt;/td&gt;
&lt;td&gt;1.8m&lt;/td&gt;
&lt;td&gt;1.75m&lt;/td&gt;
&lt;td&gt;2.8%&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;Landing Stability&lt;/td&gt;
&lt;td&gt;&amp;lt;5°&lt;/td&gt;
&lt;td&gt;3.2°&lt;/td&gt;
&lt;td&gt;✓&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;h3 id=&#34;f-5-2-energy-efficiency&#34;&gt;F.5.2 Energy Efficiency&lt;/h3&gt;

&lt;p&gt;Total energy expenditure:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;E_total = E_kinetic + E_potential + E_internal
        = 245J + 196J + 87J
        = 528J
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Efficiency compared to simplified model: 78%&lt;/p&gt;

&lt;h2 id=&#34;f-6-implementation-notes&#34;&gt;F.6 Implementation Notes&lt;/h2&gt;

&lt;h3 id=&#34;f-6-1-numerical-stability&#34;&gt;F.6.1 Numerical Stability&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;Use quaternions for orientation to avoid gimbal lock&lt;/li&gt;
&lt;li&gt;Apply low-pass filtering to joint velocities&lt;/li&gt;
&lt;li&gt;Implement safety limits on all joint angles&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;f-6-2-real-time-considerations&#34;&gt;F.6.2 Real-time Considerations&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;Pre-compute phase trajectories&lt;/li&gt;
&lt;li&gt;Use lookup tables for trigonometric functions&lt;/li&gt;
&lt;li&gt;Parallelize joint calculations when possible&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;f-6-3-robustness-enhancements&#34;&gt;F.6.3 Robustness Enhancements&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;Add compliance terms for unexpected disturbances&lt;/li&gt;
&lt;li&gt;Implement recovery strategies for off-nominal conditions&lt;/li&gt;
&lt;li&gt;Include proprioceptive feedback integration&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Chapter 6: Latent Dynamics Models for Character Animation</title>
      <link>https://vihanga.github.io/thesis/chapters/latent-dynamics/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://vihanga.github.io/thesis/chapters/latent-dynamics/</guid>
      <description>

&lt;h1 id=&#34;chapter-6-latent-dynamics-models-for-character-animation&#34;&gt;Chapter 6: Latent Dynamics Models for Character Animation&lt;/h1&gt;

&lt;p&gt;This chapter presents methods for learning compact latent representations of character dynamics, enabling more efficient learning and control of complex animation behaviors through dimensionality reduction and structured latent spaces.&lt;/p&gt;

&lt;h2 id=&#34;6-1-introduction&#34;&gt;6.1 Introduction&lt;/h2&gt;

&lt;p&gt;[Content placeholder: Introduce the concept of latent dynamics models and their benefits for character animation, including improved generalization and computational efficiency]&lt;/p&gt;

&lt;h2 id=&#34;6-2-learning-latent-representations&#34;&gt;6.2 Learning Latent Representations&lt;/h2&gt;

&lt;p&gt;This section describes methods for learning meaningful latent representations of character states and dynamics.&lt;/p&gt;

&lt;h3 id=&#34;6-2-1-variational-autoencoders-for-motion&#34;&gt;6.2.1 Variational Autoencoders for Motion&lt;/h3&gt;

&lt;p&gt;[Content placeholder: Present VAE-based approaches for learning latent representations of character poses and motions]&lt;/p&gt;

&lt;h3 id=&#34;6-2-2-dynamics-in-latent-space&#34;&gt;6.2.2 Dynamics in Latent Space&lt;/h3&gt;

&lt;p&gt;[Content placeholder: Describe methods for learning dynamics models that operate in the learned latent space rather than the full state space]&lt;/p&gt;

&lt;h3 id=&#34;6-2-3-structured-latent-spaces&#34;&gt;6.2.3 Structured Latent Spaces&lt;/h3&gt;

&lt;p&gt;[Content placeholder: Discuss techniques for imposing structure on latent spaces to improve interpretability and control]&lt;/p&gt;

&lt;h2 id=&#34;6-3-control-in-latent-space&#34;&gt;6.3 Control in Latent Space&lt;/h2&gt;

&lt;p&gt;This section presents methods for character control that operate in the learned latent representations.&lt;/p&gt;

&lt;h3 id=&#34;6-3-1-latent-policy-learning&#34;&gt;6.3.1 Latent Policy Learning&lt;/h3&gt;

&lt;p&gt;[Content placeholder: Describe reinforcement learning algorithms that learn policies in latent space]&lt;/p&gt;

&lt;h3 id=&#34;6-3-2-hierarchical-control-architectures&#34;&gt;6.3.2 Hierarchical Control Architectures&lt;/h3&gt;

&lt;p&gt;[Content placeholder: Present hierarchical control methods that use latent representations at different levels of abstraction]&lt;/p&gt;

&lt;h3 id=&#34;6-3-3-transfer-and-generalization&#34;&gt;6.3.3 Transfer and Generalization&lt;/h3&gt;

&lt;p&gt;[Content placeholder: Discuss how latent representations enable better transfer learning and generalization across different characters and tasks]&lt;/p&gt;

&lt;h2 id=&#34;6-4-applications-to-complex-behaviors&#34;&gt;6.4 Applications to Complex Behaviors&lt;/h2&gt;

&lt;h3 id=&#34;6-4-1-multi-character-coordination&#34;&gt;6.4.1 Multi-character Coordination&lt;/h3&gt;

&lt;p&gt;[Content placeholder: Demonstrate applications to coordinating multiple characters using shared latent representations]&lt;/p&gt;

&lt;h3 id=&#34;6-4-2-long-horizon-planning&#34;&gt;6.4.2 Long-horizon Planning&lt;/h3&gt;

&lt;p&gt;[Content placeholder: Show how latent dynamics models enable efficient long-horizon planning for complex animation sequences]&lt;/p&gt;

&lt;h3 id=&#34;6-4-3-interactive-character-control&#34;&gt;6.4.3 Interactive Character Control&lt;/h3&gt;

&lt;p&gt;[Content placeholder: Present applications to real-time interactive control using latent space representations]&lt;/p&gt;

&lt;h2 id=&#34;6-5-experimental-results&#34;&gt;6.5 Experimental Results&lt;/h2&gt;

&lt;h3 id=&#34;6-5-1-representation-quality&#34;&gt;6.5.1 Representation Quality&lt;/h3&gt;

&lt;p&gt;[Content placeholder: Evaluate the quality of learned latent representations through reconstruction accuracy and semantic meaningfulness]&lt;/p&gt;

&lt;h3 id=&#34;6-5-2-control-performance&#34;&gt;6.5.2 Control Performance&lt;/h3&gt;

&lt;p&gt;[Content placeholder: Compare control performance using latent dynamics models versus full-state approaches]&lt;/p&gt;

&lt;h3 id=&#34;6-5-3-computational-efficiency&#34;&gt;6.5.3 Computational Efficiency&lt;/h3&gt;

&lt;p&gt;[Content placeholder: Analyze the computational benefits of operating in latent space for both learning and inference]&lt;/p&gt;

&lt;h2 id=&#34;6-6-chapter-summary&#34;&gt;6.6 Chapter Summary&lt;/h2&gt;

&lt;p&gt;This chapter has presented a comprehensive framework for learning and utilizing latent dynamics models in character animation. By operating in learned latent spaces, we can achieve more efficient learning, better generalization, and more intuitive control of complex character behaviors.&lt;/p&gt;

&lt;p&gt;Key contributions of this chapter include:
- Novel architectures for learning structured latent representations of character dynamics
- Efficient control algorithms that operate in latent space
- Demonstration of improved sample efficiency and generalization
- Applications to challenging animation problems including multi-character coordination and long-horizon planning&lt;/p&gt;

&lt;p&gt;The latent dynamics approach developed in this chapter provides a foundation for scaling character animation systems to more complex behaviors and environments while maintaining computational tractability and control quality.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Appendix G: RLAnimate Directory</title>
      <link>https://vihanga.github.io/thesis/appendices/g/</link>
      <pubDate>Wed, 22 Jan 2025 00:00:00 +0000</pubDate>
      
      <guid>https://vihanga.github.io/thesis/appendices/g/</guid>
      <description>

&lt;h1 id=&#34;appendix-g-rlanimate-directory&#34;&gt;Appendix G: RLAnimate Directory&lt;/h1&gt;

&lt;p&gt;This appendix provides a comprehensive directory of the RLAnimate framework, including its architecture, components, API reference, and usage examples.&lt;/p&gt;

&lt;h2 id=&#34;g-1-system-architecture&#34;&gt;G.1 System Architecture&lt;/h2&gt;

&lt;h3 id=&#34;g-1-1-overview&#34;&gt;G.1.1 Overview&lt;/h3&gt;

&lt;p&gt;RLAnimate is a modular framework for physics-based character animation using reinforcement learning. The system consists of:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;RLAnimate/
├── core/
│   ├── agents/
│   ├── environments/
│   ├── physics/
│   └── utils/
├── models/
│   ├── policies/
│   ├── value_functions/
│   └── networks/
├── training/
│   ├── algorithms/
│   ├── replay_buffers/
│   └── schedulers/
├── evaluation/
│   ├── metrics/
│   ├── visualization/
│   └── benchmarks/
└── examples/
    ├── tutorials/
    ├── experiments/
    └── demos/
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;g-1-2-core-components&#34;&gt;G.1.2 Core Components&lt;/h3&gt;

&lt;h4 id=&#34;physics-engine-interface&#34;&gt;Physics Engine Interface&lt;/h4&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;class PhysicsInterface:
    &amp;quot;&amp;quot;&amp;quot;Abstract interface for physics simulation backends&amp;quot;&amp;quot;&amp;quot;
    
    def step(self, actions: np.ndarray) -&amp;gt; Tuple[State, float, bool, dict]:
        &amp;quot;&amp;quot;&amp;quot;Advance simulation by one timestep&amp;quot;&amp;quot;&amp;quot;
        
    def reset(self) -&amp;gt; State:
        &amp;quot;&amp;quot;&amp;quot;Reset simulation to initial state&amp;quot;&amp;quot;&amp;quot;
        
    def get_state(self) -&amp;gt; State:
        &amp;quot;&amp;quot;&amp;quot;Get current simulation state&amp;quot;&amp;quot;&amp;quot;
        
    def set_state(self, state: State) -&amp;gt; None:
        &amp;quot;&amp;quot;&amp;quot;Set simulation state&amp;quot;&amp;quot;&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;h4 id=&#34;character-model&#34;&gt;Character Model&lt;/h4&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;class Character:
    &amp;quot;&amp;quot;&amp;quot;Articulated character representation&amp;quot;&amp;quot;&amp;quot;
    
    properties = {
        &#39;num_joints&#39;: 21,
        &#39;num_dof&#39;: 63,
        &#39;mass&#39;: 70.0,  # kg
        &#39;height&#39;: 1.75  # m
    }
    
    joints = [
        &#39;pelvis&#39;, &#39;spine&#39;, &#39;chest&#39;, &#39;neck&#39;, &#39;head&#39;,
        &#39;left_shoulder&#39;, &#39;left_elbow&#39;, &#39;left_wrist&#39;,
        &#39;right_shoulder&#39;, &#39;right_elbow&#39;, &#39;right_wrist&#39;,
        &#39;left_hip&#39;, &#39;left_knee&#39;, &#39;left_ankle&#39;,
        &#39;right_hip&#39;, &#39;right_knee&#39;, &#39;right_ankle&#39;
    ]
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;g-2-api-reference&#34;&gt;G.2 API Reference&lt;/h2&gt;

&lt;h3 id=&#34;g-2-1-environment-api&#34;&gt;G.2.1 Environment API&lt;/h3&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;class RLAnimateEnv(gym.Env):
    &amp;quot;&amp;quot;&amp;quot;Base environment for character animation tasks&amp;quot;&amp;quot;&amp;quot;
    
    def __init__(self, config: Dict[str, Any]):
        &amp;quot;&amp;quot;&amp;quot;
        Args:
            config: Environment configuration dictionary
        &amp;quot;&amp;quot;&amp;quot;
        
    def step(self, action: np.ndarray) -&amp;gt; Tuple[np.ndarray, float, bool, dict]:
        &amp;quot;&amp;quot;&amp;quot;
        Execute action and return step information
        
        Args:
            action: Joint torques or target positions
            
        Returns:
            observation: Current state observation
            reward: Step reward
            done: Episode termination flag
            info: Additional information
        &amp;quot;&amp;quot;&amp;quot;
        
    def reset(self) -&amp;gt; np.ndarray:
        &amp;quot;&amp;quot;&amp;quot;Reset environment to initial state&amp;quot;&amp;quot;&amp;quot;
        
    def render(self, mode: str = &#39;human&#39;) -&amp;gt; Optional[np.ndarray]:
        &amp;quot;&amp;quot;&amp;quot;Render current state&amp;quot;&amp;quot;&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;g-2-2-policy-api&#34;&gt;G.2.2 Policy API&lt;/h3&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;class Policy(nn.Module):
    &amp;quot;&amp;quot;&amp;quot;Base policy network class&amp;quot;&amp;quot;&amp;quot;
    
    def forward(self, obs: torch.Tensor) -&amp;gt; Distribution:
        &amp;quot;&amp;quot;&amp;quot;
        Compute action distribution
        
        Args:
            obs: Observation tensor
            
        Returns:
            Action distribution (Normal or Categorical)
        &amp;quot;&amp;quot;&amp;quot;
        
    def get_action(self, obs: torch.Tensor, 
                   deterministic: bool = False) -&amp;gt; torch.Tensor:
        &amp;quot;&amp;quot;&amp;quot;Sample action from policy&amp;quot;&amp;quot;&amp;quot;
        
    def evaluate_actions(self, obs: torch.Tensor, 
                        actions: torch.Tensor) -&amp;gt; Tuple[torch.Tensor, torch.Tensor]:
        &amp;quot;&amp;quot;&amp;quot;Evaluate log probability and entropy of actions&amp;quot;&amp;quot;&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;g-2-3-training-api&#34;&gt;G.2.3 Training API&lt;/h3&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;class Trainer:
    &amp;quot;&amp;quot;&amp;quot;Main training orchestrator&amp;quot;&amp;quot;&amp;quot;
    
    def __init__(self, 
                 env: RLAnimateEnv,
                 policy: Policy,
                 algorithm: str = &#39;ppo&#39;,
                 config: Dict[str, Any] = None):
        &amp;quot;&amp;quot;&amp;quot;Initialize trainer with environment and policy&amp;quot;&amp;quot;&amp;quot;
        
    def train(self, 
              total_timesteps: int,
              callback: Optional[Callable] = None) -&amp;gt; Policy:
        &amp;quot;&amp;quot;&amp;quot;
        Train policy for specified timesteps
        
        Returns:
            Trained policy
        &amp;quot;&amp;quot;&amp;quot;
        
    def save(self, path: str) -&amp;gt; None:
        &amp;quot;&amp;quot;&amp;quot;Save trained model&amp;quot;&amp;quot;&amp;quot;
        
    def load(self, path: str) -&amp;gt; None:
        &amp;quot;&amp;quot;&amp;quot;Load trained model&amp;quot;&amp;quot;&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;g-3-configuration-system&#34;&gt;G.3 Configuration System&lt;/h2&gt;

&lt;h3 id=&#34;g-3-1-environment-configuration&#34;&gt;G.3.1 Environment Configuration&lt;/h3&gt;

&lt;pre&gt;&lt;code class=&#34;language-yaml&#34;&gt;# config/env/locomotion.yaml
env:
  name: &amp;quot;Locomotion-v1&amp;quot;
  physics_backend: &amp;quot;mujoco&amp;quot;
  timestep: 0.01
  max_episode_steps: 1000
  
  observation:
    include_velocities: true
    include_accelerations: false
    history_length: 3
    
  reward:
    weights:
      task: 1.0
      energy: 0.1
      smoothness: 0.05
      balance: 0.2
      
  termination:
    fall_threshold: 0.3
    max_joint_error: 45  # degrees
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;g-3-2-training-configuration&#34;&gt;G.3.2 Training Configuration&lt;/h3&gt;

&lt;pre&gt;&lt;code class=&#34;language-yaml&#34;&gt;# config/train/ppo_default.yaml
algorithm: &amp;quot;ppo&amp;quot;
policy:
  network_type: &amp;quot;mlp&amp;quot;
  hidden_sizes: [256, 256]
  activation: &amp;quot;tanh&amp;quot;
  
training:
  learning_rate: 3e-4
  n_steps: 2048
  batch_size: 64
  n_epochs: 10
  gamma: 0.99
  gae_lambda: 0.95
  clip_range: 0.2
  
  schedule:
    learning_rate: &amp;quot;linear&amp;quot;
    clip_range: &amp;quot;constant&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;g-4-usage-examples&#34;&gt;G.4 Usage Examples&lt;/h2&gt;

&lt;h3 id=&#34;g-4-1-basic-training-script&#34;&gt;G.4.1 Basic Training Script&lt;/h3&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import rlanimate as rla

# Create environment
env = rla.make_env(&#39;Locomotion-v1&#39;)

# Create policy
policy = rla.Policy(
    observation_space=env.observation_space,
    action_space=env.action_space,
    hidden_sizes=[256, 256]
)

# Create trainer
trainer = rla.Trainer(
    env=env,
    policy=policy,
    algorithm=&#39;ppo&#39;,
    config={&#39;learning_rate&#39;: 3e-4}
)

# Train
policy = trainer.train(total_timesteps=1_000_000)

# Save model
trainer.save(&#39;models/locomotion_ppo.zip&#39;)

# Evaluate
mean_reward = rla.evaluate(env, policy, n_episodes=100)
print(f&amp;quot;Mean reward: {mean_reward}&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;g-4-2-custom-task-definition&#34;&gt;G.4.2 Custom Task Definition&lt;/h3&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;class JumpingTask(rla.Task):
    &amp;quot;&amp;quot;&amp;quot;Custom jumping task&amp;quot;&amp;quot;&amp;quot;
    
    def __init__(self, target_height: float = 0.5):
        self.target_height = target_height
        
    def compute_reward(self, state: State, action: np.ndarray) -&amp;gt; float:
        # Height reward
        height_reward = np.exp(-abs(state.com_height - self.target_height))
        
        # Posture reward
        posture_reward = self.compute_posture_reward(state)
        
        # Energy penalty
        energy_penalty = -0.001 * np.sum(action**2)
        
        return height_reward + 0.5 * posture_reward + energy_penalty
        
    def is_success(self, state: State) -&amp;gt; bool:
        return state.max_com_height &amp;gt;= self.target_height * 0.95
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;g-4-3-visualization-script&#34;&gt;G.4.3 Visualization Script&lt;/h3&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import rlanimate.visualization as viz

# Load trained model
env = rla.make_env(&#39;Locomotion-v1&#39;)
policy = rla.load_policy(&#39;models/locomotion_ppo.zip&#39;)

# Create visualizer
visualizer = viz.Visualizer(env, policy)

# Interactive visualization
visualizer.run_interactive()

# Record video
visualizer.record_video(
    &#39;output/locomotion_demo.mp4&#39;,
    n_episodes=5,
    fps=30
)

# Generate trajectory plots
trajectories = visualizer.collect_trajectories(n_episodes=10)
viz.plot_trajectories(trajectories, save_path=&#39;output/trajectories.png&#39;)
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;g-5-extension-guide&#34;&gt;G.5 Extension Guide&lt;/h2&gt;

&lt;h3 id=&#34;g-5-1-adding-new-environments&#34;&gt;G.5.1 Adding New Environments&lt;/h3&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# rlanimate/envs/custom_env.py
class CustomEnv(RLAnimateEnv):
    &amp;quot;&amp;quot;&amp;quot;Template for custom environment&amp;quot;&amp;quot;&amp;quot;
    
    def __init__(self, config):
        super().__init__(config)
        # Initialize custom components
        
    def _compute_reward(self, state, action):
        # Implement reward function
        pass
        
    def _get_observation(self, state):
        # Implement observation extraction
        pass
        
# Register environment
from gym.envs.registration import register
register(
    id=&#39;CustomEnv-v1&#39;,
    entry_point=&#39;rlanimate.envs:CustomEnv&#39;,
)
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;g-5-2-adding-new-algorithms&#34;&gt;G.5.2 Adding New Algorithms&lt;/h3&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# rlanimate/algorithms/custom_algo.py
class CustomAlgorithm(BaseAlgorithm):
    &amp;quot;&amp;quot;&amp;quot;Template for custom RL algorithm&amp;quot;&amp;quot;&amp;quot;
    
    def train_step(self, batch):
        # Implement training step
        pass
        
    def update_policy(self, trajectories):
        # Implement policy update
        pass
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;g-6-performance-benchmarks&#34;&gt;G.6 Performance Benchmarks&lt;/h2&gt;

&lt;h3 id=&#34;g-6-1-training-speed&#34;&gt;G.6.1 Training Speed&lt;/h3&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Algorithm&lt;/th&gt;
&lt;th&gt;Steps/Second&lt;/th&gt;
&lt;th&gt;GPU Memory&lt;/th&gt;
&lt;th&gt;Training Time (1M steps)&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;PPO&lt;/td&gt;
&lt;td&gt;15,000&lt;/td&gt;
&lt;td&gt;2.1 GB&lt;/td&gt;
&lt;td&gt;1.1 hours&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;SAC&lt;/td&gt;
&lt;td&gt;12,000&lt;/td&gt;
&lt;td&gt;2.8 GB&lt;/td&gt;
&lt;td&gt;1.4 hours&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;TD3&lt;/td&gt;
&lt;td&gt;13,500&lt;/td&gt;
&lt;td&gt;2.5 GB&lt;/td&gt;
&lt;td&gt;1.2 hours&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;h3 id=&#34;g-6-2-task-performance&#34;&gt;G.6.2 Task Performance&lt;/h3&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Task&lt;/th&gt;
&lt;th&gt;PPO&lt;/th&gt;
&lt;th&gt;SAC&lt;/th&gt;
&lt;th&gt;TD3&lt;/th&gt;
&lt;th&gt;Human Demo&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Walk&lt;/td&gt;
&lt;td&gt;0.92&lt;/td&gt;
&lt;td&gt;0.94&lt;/td&gt;
&lt;td&gt;0.91&lt;/td&gt;
&lt;td&gt;0.98&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;Run&lt;/td&gt;
&lt;td&gt;0.88&lt;/td&gt;
&lt;td&gt;0.91&lt;/td&gt;
&lt;td&gt;0.89&lt;/td&gt;
&lt;td&gt;0.97&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;Jump&lt;/td&gt;
&lt;td&gt;0.85&lt;/td&gt;
&lt;td&gt;0.82&lt;/td&gt;
&lt;td&gt;0.86&lt;/td&gt;
&lt;td&gt;0.95&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;h2 id=&#34;g-7-troubleshooting&#34;&gt;G.7 Troubleshooting&lt;/h2&gt;

&lt;h3 id=&#34;g-7-1-common-issues&#34;&gt;G.7.1 Common Issues&lt;/h3&gt;

&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;ImportError&lt;/strong&gt;: Ensure all dependencies are installed&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;CUDA Error&lt;/strong&gt;: Check GPU compatibility and drivers&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Memory Error&lt;/strong&gt;: Reduce batch size or model size&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Convergence Issues&lt;/strong&gt;: Adjust learning rate or exploration&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&#34;g-7-2-debug-mode&#34;&gt;G.7.2 Debug Mode&lt;/h3&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Enable debug logging
import logging
logging.basicConfig(level=logging.DEBUG)

# Run with debug flags
env = rla.make_env(&#39;Locomotion-v1&#39;, debug=True)
trainer = rla.Trainer(env, policy, debug=True)
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;g-8-license-and-citation&#34;&gt;G.8 License and Citation&lt;/h2&gt;

&lt;h3 id=&#34;g-8-1-license&#34;&gt;G.8.1 License&lt;/h3&gt;

&lt;p&gt;RLAnimate is released under the MIT License.&lt;/p&gt;

&lt;h3 id=&#34;g-8-2-citation&#34;&gt;G.8.2 Citation&lt;/h3&gt;

&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@software{rlanimate2024,
  title={RLAnimate: A Framework for Physics-based Character Animation},
  author={[Author Names]},
  year={2024},
  url={https://github.com/[username]/rlanimate}
}
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>Chapter 7: Conversational Gesture Generation</title>
      <link>https://vihanga.github.io/thesis/chapters/conversational-gestures/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://vihanga.github.io/thesis/chapters/conversational-gestures/</guid>
      <description>

&lt;h1 id=&#34;chapter-7-conversational-gesture-generation&#34;&gt;Chapter 7: Conversational Gesture Generation&lt;/h1&gt;

&lt;p&gt;This chapter addresses the specific challenge of generating natural conversational gestures that accompany speech, presenting methods for creating embodied conversational agents with realistic nonverbal communication behaviors.&lt;/p&gt;

&lt;h2 id=&#34;7-1-introduction&#34;&gt;7.1 Introduction&lt;/h2&gt;

&lt;p&gt;[Content placeholder: Introduce the importance of conversational gestures in human communication and the challenges of automatic gesture generation for virtual agents]&lt;/p&gt;

&lt;h2 id=&#34;7-2-understanding-conversational-gestures&#34;&gt;7.2 Understanding Conversational Gestures&lt;/h2&gt;

&lt;p&gt;This section provides background on the nature and function of conversational gestures.&lt;/p&gt;

&lt;h3 id=&#34;7-2-1-types-of-conversational-gestures&#34;&gt;7.2.1 Types of Conversational Gestures&lt;/h3&gt;

&lt;p&gt;[Content placeholder: Categorize different types of gestures including beat gestures, iconic gestures, metaphoric gestures, and deictic gestures]&lt;/p&gt;

&lt;h3 id=&#34;7-2-2-speech-gesture-synchrony&#34;&gt;7.2.2 Speech-Gesture Synchrony&lt;/h3&gt;

&lt;p&gt;[Content placeholder: Discuss the temporal relationships between speech and gesture, including principles of synchronization]&lt;/p&gt;

&lt;h3 id=&#34;7-2-3-individual-and-cultural-variations&#34;&gt;7.2.3 Individual and Cultural Variations&lt;/h3&gt;

&lt;p&gt;[Content placeholder: Address variations in gesture patterns across individuals and cultures]&lt;/p&gt;

&lt;h2 id=&#34;7-3-data-driven-gesture-generation&#34;&gt;7.3 Data-Driven Gesture Generation&lt;/h2&gt;

&lt;p&gt;This section presents methods for learning gesture generation models from human data.&lt;/p&gt;

&lt;h3 id=&#34;7-3-1-multimodal-data-collection&#34;&gt;7.3.1 Multimodal Data Collection&lt;/h3&gt;

&lt;p&gt;[Content placeholder: Describe methods for collecting synchronized speech and motion data for training gesture generation models]&lt;/p&gt;

&lt;h3 id=&#34;7-3-2-feature-extraction-and-representation&#34;&gt;7.3.2 Feature Extraction and Representation&lt;/h3&gt;

&lt;p&gt;[Content placeholder: Present techniques for extracting relevant features from speech (prosody, semantics) and motion data]&lt;/p&gt;

&lt;h3 id=&#34;7-3-3-sequence-to-sequence-models&#34;&gt;7.3.3 Sequence-to-Sequence Models&lt;/h3&gt;

&lt;p&gt;[Content placeholder: Describe neural architectures for mapping from speech features to gesture sequences]&lt;/p&gt;

&lt;h2 id=&#34;7-4-reinforcement-learning-for-gesture-adaptation&#34;&gt;7.4 Reinforcement Learning for Gesture Adaptation&lt;/h2&gt;

&lt;p&gt;This section explores how RL can be used to adapt and improve gesture generation based on interaction feedback.&lt;/p&gt;

&lt;h3 id=&#34;7-4-1-interactive-learning-framework&#34;&gt;7.4.1 Interactive Learning Framework&lt;/h3&gt;

&lt;p&gt;[Content placeholder: Present an RL framework for learning gesture policies through interaction with human users]&lt;/p&gt;

&lt;h3 id=&#34;7-4-2-reward-design-for-natural-gestures&#34;&gt;7.4.2 Reward Design for Natural Gestures&lt;/h3&gt;

&lt;p&gt;[Content placeholder: Discuss reward functions that capture naturalness, expressiveness, and communicative effectiveness]&lt;/p&gt;

&lt;h3 id=&#34;7-4-3-online-adaptation&#34;&gt;7.4.3 Online Adaptation&lt;/h3&gt;

&lt;p&gt;[Content placeholder: Describe methods for online adaptation of gesture generation to individual users and contexts]&lt;/p&gt;

&lt;h2 id=&#34;7-5-evaluation-methods&#34;&gt;7.5 Evaluation Methods&lt;/h2&gt;

&lt;h3 id=&#34;7-5-1-objective-metrics&#34;&gt;7.5.1 Objective Metrics&lt;/h3&gt;

&lt;p&gt;[Content placeholder: Define quantitative metrics for evaluating gesture generation including synchrony, diversity, and appropriateness]&lt;/p&gt;

&lt;h3 id=&#34;7-5-2-user-studies&#34;&gt;7.5.2 User Studies&lt;/h3&gt;

&lt;p&gt;[Content placeholder: Present user study methodologies for evaluating the perceived naturalness and effectiveness of generated gestures]&lt;/p&gt;

&lt;h3 id=&#34;7-5-3-comparative-analysis&#34;&gt;7.5.3 Comparative Analysis&lt;/h3&gt;

&lt;p&gt;[Content placeholder: Compare different approaches including rule-based, data-driven, and RL-based methods]&lt;/p&gt;

&lt;h2 id=&#34;7-6-applications-and-case-studies&#34;&gt;7.6 Applications and Case Studies&lt;/h2&gt;

&lt;h3 id=&#34;7-6-1-virtual-assistants&#34;&gt;7.6.1 Virtual Assistants&lt;/h3&gt;

&lt;p&gt;[Content placeholder: Demonstrate applications to embodied virtual assistants with natural gesture behaviors]&lt;/p&gt;

&lt;h3 id=&#34;7-6-2-social-robots&#34;&gt;7.6.2 Social Robots&lt;/h3&gt;

&lt;p&gt;[Content placeholder: Show applications to social robotics where appropriate gesture generation enhances human-robot interaction]&lt;/p&gt;

&lt;h3 id=&#34;7-6-3-virtual-reality-avatars&#34;&gt;7.6.3 Virtual Reality Avatars&lt;/h3&gt;

&lt;p&gt;[Content placeholder: Present applications to VR avatars that need to generate gestures in real-time during conversation]&lt;/p&gt;

&lt;h2 id=&#34;7-7-chapter-summary&#34;&gt;7.7 Chapter Summary&lt;/h2&gt;

&lt;p&gt;This chapter has presented a comprehensive approach to conversational gesture generation, combining insights from human communication research with advanced machine learning techniques. The methods developed enable virtual agents to communicate more naturally and effectively through appropriate nonverbal behaviors.&lt;/p&gt;

&lt;p&gt;Key contributions of this chapter include:
- A framework for learning gesture generation models from multimodal human data
- Novel RL approaches for adapting gesture generation based on interaction feedback
- Comprehensive evaluation methods for assessing gesture quality and effectiveness
- Demonstration of applications across various domains including virtual assistants and social robotics&lt;/p&gt;

&lt;p&gt;The work presented in this chapter advances the state of the art in embodied conversational agents, bringing us closer to virtual characters that can engage in natural, multimodal communication with humans.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Chapter 8: Conclusion</title>
      <link>https://vihanga.github.io/thesis/chapters/conclusion/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://vihanga.github.io/thesis/chapters/conclusion/</guid>
      <description>

&lt;h1 id=&#34;chapter-8-conclusion&#34;&gt;Chapter 8: Conclusion&lt;/h1&gt;

&lt;p&gt;This chapter summarizes the contributions of this thesis, discusses their implications for the field of character animation and reinforcement learning, and outlines directions for future research.&lt;/p&gt;

&lt;h2 id=&#34;8-1-summary-of-contributions&#34;&gt;8.1 Summary of Contributions&lt;/h2&gt;

&lt;p&gt;This thesis has presented a comprehensive exploration of reinforcement learning methods for character animation, addressing fundamental challenges and proposing novel solutions across multiple domains.&lt;/p&gt;

&lt;h3 id=&#34;8-1-1-technical-contributions&#34;&gt;8.1.1 Technical Contributions&lt;/h3&gt;

&lt;p&gt;[Content placeholder: Summarize the main technical contributions including:
- Novel RL algorithms for physics-based animation
- Model-based and model-free approaches for animation control
- Methods for leveraging existing motion data in RL frameworks
- Latent dynamics models for efficient learning and control
- Gesture generation techniques for conversational agents]&lt;/p&gt;

&lt;h3 id=&#34;8-1-2-theoretical-insights&#34;&gt;8.1.2 Theoretical Insights&lt;/h3&gt;

&lt;p&gt;[Content placeholder: Discuss theoretical insights gained about:
- The relationship between physics-based simulation and learning
- Trade-offs between model-based and model-free approaches
- The role of data priors in animation learning
- Connections between human motion principles and RL objectives]&lt;/p&gt;

&lt;h3 id=&#34;8-1-3-practical-applications&#34;&gt;8.1.3 Practical Applications&lt;/h3&gt;

&lt;p&gt;[Content placeholder: Highlight practical applications demonstrated including:
- Interactive character control systems
- Animation tools for content creation
- Embodied conversational agents
- Motion synthesis for games and virtual environments]&lt;/p&gt;

&lt;h2 id=&#34;8-2-impact-and-significance&#34;&gt;8.2 Impact and Significance&lt;/h2&gt;

&lt;h3 id=&#34;8-2-1-advancing-the-state-of-the-art&#34;&gt;8.2.1 Advancing the State of the Art&lt;/h3&gt;

&lt;p&gt;[Content placeholder: Discuss how this work advances the state of the art in character animation, comparing with previous approaches and highlighting improvements]&lt;/p&gt;

&lt;h3 id=&#34;8-2-2-bridging-communities&#34;&gt;8.2.2 Bridging Communities&lt;/h3&gt;

&lt;p&gt;[Content placeholder: Describe how this work bridges the computer graphics and machine learning communities, fostering cross-disciplinary collaboration]&lt;/p&gt;

&lt;h3 id=&#34;8-2-3-enabling-new-applications&#34;&gt;8.2.3 Enabling New Applications&lt;/h3&gt;

&lt;p&gt;[Content placeholder: Discuss new applications enabled by the methods developed in this thesis, from entertainment to robotics]&lt;/p&gt;

&lt;h2 id=&#34;8-3-limitations-and-challenges&#34;&gt;8.3 Limitations and Challenges&lt;/h2&gt;

&lt;h3 id=&#34;8-3-1-computational-requirements&#34;&gt;8.3.1 Computational Requirements&lt;/h3&gt;

&lt;p&gt;[Content placeholder: Acknowledge computational limitations and discuss trade-offs between quality and efficiency]&lt;/p&gt;

&lt;h3 id=&#34;8-3-2-generalization-boundaries&#34;&gt;8.3.2 Generalization Boundaries&lt;/h3&gt;

&lt;p&gt;[Content placeholder: Discuss limitations in generalization across different characters, environments, and tasks]&lt;/p&gt;

&lt;h3 id=&#34;8-3-3-evaluation-challenges&#34;&gt;8.3.3 Evaluation Challenges&lt;/h3&gt;

&lt;p&gt;[Content placeholder: Address the ongoing challenge of objectively evaluating animation quality and naturalness]&lt;/p&gt;

&lt;h2 id=&#34;8-4-future-directions&#34;&gt;8.4 Future Directions&lt;/h2&gt;

&lt;h3 id=&#34;8-4-1-scaling-to-more-complex-behaviors&#34;&gt;8.4.1 Scaling to More Complex Behaviors&lt;/h3&gt;

&lt;p&gt;[Content placeholder: Outline research directions for scaling the methods to more complex behaviors and longer time horizons]&lt;/p&gt;

&lt;h3 id=&#34;8-4-2-multi-agent-and-social-interactions&#34;&gt;8.4.2 Multi-agent and Social Interactions&lt;/h3&gt;

&lt;p&gt;[Content placeholder: Discuss extensions to multi-agent scenarios and social interaction modeling]&lt;/p&gt;

&lt;h3 id=&#34;8-4-3-integration-with-large-language-models&#34;&gt;8.4.3 Integration with Large Language Models&lt;/h3&gt;

&lt;p&gt;[Content placeholder: Explore potential integration with LLMs for high-level behavior specification and control]&lt;/p&gt;

&lt;h3 id=&#34;8-4-4-real-world-deployment&#34;&gt;8.4.4 Real-world Deployment&lt;/h3&gt;

&lt;p&gt;[Content placeholder: Discuss pathways for deploying these methods in real-world applications, from games to robotics]&lt;/p&gt;

&lt;h2 id=&#34;8-5-closing-remarks&#34;&gt;8.5 Closing Remarks&lt;/h2&gt;

&lt;p&gt;The work presented in this thesis represents a significant step forward in creating intelligent, adaptive character animation systems through reinforcement learning. By addressing fundamental challenges in physics-based animation, data integration, and behavioral modeling, we have demonstrated that RL can be a powerful tool for creating lifelike virtual characters.&lt;/p&gt;

&lt;p&gt;The methods and insights developed here open new possibilities for interactive entertainment, virtual reality, robotics, and human-computer interaction. As we continue to push the boundaries of what is possible with learned animation systems, the vision of truly intelligent virtual characters that can adapt, learn, and interact naturally with humans comes ever closer to reality.&lt;/p&gt;

&lt;p&gt;The journey of bringing artificial characters to life through learning is far from complete, but the foundations laid in this thesis provide a solid basis for future innovations. It is my hope that this work will inspire continued research at the intersection of animation and machine learning, ultimately leading to virtual characters that are indistinguishable from their human counterparts in their ability to move, gesture, and express themselves naturally.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
