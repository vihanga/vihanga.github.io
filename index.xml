<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Vihanga Gamage&#39;s Corner of the World Wide Web on Vihanga Gamage&#39;s Corner of the World Wide Web</title>
    <link>https://vihanga.github.io/</link>
    <description>Recent content in Vihanga Gamage&#39;s Corner of the World Wide Web on Vihanga Gamage&#39;s Corner of the World Wide Web</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>&amp;copy; 2018</copyright>
    <lastBuildDate>Wed, 22 Jan 2025 00:00:00 +0000</lastBuildDate>
    <atom:link href="/" rel="self" type="application/rss+xml" />
    
    <item>
      <title>Chapter 1: Introduction</title>
      <link>https://vihanga.github.io/thesis/chapters/introduction/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://vihanga.github.io/thesis/chapters/introduction/</guid>
      <description>

&lt;h1 id=&#34;introduction&#34;&gt;Introduction&lt;/h1&gt;

&lt;p&gt;This thesis tackles a fundamental challenge in computer animation: how can we create virtual characters that move naturally and responsively without the prohibitive costs of motion capture?&lt;/p&gt;

&lt;h2 id=&#34;the-problem&#34;&gt;The Problem&lt;/h2&gt;

&lt;p&gt;Traditional character animation relies heavily on motion capture - an expensive, time-consuming process that requires recording human actors for every possible behavior variation. While this produces high-quality results, it&amp;rsquo;s simply not scalable for interactive applications where characters need to respond dynamically to user input or changing scenarios.&lt;/p&gt;

&lt;p&gt;Meanwhile, existing alternatives have significant limitations. Supervised learning methods can generate animations but lack flexibility. Physics-based reinforcement learning works brilliantly for athletic movements but fails for social behaviors like gestures - there&amp;rsquo;s no physics simulation to tell us if a wave looks &amp;ldquo;friendly&amp;rdquo; or a gesture feels &amp;ldquo;natural.&amp;rdquo;&lt;/p&gt;

&lt;h2 id=&#34;my-approach&#34;&gt;My Approach&lt;/h2&gt;

&lt;p&gt;I developed RLAnimate, a framework that fundamentally rethinks how we approach character animation. Instead of using reward functions (the traditional RL approach), I use motion capture data as learning objectives. This allows agents to learn what &amp;ldquo;human-like&amp;rdquo; means directly from examples, while maintaining the flexibility and responsiveness that makes RL powerful.&lt;/p&gt;

&lt;p&gt;The key insight is treating animation as a model-based reinforcement learning problem where agents learn the dynamics of human movement. By understanding how joint rotations create behaviors, agents can generate novel animations that look natural while adapting to changing requirements in real-time.&lt;/p&gt;

&lt;h2 id=&#34;key-contributions&#34;&gt;Key Contributions&lt;/h2&gt;

&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;A new paradigm for animation RL&lt;/strong&gt; - Using motion capture data as objectives rather than rewards&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Multi-behavior agents&lt;/strong&gt; - Single agents that can wave, point, and generate conversational gestures&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Technical innovations&lt;/strong&gt; - Quaternion neural networks, realism regularization, and hierarchical dynamics models&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Proven results&lt;/strong&gt; - Animations that are statistically indistinguishable from human motion capture&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&#34;research-questions&#34;&gt;Research Questions&lt;/h2&gt;

&lt;p&gt;The thesis systematically addresses four key questions:
- Can RL work for animation without physics? (Yes - through latent dynamics learning)
- Can we make it human-like? (Yes - using motion capture as objectives)
- How do we handle complexity like fingers? (Quaternions + dedicated animation dynamics)
- Can it work for conversational gestures? (Yes - with realism regularization)&lt;/p&gt;

&lt;h2 id=&#34;thesis-journey&#34;&gt;Thesis Journey&lt;/h2&gt;

&lt;p&gt;The work progresses from establishing basic feasibility (Chapter 4) through developing the core RLAnimate methodology (Chapter 5), extending to complex movements with fingers (Chapter 6), and culminating in conversational beat gestures that match human quality (Chapter 7).&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Chapter 2: Virtual Character Animation and Perception</title>
      <link>https://vihanga.github.io/thesis/chapters/data-driven-animation/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://vihanga.github.io/thesis/chapters/data-driven-animation/</guid>
      <description>

&lt;h1 id=&#34;virtual-character-animation-and-perception&#34;&gt;Virtual Character Animation and Perception&lt;/h1&gt;

&lt;p&gt;This chapter establishes the foundation for understanding virtual character animation - both the technical challenges and the perceptual requirements that make animation &amp;ldquo;feel right&amp;rdquo; to human viewers.&lt;/p&gt;

&lt;h2 id=&#34;the-animation-pipeline&#34;&gt;The Animation Pipeline&lt;/h2&gt;

&lt;p&gt;Creating believable virtual characters isn&amp;rsquo;t just about making them move - it&amp;rsquo;s a complex pipeline that starts with 3D modeling and ends with real-time rendering. I explore how animation data flows through this pipeline, from motion capture or procedural generation to the final pixels on screen. The key insight? Each step introduces opportunities for both quality improvements and computational optimizations.&lt;/p&gt;

&lt;h2 id=&#34;why-perception-matters&#34;&gt;Why Perception Matters&lt;/h2&gt;

&lt;p&gt;Here&amp;rsquo;s what most animation systems miss: humans are incredibly sensitive to movement patterns. We can spot &amp;ldquo;unnatural&amp;rdquo; motion in milliseconds, even if we can&amp;rsquo;t articulate why. I dive into the perceptual studies that reveal what makes animation feel human-like - from timing and smoothness to the subtle coordination between body parts.&lt;/p&gt;

&lt;p&gt;This understanding drives a crucial design decision in my work: animation quality isn&amp;rsquo;t just about technical metrics like joint angles or velocities. It&amp;rsquo;s about matching the statistical patterns of human movement that our brains have evolved to recognize.&lt;/p&gt;

&lt;h2 id=&#34;current-approaches-and-their-limits&#34;&gt;Current Approaches and Their Limits&lt;/h2&gt;

&lt;p&gt;The field has tried three main approaches:&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Motion Graphs and Blending&lt;/strong&gt;: Works well for predetermined scenarios but lacks flexibility. You need exponentially more data as behaviors become more complex.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Neural Networks&lt;/strong&gt;: Can generate smooth animations but often produce &amp;ldquo;average&amp;rdquo; motions that lack character and specificity. They struggle with rare behaviors and edge cases.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Physics-based RL&lt;/strong&gt;: Produces incredibly athletic movements - backflips, martial arts, parkour. But try to make a physics-based agent wave &amp;ldquo;friendly&amp;rdquo; and you&amp;rsquo;ll see the problem. There&amp;rsquo;s no physics equation for social appropriateness.&lt;/p&gt;

&lt;h2 id=&#34;the-gap-this-thesis-fills&#34;&gt;The Gap This Thesis Fills&lt;/h2&gt;

&lt;p&gt;What&amp;rsquo;s missing is a method that combines the flexibility of RL with the quality of motion capture data, without requiring physics simulation. That&amp;rsquo;s exactly what RLAnimate provides - and understanding this background shows why that combination is so powerful.&lt;/p&gt;

&lt;p&gt;The chapter sets up the key tension: we want animation that&amp;rsquo;s both high-quality (matching human perception) and flexible (responding to dynamic scenarios). Traditional methods excel at one or the other, but not both. This creates the perfect motivation for the model-based RL approach I develop in the following chapters.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Chapter 3: Model-based Reinforcement Learning</title>
      <link>https://vihanga.github.io/thesis/chapters/model-based-rl/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://vihanga.github.io/thesis/chapters/model-based-rl/</guid>
      <description>

&lt;h1 id=&#34;model-based-reinforcement-learning&#34;&gt;Model-based Reinforcement Learning&lt;/h1&gt;

&lt;p&gt;This chapter provides the theoretical foundation for understanding how model-based RL can revolutionize character animation. I explain why learning a model of the world enables more sample-efficient and generalizable agents compared to traditional approaches.&lt;/p&gt;

&lt;h2 id=&#34;the-power-of-world-models&#34;&gt;The Power of World Models&lt;/h2&gt;

&lt;p&gt;Most RL systems are model-free - they learn through trial and error what actions lead to good outcomes. That works great if you have millions of attempts to practice. But what if you need to learn from limited data? That&amp;rsquo;s where model-based RL shines.&lt;/p&gt;

&lt;p&gt;By learning a model of how the world works - in our case, how human bodies move - agents can plan ahead, imagine consequences, and generalize from limited examples. It&amp;rsquo;s the difference between memorizing dance moves and understanding the principles of movement.&lt;/p&gt;

&lt;h2 id=&#34;latent-dynamics-the-secret-sauce&#34;&gt;Latent Dynamics: The Secret Sauce&lt;/h2&gt;

&lt;p&gt;Here&amp;rsquo;s the key insight that makes RLAnimate work: we don&amp;rsquo;t need to model every muscle and bone. Instead, we learn compact &amp;ldquo;latent&amp;rdquo; representations that capture the essence of movement patterns.&lt;/p&gt;

&lt;p&gt;I introduce the mathematical framework of latent dynamics models, showing how they compress high-dimensional movement data into manageable representations while preserving the critical information for generating natural motion. This is what allows our agents to run at 5ms per frame - we&amp;rsquo;re working in an efficient latent space, not raw joint angles.&lt;/p&gt;

&lt;h2 id=&#34;representation-learning-and-realism&#34;&gt;Representation Learning and Realism&lt;/h2&gt;

&lt;p&gt;The chapter explores how to learn good representations for animation. Not all latent spaces are created equal - some preserve the nuances that make motion feel human, others lose them. I examine techniques from variational autoencoders to adversarial learning, setting up the theoretical foundation for the realism regularization used in later chapters.&lt;/p&gt;

&lt;h2 id=&#34;why-model-based-for-animation&#34;&gt;Why Model-based for Animation?&lt;/h2&gt;

&lt;p&gt;The chapter concludes by connecting these abstract concepts to character animation:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Sample Efficiency&lt;/strong&gt;: Learn new behaviors from just a few motion capture examples&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Generalization&lt;/strong&gt;: Interpolate and extrapolate beyond the training data&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Planning&lt;/strong&gt;: Generate coherent long-term motion sequences&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Speed&lt;/strong&gt;: Efficient latent representations enable real-time performance&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;This theoretical grounding is essential for understanding why RLAnimate succeeds where other methods fail. Model-based RL isn&amp;rsquo;t just another technique - it&amp;rsquo;s fundamentally the right abstraction for the animation problem.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Chapter 4: Model-based Character Animation</title>
      <link>https://vihanga.github.io/thesis/chapters/model-based-animation/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://vihanga.github.io/thesis/chapters/model-based-animation/</guid>
      <description>

&lt;h1 id=&#34;model-based-character-animation&#34;&gt;Model-based Character Animation&lt;/h1&gt;

&lt;p&gt;This chapter presents exploratory work establishing the feasibility of applying reinforcement learning to dynamic character animation tasks without reliance on physics simulation. The work addresses a fundamental research question: can reinforcement learning effectively control character animation for portraying social behaviours?&lt;/p&gt;

&lt;h2 id=&#34;motivation-and-objectives&#34;&gt;Motivation and Objectives&lt;/h2&gt;

&lt;p&gt;Previous work in RL-based animation has relied heavily on physics signals for training, rendering such approaches unsuitable for social behaviours where physics feedback is absent or irrelevant. This chapter establishes a methodology for training agents that achieve three key objectives:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Agent functionality relies only on signals relevant to all animation sequences, regardless of the behaviour being portrayed&lt;/li&gt;
&lt;li&gt;Multiple types of behaviours can be portrayed by a single agent instance using shared dynamics models&lt;/li&gt;
&lt;li&gt;Animation generation occurs dynamically on a frame-by-frame basis, enabling real-time flexibility&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;experimental-approach&#34;&gt;Experimental Approach&lt;/h2&gt;

&lt;p&gt;The investigation begins with model-free reinforcement learning experiments, following the precedent set by physics-based methods. These initial experiments reveal limitations that motivate the exploration of model-based approaches, which offer the robust sample efficiency demonstrated in supervised learning applications for character animation.&lt;/p&gt;

&lt;p&gt;The experiments utilise star jump animations as a test case - a behaviour that requires coordination across multiple joints while remaining sufficiently constrained for systematic analysis. This choice enables focused investigation of the core technical challenges without the confounding factors present in more complex social behaviours.&lt;/p&gt;

&lt;h2 id=&#34;model-based-learning-results&#34;&gt;Model-based Learning Results&lt;/h2&gt;

&lt;p&gt;The model-based approach demonstrates several advantages over model-free alternatives:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Sample efficiency&lt;/strong&gt;: Effective learning from limited motion capture data (5 minutes)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Motion quality&lt;/strong&gt;: Generated animations maintain temporal coherence and naturalness&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Computational efficiency&lt;/strong&gt;: Real-time performance with inference times under 5ms per frame&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Implicit regularisation&lt;/strong&gt;: The learned dynamics model constrains generated motion to remain within the manifold of human movement&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;key-contributions&#34;&gt;Key Contributions&lt;/h2&gt;

&lt;p&gt;This chapter establishes that model-based reinforcement learning provides a viable paradigm for character animation without physics simulation. The learned latent dynamics models effectively capture movement patterns from motion capture data, enabling agents to generate novel animations while maintaining human-like qualities.&lt;/p&gt;

&lt;p&gt;The primary contribution is the establishment of a basic framework that will be extended in subsequent chapters. This work was published in &amp;ldquo;Learned Dynamics Models and Online Planning for Model-Based Animation Agents&amp;rdquo; at the KES Agents and Multi-Agent Systems 2021 conference.&lt;/p&gt;

&lt;h2 id=&#34;limitations-and-future-directions&#34;&gt;Limitations and Future Directions&lt;/h2&gt;

&lt;p&gt;While successful within its scope, this initial work reveals several limitations that motivate subsequent developments:
- Single-behaviour agents lack the flexibility required for interactive applications
- Absence of interaction with physical environments limits applicability
- The framework requires extension to handle more complex social behaviours&lt;/p&gt;

&lt;p&gt;These limitations directly inform the development of RLAnimate in Chapter 5, which addresses these constraints through a more sophisticated approach to behaviour representation and multi-task learning.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Chapter 5: RLAnimate - Data-driven RL for Character Animation</title>
      <link>https://vihanga.github.io/thesis/chapters/data-driven-rl/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://vihanga.github.io/thesis/chapters/data-driven-rl/</guid>
      <description>

&lt;h1 id=&#34;rlanimate-data-driven-reinforcement-learning-for-character-animation&#34;&gt;RLAnimate: Data-driven Reinforcement Learning for Character Animation&lt;/h1&gt;

&lt;p&gt;This chapter introduces RLAnimate, a novel framework that fundamentally reconceptualises reinforcement learning for character animation. Rather than relying on hand-crafted reward functions, RLAnimate utilises motion capture data as learning objectives, addressing the challenge of articulating human-like movement mathematically.&lt;/p&gt;

&lt;h2 id=&#34;theoretical-foundation&#34;&gt;Theoretical Foundation&lt;/h2&gt;

&lt;p&gt;The central insight of RLAnimate lies in reformulating the animation problem: instead of defining rewards for desired outcomes, the framework learns to match the dynamics of human movement. This approach acknowledges that while defining &amp;ldquo;wave friendly&amp;rdquo; or &amp;ldquo;point naturally&amp;rdquo; through equations proves intractable, motion capture data implicitly contains this information.&lt;/p&gt;

&lt;p&gt;The framework addresses the research question: &amp;ldquo;To what extent can a RL-based algorithm for animation control be trained to generate human-like social behaviours?&amp;rdquo; The answer, as demonstrated through extensive evaluation, is that data-driven objectives enable generation of animations that match human motion capture in both quantitative metrics and perceptual studies.&lt;/p&gt;

&lt;h2 id=&#34;the-a1-architecture&#34;&gt;The A1 Architecture&lt;/h2&gt;

&lt;p&gt;The initial RLAnimate architecture (A1) establishes the core components:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Behaviour encoders&lt;/strong&gt;: Learn representations of intended behaviours from motion capture exemplars&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Perceptual encoders&lt;/strong&gt;: Process current character state into task-relevant features&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Latent dynamics models&lt;/strong&gt;: Capture temporal evolution of human movement patterns&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Multi-behaviour support&lt;/strong&gt;: Enable single agents to perform multiple distinct behaviours&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;This architecture demonstrates that motion capture data can serve as sufficient supervision for learning complex animation controllers without explicit reward engineering.&lt;/p&gt;

&lt;h2 id=&#34;multi-behaviour-learning&#34;&gt;Multi-Behaviour Learning&lt;/h2&gt;

&lt;p&gt;A key contribution is the framework&amp;rsquo;s ability to train single agents capable of multiple behaviours. This contrasts with previous approaches requiring separate models for each behaviour type. The shared latent dynamics capture common principles of human movement, while behaviour-specific encoders provide the necessary specialisation.&lt;/p&gt;

&lt;p&gt;Empirical results demonstrate:
- &lt;strong&gt;93% behaviour accuracy&lt;/strong&gt;: Agents correctly perform requested behaviours
- &lt;strong&gt;5ms inference time&lt;/strong&gt;: Suitable for real-time interactive applications
- &lt;strong&gt;Statistical motion quality&lt;/strong&gt;: Generated animations match human motion capture statistics
- &lt;strong&gt;Data efficiency&lt;/strong&gt;: Effective learning from minutes rather than hours of motion capture&lt;/p&gt;

&lt;h2 id=&#34;the-a2-architecture-physical-grounding&#34;&gt;The A2 Architecture: Physical Grounding&lt;/h2&gt;

&lt;p&gt;The A2 variant introduces physical grounding while maintaining the data-driven approach:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Contact modelling&lt;/strong&gt;: Explicit representation of foot-ground contacts&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Momentum consistency&lt;/strong&gt;: Conservation principles integrated into dynamics learning&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Object interaction&lt;/strong&gt;: Framework for manipulating virtual objects&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Maintained ease of training&lt;/strong&gt;: Physical constraints learned from data rather than hard-coded&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;methodological-contributions&#34;&gt;Methodological Contributions&lt;/h2&gt;

&lt;p&gt;RLAnimate contributes several methodological innovations:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Idealness formulation&lt;/strong&gt;: Motion capture defines ideal actions at each timestep, eliminating reward function design&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Latent planning&lt;/strong&gt;: Efficient trajectory optimisation in learned representation spaces&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Behaviour composability&lt;/strong&gt;: Architecture supporting seamless transitions between behaviours&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Sample efficiency&lt;/strong&gt;: Orders of magnitude reduction in required training data compared to model-free approaches&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&#34;publications-and-impact&#34;&gt;Publications and Impact&lt;/h2&gt;

&lt;p&gt;The work presented in this chapter resulted in two peer-reviewed publications:
- &amp;ldquo;Data-driven reinforcement learning for virtual character animation control&amp;rdquo; (ALA Workshop, AAMAS 2021)
- &amp;ldquo;Latent Dynamics for Artefact-Free Character Animation via Data-Driven Reinforcement Learning&amp;rdquo; (ICANN 2021)&lt;/p&gt;

&lt;h2 id=&#34;significance&#34;&gt;Significance&lt;/h2&gt;

&lt;p&gt;RLAnimate establishes a new paradigm for animation synthesis, demonstrating that reinforcement learning can produce human-quality animation without physics simulation or reward engineering. By treating animation as matching human movement dynamics rather than optimising explicit objectives, the framework achieves:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Natural quality without manual tuning&lt;/li&gt;
&lt;li&gt;Multi-behaviour flexibility in single agents&lt;/li&gt;
&lt;li&gt;Practical training requirements&lt;/li&gt;
&lt;li&gt;Real-time performance suitable for deployment&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;This foundation enables the subsequent developments in finger animation (Chapter 6) and conversational gestures (Chapter 7), validating the extensibility and robustness of the data-driven approach.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Chapter 6: Quaternions and Finger Animation</title>
      <link>https://vihanga.github.io/thesis/chapters/latent-dynamics/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://vihanga.github.io/thesis/chapters/latent-dynamics/</guid>
      <description>

&lt;h1 id=&#34;quaternions-and-finger-animation&#34;&gt;Quaternions and Finger Animation&lt;/h1&gt;

&lt;p&gt;This chapter addresses the challenges of extending RLAnimate to accommodate fine-grained articulations, specifically finger animation. The inclusion of an additional thirty joints for finger control revealed limitations in the original architecture, necessitating fundamental modifications to both the representation and dynamics modelling approaches.&lt;/p&gt;

&lt;h2 id=&#34;technical-challenges-in-finger-animation&#34;&gt;Technical Challenges in Finger Animation&lt;/h2&gt;

&lt;p&gt;The incorporation of finger movements presents unique challenges for animation synthesis:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Increased dimensionality&lt;/strong&gt;: Addition of 30+ joints substantially expands the state and action spaces&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Hierarchical dependencies&lt;/strong&gt;: Finger movements operate within the broader kinematic structure of arm and torso motion&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Subtle articulations&lt;/strong&gt;: Small inaccuracies in finger positions produce visually apparent artefacts&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Representational singularities&lt;/strong&gt;: Euler angle representations suffer from gimbal lock, particularly problematic for the complex rotations required in hand animation&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;These challenges necessitated architectural innovations to maintain animation quality while managing the increased complexity.&lt;/p&gt;

&lt;h2 id=&#34;quaternion-representations-for-robust-animation&#34;&gt;Quaternion Representations for Robust Animation&lt;/h2&gt;

&lt;p&gt;The transition from Euler angles to quaternion representations addresses fundamental limitations in rotation parameterisation:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Singularity-free representation&lt;/strong&gt;: Quaternions eliminate gimbal lock, ensuring continuous rotation spaces&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Compact encoding&lt;/strong&gt;: Four parameters provide complete orientation information versus nine for rotation matrices&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Smooth interpolation&lt;/strong&gt;: Unit quaternion interpolation naturally produces intermediate orientations&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Stable gradients&lt;/strong&gt;: The quaternion manifold provides well-behaved gradients for neural network training&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;A novel quaternion activation function ensures all network outputs remain valid unit quaternions, eliminating a class of animation artefacts at the architectural level.&lt;/p&gt;

&lt;h2 id=&#34;hierarchical-dynamics-architecture&#34;&gt;Hierarchical Dynamics Architecture&lt;/h2&gt;

&lt;p&gt;The A3 architecture introduces a dedicated animation dynamics module that captures the hierarchical nature of human movement:&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Separated dynamics modelling&lt;/strong&gt;: Physical dynamics (body movement) and animation dynamics (detailed articulations) are modelled separately, acknowledging their different statistical properties and control requirements.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Hierarchical state propagation&lt;/strong&gt;: The animation dynamics module receives information from physical dynamics, enabling finger movements to respond appropriately to arm and body motion.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Stochastic components&lt;/strong&gt;: Natural variation in finger movements is captured through learned stochastic elements, preventing robotic uniformity in generated animations.&lt;/p&gt;

&lt;h2 id=&#34;empirical-validation&#34;&gt;Empirical Validation&lt;/h2&gt;

&lt;p&gt;Evaluation on waving and pointing behaviours augmented with finger animation demonstrates:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Quantitative improvements&lt;/strong&gt;: 5.34 point increase in animation similarity scores compared to A2 baseline&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Perceptual quality&lt;/strong&gt;: All velocity errors remain below the 0.4 threshold for human perception&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Computational efficiency&lt;/strong&gt;: Maintained 5ms inference time despite 30+ additional joints&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Visual fidelity&lt;/strong&gt;: Natural finger curls during waving and appropriate hand shapes during pointing&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;contributions-and-implications&#34;&gt;Contributions and Implications&lt;/h2&gt;

&lt;p&gt;This chapter&amp;rsquo;s primary contributions include:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Quaternion neural networks for animation&lt;/strong&gt;: A complete framework for quaternion-based rotation learning in animation contexts&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Hierarchical latent dynamics&lt;/strong&gt;: Architectural patterns for modelling nested movement dependencies&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Scalability demonstration&lt;/strong&gt;: Evidence that the RLAnimate framework can accommodate substantial increases in animation complexity&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;The successful incorporation of finger animation validates the extensibility of the RLAnimate framework and establishes architectural patterns for handling fine-grained articulations. These developments prove essential for the subsequent challenge of conversational gesture generation, where subtle hand movements convey significant communicative content.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Chapter 7: Beat Gestures - The Ultimate Test</title>
      <link>https://vihanga.github.io/thesis/chapters/conversational-gestures/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://vihanga.github.io/thesis/chapters/conversational-gestures/</guid>
      <description>

&lt;h1 id=&#34;conversational-beat-gesture-generation&#34;&gt;Conversational Beat Gesture Generation&lt;/h1&gt;

&lt;p&gt;This chapter presents the application of RLAnimate to conversational beat gesture generation, representing the most challenging test of the framework&amp;rsquo;s capabilities. Beat gestures - rhythmic hand movements that accompany speech - lack goal-directed objectives and require precise synchronisation with speech prosody, making them unsuitable for physics-based or reward-engineered approaches.&lt;/p&gt;

&lt;h2 id=&#34;the-challenge-of-beat-gesture-synthesis&#34;&gt;The Challenge of Beat Gesture Synthesis&lt;/h2&gt;

&lt;p&gt;Beat gestures present unique challenges that distinguish them from previously addressed behaviours:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Absence of explicit objectives&lt;/strong&gt;: Unlike pointing or waving, beat gestures have no target or completion criteria&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Speech-motion coupling&lt;/strong&gt;: Gestures must synchronise with prosodic features at multiple temporal scales&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Subjective evaluation&lt;/strong&gt;: Quality assessment relies entirely on human perception of naturalness&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;High variability&lt;/strong&gt;: The same utterance permits multiple valid gesture realisations&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;These characteristics necessitate a fundamental rethinking of how reinforcement learning can be applied to animation synthesis.&lt;/p&gt;

&lt;h2 id=&#34;realism-regularisation-framework&#34;&gt;Realism Regularisation Framework&lt;/h2&gt;

&lt;p&gt;The A4 architecture introduces a comprehensive realism regularisation framework comprising four complementary components:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Rotation regularisation&lt;/strong&gt;: Constrains joint configurations to anatomically plausible ranges&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Physics regularisation&lt;/strong&gt;: Ensures conservation of momentum and energy consistency&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Smoothness regularisation&lt;/strong&gt;: Penalises high-frequency jitter and discontinuous motion&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Adversarial regularisation&lt;/strong&gt;: Learned discriminator distinguishing generated from human motion&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;The synergistic combination of these regularisation methods creates a robust prior for human-like movement, addressing the challenge of defining &amp;ldquo;natural&amp;rdquo; motion mathematically.&lt;/p&gt;

&lt;h2 id=&#34;speech-motion-architecture&#34;&gt;Speech-Motion Architecture&lt;/h2&gt;

&lt;p&gt;The hierarchical speech encoder processes acoustic features at multiple temporal resolutions:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Phoneme-level processing&lt;/strong&gt;: Captures fine-grained articulation timing&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Word-level aggregation&lt;/strong&gt;: Identifies stress and emphasis patterns&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Phrase-level context&lt;/strong&gt;: Maintains coherent gesture sequences across utterances&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;This multi-scale processing enables the generation of gestures that respond appropriately to both local prosodic variations and global speech patterns.&lt;/p&gt;

&lt;h2 id=&#34;perceptual-validation-study&#34;&gt;Perceptual Validation Study&lt;/h2&gt;

&lt;p&gt;A rigorous perceptual evaluation with 24 participants viewing 480 video clips provides definitive validation:&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Statistical parity with human motion&lt;/strong&gt;: No significant difference between RLAnimate and motion capture across three evaluation criteria (p &amp;gt; 0.31 for all measures)&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Superiority over state-of-the-art&lt;/strong&gt;: Significant improvements over Gesticulator, the leading supervised learning approach:
- Human-likeness: 4.33 vs 3.19 (p &amp;lt; 0.001)
- Speech reflection: 4.12 vs 3.42 (p &amp;lt; 0.001)
- Synchronisation: 4.48 vs 3.60 (p &amp;lt; 0.001)&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Qualitative assessment&lt;/strong&gt;: Participants could not reliably distinguish RLAnimate-generated gestures from recorded human motion.&lt;/p&gt;

&lt;h2 id=&#34;technical-contributions&#34;&gt;Technical Contributions&lt;/h2&gt;

&lt;p&gt;This chapter advances the state of the art through several innovations:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Realism regularisation&lt;/strong&gt;: A principled framework for incorporating multiple complementary constraints on generated motion&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Speech-conditioned generation&lt;/strong&gt;: Architecture patterns for synchronising movement with acoustic features&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Stochastic variability&lt;/strong&gt;: Mechanisms for generating diverse but appropriate gestures for repeated utterances&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Perceptual validation methodology&lt;/strong&gt;: Rigorous experimental design for assessing animation quality&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&#34;implications-and-significance&#34;&gt;Implications and Significance&lt;/h2&gt;

&lt;p&gt;The successful generation of perceptually indistinguishable beat gestures validates the core thesis proposition: model-based reinforcement learning with motion capture objectives can produce animation quality matching human performance. This achievement has immediate applications in:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Virtual assistants requiring natural nonverbal communication&lt;/li&gt;
&lt;li&gt;Educational technology with engaging animated instructors&lt;/li&gt;
&lt;li&gt;Therapeutic applications demanding believable social presence&lt;/li&gt;
&lt;li&gt;Entertainment systems with dynamically responsive characters&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The work demonstrates that reinforcement learning can address even the most subjective and culturally-dependent aspects of human movement, establishing a new baseline for what constitutes &amp;ldquo;human-like&amp;rdquo; in artificial animation systems.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Chapter 8: Conclusions and Future Impact</title>
      <link>https://vihanga.github.io/thesis/chapters/conclusion/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://vihanga.github.io/thesis/chapters/conclusion/</guid>
      <description>

&lt;h1 id=&#34;conclusions-and-future-impact&#34;&gt;Conclusions and Future Impact&lt;/h1&gt;

&lt;p&gt;This thesis fundamentally changes how we think about character animation. By treating animation as a model-based RL problem with motion capture objectives, I&amp;rsquo;ve created a system that produces human-quality movement at real-time speeds. But this is just the beginning.&lt;/p&gt;

&lt;h2 id=&#34;what-we-ve-achieved&#34;&gt;What We&amp;rsquo;ve Achieved&lt;/h2&gt;

&lt;p&gt;RLAnimate represents several breakthrough achievements:&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Human-quality animation from RL&lt;/strong&gt;: For the first time, RL agents generate movement that humans can&amp;rsquo;t distinguish from motion capture. This isn&amp;rsquo;t incremental improvement - it&amp;rsquo;s a paradigm shift.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Real-time performance&lt;/strong&gt;: 5ms per frame means these aren&amp;rsquo;t just research demos. They&amp;rsquo;re ready for games, VR, and interactive applications today.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Multi-behavior flexibility&lt;/strong&gt;: Single agents that wave, point, and gesture naturally - no behavior-specific engineering required.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Scalability to complexity&lt;/strong&gt;: From simple waves to 30+ finger joints to speech-synchronized gestures, the approach scales.&lt;/p&gt;

&lt;h2 id=&#34;the-technical-revolution&#34;&gt;The Technical Revolution&lt;/h2&gt;

&lt;p&gt;Three key innovations make this possible:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Motion capture as objectives&lt;/strong&gt;: Eliminating reward engineering by learning directly from human examples&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Latent dynamics models&lt;/strong&gt;: Efficient representations that capture movement essence&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Realism regularization&lt;/strong&gt;: Multiple complementary methods ensuring human-like quality&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Together, these create a framework that&amp;rsquo;s both theoretically principled and practically effective.&lt;/p&gt;

&lt;h2 id=&#34;why-this-matters-beyond-animation&#34;&gt;Why This Matters Beyond Animation&lt;/h2&gt;

&lt;p&gt;The implications extend far beyond making pretty animations:&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;For AI&lt;/strong&gt;: We&amp;rsquo;ve shown that complex, subjective human behaviors can be learned without explicit programming. The approach could apply to any domain where we have examples but can&amp;rsquo;t write rules.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;For HCI&lt;/strong&gt;: Natural movement is crucial for acceptance of virtual agents. This work enables a new generation of interfaces that communicate through body language.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;For Science&lt;/strong&gt;: By learning what makes movement &amp;ldquo;human-like,&amp;rdquo; we&amp;rsquo;re gaining insights into human motor control and social signaling.&lt;/p&gt;

&lt;h2 id=&#34;limitations-and-honesty&#34;&gt;Limitations and Honesty&lt;/h2&gt;

&lt;p&gt;No system is perfect. Current limitations include:
- Training requires motion capture data (though much less than alternatives)
- Style control is implicit rather than parametric
- Physical interactions remain challenging
- Cultural gesture variations need more exploration&lt;/p&gt;

&lt;p&gt;These aren&amp;rsquo;t fundamental barriers - they&amp;rsquo;re the next research challenges.&lt;/p&gt;

&lt;h2 id=&#34;the-road-ahead&#34;&gt;The Road Ahead&lt;/h2&gt;

&lt;p&gt;This thesis opens several exciting directions:&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Behavioral complexity&lt;/strong&gt;: Extending to full-body social interactions, emotional expressions, and context-aware responses.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Zero-shot generalization&lt;/strong&gt;: Learning movement principles that transfer across characters and scenarios without retraining.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Integration with LLMs&lt;/strong&gt;: Imagine language models that don&amp;rsquo;t just speak but move naturally as they communicate.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Real-world robotics&lt;/strong&gt;: The principles could enable robots that move in ways humans find natural and non-threatening.&lt;/p&gt;

&lt;h2 id=&#34;a-personal-vision&#34;&gt;A Personal Vision&lt;/h2&gt;

&lt;p&gt;I believe we&amp;rsquo;re at an inflection point. Just as deep learning revolutionized computer vision, model-based RL with human objectives will revolutionize character animation. We&amp;rsquo;re moving from &amp;ldquo;making characters move&amp;rdquo; to &amp;ldquo;bringing characters to life.&amp;rdquo;&lt;/p&gt;

&lt;p&gt;The tools are here. The methods work. What we create with them - more engaging games, more effective education, more natural human-computer interaction - is limited only by imagination.&lt;/p&gt;

&lt;h2 id=&#34;final-thoughts&#34;&gt;Final Thoughts&lt;/h2&gt;

&lt;p&gt;This thesis asked whether RL could create human-like animation without physics simulation or reward engineering. The answer is definitively yes. But more importantly, it&amp;rsquo;s shown a path forward for creating AI systems that capture the subtlety and beauty of human movement.&lt;/p&gt;

&lt;p&gt;Virtual characters that move like us aren&amp;rsquo;t just technically impressive - they&amp;rsquo;re emotionally resonant. They make technology feel more human. In a world increasingly mediated by screens and virtual interactions, that&amp;rsquo;s not just an academic achievement. It&amp;rsquo;s a step toward technology that truly understands and reflects our humanity.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Chapter 4: Supplementary Material</title>
      <link>https://vihanga.github.io/thesis/supplementary/c4/</link>
      <pubDate>Wed, 22 Jan 2025 00:00:00 +0000</pubDate>
      
      <guid>https://vihanga.github.io/thesis/supplementary/c4/</guid>
      <description>

&lt;h1 id=&#34;chapter-4-model-based-character-animation&#34;&gt;Chapter 4: Model-Based Character Animation&lt;/h1&gt;

&lt;p&gt;This page contains supplementary material referenced in Chapter 4 of the thesis, demonstrating the model-based reinforcement learning agents trained to portray gazing and pointing behaviors.&lt;/p&gt;

&lt;h2 id=&#34;video-demonstrations&#34;&gt;Video Demonstrations&lt;/h2&gt;

&lt;h3 id=&#34;model-based-animation-agents-amsta-2021&#34;&gt;Model-Based Animation Agents (AMSTA 2021)&lt;/h3&gt;

&lt;p&gt;The following video demonstrates the agents trained using learned dynamics models and online planning for target-driven behaviors:&lt;/p&gt;


&lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
  &lt;iframe src=&#34;//www.youtube.com/embed/XBNY2tPK7JE&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; allowfullscreen title=&#34;YouTube Video&#34;&gt;&lt;/iframe&gt;
&lt;/div&gt;


&lt;p&gt;This video shows:
- Gazing behavior with dynamic target tracking
- Pointing behavior with both left and right arms
- Combined gaze and point behaviors
- Real-time responsiveness to changing targets
- Comparison with inverse kinematics baseline&lt;/p&gt;

&lt;h3 id=&#34;key-results-demonstrated&#34;&gt;Key Results Demonstrated&lt;/h3&gt;

&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Computational Efficiency&lt;/strong&gt;: 5ms per frame (40x faster than IK baseline)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Dynamic Flexibility&lt;/strong&gt;: Agents adapt to moving targets without computational overhead&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Multi-behavior Support&lt;/strong&gt;: Single agent architecture supports gaze, point, and combined behaviors&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Beta Distribution Planning&lt;/strong&gt;: Novel approach to smooth animation generation&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&#34;navigation&#34;&gt;Navigation&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;../c5a/&#34;&gt;Chapter 5 Part A: Model-free RL →&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://vihanga.github.io/thesis/&#34; target=&#34;_blank&#34;&gt;← Back to thesis main page&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://vihanga.github.io/thesis/chapters/model-based-animation/&#34; target=&#34;_blank&#34;&gt;Chapter 4 in main thesis&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Chapter 5 Supplementary Material - Part A: RLAnimate Output Sequences</title>
      <link>https://vihanga.github.io/thesis/supplementary/c5a/</link>
      <pubDate>Wed, 22 Jan 2025 00:00:00 +0000</pubDate>
      
      <guid>https://vihanga.github.io/thesis/supplementary/c5a/</guid>
      <description>

&lt;h1 id=&#34;chapter-5-part-a-rlanimate-output-sequences&#34;&gt;Chapter 5 Part A: RLAnimate Output Sequences&lt;/h1&gt;

&lt;p&gt;This page contains supplementary material for Chapter 5, demonstrating the RLAnimate A1 agents trained to portray waving and pointing behaviors.&lt;/p&gt;

&lt;h2 id=&#34;video-demonstrations&#34;&gt;Video Demonstrations&lt;/h2&gt;

&lt;h3 id=&#34;rlanimate-output-sequences-and-comparison-to-control-agents-ala-2021&#34;&gt;RLAnimate Output Sequences and Comparison to Control Agents (ALA 2021)&lt;/h3&gt;

&lt;p&gt;The following video shows RLAnimate agents performing waving and pointing behaviors, with comparisons to baseline control agents:&lt;/p&gt;


&lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
  &lt;iframe src=&#34;//www.youtube.com/embed/XyxD86Jz4Z0&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; allowfullscreen title=&#34;YouTube Video&#34;&gt;&lt;/iframe&gt;
&lt;/div&gt;


&lt;p&gt;This video demonstrates:
- A1 agent performing waving behaviors with varying exaggeration levels
- A1 agent performing pointing behaviors to different targets
- Comparison with single dynamics baseline
- Comparison with supervised learning baseline
- Natural, human-like motion generation&lt;/p&gt;

&lt;h3 id=&#34;key-results-shown&#34;&gt;Key Results Shown&lt;/h3&gt;

&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Multi-behavior Support&lt;/strong&gt;: Single agent portraying both waving and pointing&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Dynamic Control&lt;/strong&gt;: Real-time adaptation to behavior parameters&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Human-like Quality&lt;/strong&gt;: Smooth, natural movements without artifacts&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Superior Performance&lt;/strong&gt;: Outperforms baselines on test set (87.23 vs 70.94)&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&#34;navigation&#34;&gt;Navigation&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;../c5b/&#34;&gt;Chapter 5 Part B: Behavior Flexibility →&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://vihanga.github.io/thesis/&#34; target=&#34;_blank&#34;&gt;← Back to thesis main page&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://vihanga.github.io/thesis/chapters/data-driven-rl/&#34; target=&#34;_blank&#34;&gt;Chapter 5 in main thesis&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Chapter 5 Supplementary Material - Part B: Behavior Flexibility</title>
      <link>https://vihanga.github.io/thesis/supplementary/c5b/</link>
      <pubDate>Wed, 22 Jan 2025 00:00:00 +0000</pubDate>
      
      <guid>https://vihanga.github.io/thesis/supplementary/c5b/</guid>
      <description>

&lt;h1 id=&#34;chapter-5-part-b-rlanimate-behavior-portrayal-flexibility&#34;&gt;Chapter 5 Part B: RLAnimate Behavior Portrayal Flexibility&lt;/h1&gt;

&lt;p&gt;This page contains supplementary material for Chapter 5, demonstrating the flexibility and dynamic control capabilities of RLAnimate agents.&lt;/p&gt;

&lt;h2 id=&#34;video-demonstrations&#34;&gt;Video Demonstrations&lt;/h2&gt;

&lt;h3 id=&#34;rlanimate-behavior-portrayal-flexibility-ala-2021&#34;&gt;RLAnimate Behavior Portrayal Flexibility (ALA 2021)&lt;/h3&gt;

&lt;p&gt;The following video demonstrates the dynamic flexibility of RLAnimate agents in portraying varied behaviors:&lt;/p&gt;


&lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
  &lt;iframe src=&#34;//www.youtube.com/embed/cdIM7vMi1XM&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; allowfullscreen title=&#34;YouTube Video&#34;&gt;&lt;/iframe&gt;
&lt;/div&gt;


&lt;p&gt;This video shows:
- Waving with continuously variable exaggeration levels
- Pointing to dynamically changing targets
- Smooth transitions between behaviors
- Real-time responsiveness to parameter changes
- Natural variation in movement execution&lt;/p&gt;

&lt;h3 id=&#34;key-capabilities-demonstrated&#34;&gt;Key Capabilities Demonstrated&lt;/h3&gt;

&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Continuous Control&lt;/strong&gt;: Smooth interpolation between behavior parameters&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Dynamic Adaptation&lt;/strong&gt;: Real-time response to changing objectives&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Natural Variation&lt;/strong&gt;: Human-like variability in repeated behaviors&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Behavior Transitions&lt;/strong&gt;: Seamless switching between waving and pointing&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&#34;navigation&#34;&gt;Navigation&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;../c5a/&#34;&gt;← Chapter 5 Part A: Output Sequences&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;../c6/&#34;&gt;Chapter 6 Supplementary Material →&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://vihanga.github.io/thesis/&#34; target=&#34;_blank&#34;&gt;← Back to thesis main page&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://vihanga.github.io/thesis/chapters/data-driven-rl/&#34; target=&#34;_blank&#34;&gt;Chapter 5 in main thesis&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Chapter 6 Supplementary Material: Quaternions and Finger Animation</title>
      <link>https://vihanga.github.io/thesis/supplementary/c6/</link>
      <pubDate>Wed, 22 Jan 2025 00:00:00 +0000</pubDate>
      
      <guid>https://vihanga.github.io/thesis/supplementary/c6/</guid>
      <description>

&lt;h1 id=&#34;chapter-6-latent-dynamic-augmented-animation-output&#34;&gt;Chapter 6: Latent Dynamic-augmented Animation Output&lt;/h1&gt;

&lt;p&gt;This page contains supplementary material referenced in Chapter 6 of the thesis, demonstrating the A3 architecture with quaternion rotations and animation dynamics for finger animation.&lt;/p&gt;

&lt;h2 id=&#34;a3-architecture-results&#34;&gt;A3 Architecture Results&lt;/h2&gt;

&lt;p&gt;The A3 architecture introduced in Chapter 6 addresses the challenge of incorporating finger animation through:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Quaternion Representations&lt;/strong&gt;: Eliminating gimbal lock and improving computational efficiency&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Animation Dynamics Model&lt;/strong&gt;: Dedicated latent dynamics for animation generation&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Hierarchical Learning&lt;/strong&gt;: Better capture of joint dependencies&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&#34;performance-improvements&#34;&gt;Performance Improvements&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;A2 + Quaternion baseline&lt;/strong&gt;: 81.23 similarity score&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;A3 Standard&lt;/strong&gt;: 86.57 similarity score (5.34 point improvement)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;A3 with all latents&lt;/strong&gt;: 86.19 similarity score&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;All velocity errors remain under the 0.4 perceptual threshold, confirming that the A3 architecture successfully maintains motion quality while handling the additional complexity of finger animation. As noted in the thesis (Section 6.3), visual analysis confirms that the A3 architecture maintains animation quality despite the increased complexity of animating 30+ finger joints.&lt;/p&gt;

&lt;h2 id=&#34;technical-contributions&#34;&gt;Technical Contributions&lt;/h2&gt;

&lt;h3 id=&#34;quaternion-activation-function&#34;&gt;Quaternion Activation Function&lt;/h3&gt;

&lt;p&gt;The novel quaternion activation function ensures all neural network outputs are valid unit quaternions:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;f(w,x,y,z) = [w,x,y,z] / sqrt(w² + x² + y² + z²)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This eliminates singularities (except at origin) and provides smooth gradients for training.&lt;/p&gt;

&lt;h3 id=&#34;animation-dynamics-model&#34;&gt;Animation Dynamics Model&lt;/h3&gt;

&lt;p&gt;The dedicated animation dynamics model learns hierarchical relationships between joints through:
- Deterministic state: h&lt;em&gt;t^a = f(h&lt;/em&gt;{t-1}^a, s_{t-1}, h_t^p, p_t, h_t^b, b_t)
- Stochastic component for natural variation
- Temporal consistency through recurrent processing&lt;/p&gt;

&lt;h3 id=&#34;a3-agent-capabilities&#34;&gt;A3 Agent Capabilities&lt;/h3&gt;

&lt;p&gt;The A3 agents demonstrate:
- Successful incorporation of finger movements in waving behaviors
- Natural finger positioning during pointing tasks
- Smooth transitions without artifacts
- Maintained performance under the 0.4 velocity error threshold&lt;/p&gt;

&lt;h2 id=&#34;navigation&#34;&gt;Navigation&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;../c5b/&#34;&gt;← Chapter 5 Part B: Behavior Flexibility&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;../c7/&#34;&gt;Chapter 7 Supplementary Material →&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://vihanga.github.io/thesis/&#34; target=&#34;_blank&#34;&gt;← Back to thesis main page&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://vihanga.github.io/thesis/chapters/latent-dynamics/&#34; target=&#34;_blank&#34;&gt;Chapter 6 in main thesis&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Chapter 7 Supplementary Material: Beat Gestures</title>
      <link>https://vihanga.github.io/thesis/supplementary/c7/</link>
      <pubDate>Wed, 22 Jan 2025 00:00:00 +0000</pubDate>
      
      <guid>https://vihanga.github.io/thesis/supplementary/c7/</guid>
      <description>

&lt;h1 id=&#34;chapter-7-portraying-conversational-gestures-via-realism-regularisation&#34;&gt;Chapter 7: Portraying Conversational Gestures via Realism Regularisation&lt;/h1&gt;

&lt;p&gt;This page contains supplementary material referenced in Chapter 7, including the video stimuli used in the perceptual evaluation study comparing RLAnimate A4, Gesticulator, and motion capture.&lt;/p&gt;

&lt;h2 id=&#34;perceptual-study-video-stimuli&#34;&gt;Perceptual Study Video Stimuli&lt;/h2&gt;

&lt;p&gt;The following videos were used in the perceptual evaluation with 28 participants. Each stimulus set was rendered with three methods: RLAnimate A4, Gesticulator, and Motion Capture (ground truth).&lt;/p&gt;

&lt;style&gt;
.video-set {
  margin-bottom: 50px;
  background: #f5f5f5;
  padding: 20px;
  border-radius: 8px;
}

.video-grid {
  display: grid;
  grid-template-columns: repeat(3, 1fr);
  gap: 20px;
  margin-bottom: 20px;
}

.video-container {
  position: relative;
}

.video-container h4 {
  text-align: center;
  margin-bottom: 10px;
  font-size: 1.1em;
  color: #333;
}

.video-container video {
  width: 100%;
  border-radius: 4px;
  box-shadow: 0 2px 4px rgba(0,0,0,0.1);
}

.play-controls {
  text-align: center;
  margin-top: 20px;
}

.play-button {
  background-color: #007bff;
  color: white;
  border: none;
  padding: 12px 30px;
  font-size: 16px;
  border-radius: 4px;
  cursor: pointer;
  transition: background-color 0.3s;
}

.play-button:hover {
  background-color: #0056b3;
}

.play-button.playing {
  background-color: #dc3545;
}

.play-button.playing:hover {
  background-color: #c82333;
}

.play-button:disabled {
  background-color: #6c757d;
  cursor: not-allowed;
}

/* Loading indicator */
.loading-indicator {
  position: fixed;
  top: 20px;
  right: 20px;
  background: #333;
  color: white;
  padding: 15px 20px;
  border-radius: 8px;
  display: none;
  z-index: 1000;
  box-shadow: 0 4px 6px rgba(0,0,0,0.3);
}

.loading-indicator.show {
  display: block;
}

.loading-progress {
  font-size: 14px;
  margin-top: 5px;
}

/* Buffer status indicator */
.buffer-status {
  position: absolute;
  top: 5px;
  right: 5px;
  background: rgba(0,0,0,0.7);
  color: white;
  padding: 2px 8px;
  border-radius: 3px;
  font-size: 11px;
  display: none;
}

.buffer-status.loading {
  display: block;
  background: rgba(255,193,7,0.9);
  color: #333;
}

.buffer-status.ready {
  display: block;
  background: rgba(40,167,69,0.9);
}

@media (max-width: 768px) {
  .video-grid {
    grid-template-columns: 1fr;
  }
  
  .loading-indicator {
    top: 10px;
    right: 10px;
    font-size: 14px;
  }
}
&lt;/style&gt;

&lt;!-- Loading indicator --&gt;

&lt;div class=&#34;loading-indicator&#34; id=&#34;loadingIndicator&#34;&gt;
  &lt;div&gt;Loading videos...&lt;/div&gt;
  &lt;div class=&#34;loading-progress&#34; id=&#34;loadingProgress&#34;&gt;0%&lt;/div&gt;
&lt;/div&gt;

&lt;script&gt;
let videosLoaded = 0;
let totalVideos = 0;
const videoBufferStatus = new Map();

function updateLoadingProgress() {
  const progress = Math.round((videosLoaded / totalVideos) * 100);
  const progressEl = document.getElementById(&#39;loadingProgress&#39;);
  const indicator = document.getElementById(&#39;loadingIndicator&#39;);
  
  if (progressEl) {
    progressEl.textContent = `${progress}% (${videosLoaded}/${totalVideos} videos)`;
  }
  
  if (videosLoaded === totalVideos &amp;&amp; indicator) {
    setTimeout(() =&gt; {
      indicator.classList.remove(&#39;show&#39;);
    }, 1000);
  }
}

function preloadVideo(video, setId) {
  // Add buffer status indicator
  const container = video.closest(&#39;.video-container&#39;);
  const statusEl = document.createElement(&#39;div&#39;);
  statusEl.className = &#39;buffer-status loading&#39;;
  statusEl.textContent = &#39;Loading...&#39;;
  container.appendChild(statusEl);
  
  // Set preload attribute
  video.preload = &#39;auto&#39;;
  
  // Track loading progress
  video.addEventListener(&#39;loadeddata&#39;, () =&gt; {
    statusEl.textContent = &#39;Buffering...&#39;;
  });
  
  video.addEventListener(&#39;canplaythrough&#39;, () =&gt; {
    videosLoaded++;
    videoBufferStatus.set(video, true);
    statusEl.className = &#39;buffer-status ready&#39;;
    statusEl.textContent = &#39;Ready&#39;;
    updateLoadingProgress();
    
    // Hide ready status after 3 seconds
    setTimeout(() =&gt; {
      statusEl.style.display = &#39;none&#39;;
    }, 3000);
    
    // Enable play button when all videos in set are ready
    const setVideos = document.querySelectorAll(`#${setId} video`);
    const allReady = Array.from(setVideos).every(v =&gt; videoBufferStatus.get(v));
    if (allReady) {
      const button = document.querySelector(`#${setId} .play-button`);
      if (button) {
        button.disabled = false;
        button.textContent = &#39;▶ Play All&#39;;
      }
    }
  });
  
  video.addEventListener(&#39;error&#39;, () =&gt; {
    statusEl.className = &#39;buffer-status loading&#39;;
    statusEl.textContent = &#39;Error&#39;;
    statusEl.style.background = &#39;rgba(220,53,69,0.9)&#39;;
  });
  
  // Force load by setting currentTime
  video.load();
}

function toggleVideos(setId) {
  const videos = document.querySelectorAll(`#${setId} video`);
  const button = document.querySelector(`#${setId} .play-button`);
  
  // Check if all videos are ready
  const allReady = Array.from(videos).every(v =&gt; videoBufferStatus.get(v));
  if (!allReady) {
    alert(&#39;Videos are still loading. Please wait...&#39;);
    return;
  }
  
  const isPlaying = videos[0].paused;
  
  videos.forEach(video =&gt; {
    if (isPlaying) {
      video.play().catch(e =&gt; console.error(&#39;Play error:&#39;, e));
    } else {
      video.pause();
    }
  });
  
  if (isPlaying) {
    button.textContent = &#39;⏸ Pause All&#39;;
    button.classList.add(&#39;playing&#39;);
  } else {
    button.textContent = &#39;▶ Play All&#39;;
    button.classList.remove(&#39;playing&#39;);
  }
}

// Sync video playback positions
function syncVideos(setId) {
  const videos = document.querySelectorAll(`#${setId} video`);
  const masterVideo = videos[0];
  
  masterVideo.addEventListener(&#39;timeupdate&#39;, () =&gt; {
    videos.forEach((video, index) =&gt; {
      if (index !== 0 &amp;&amp; Math.abs(video.currentTime - masterVideo.currentTime) &gt; 0.1) {
        video.currentTime = masterVideo.currentTime;
      }
    });
  });
  
  // Reset all videos when one ends
  videos.forEach(video =&gt; {
    video.addEventListener(&#39;ended&#39;, () =&gt; {
      videos.forEach(v =&gt; {
        v.currentTime = 0;
        v.pause();
      });
      const button = document.querySelector(`#${setId} .play-button`);
      button.textContent = &#39;▶ Play All&#39;;
      button.classList.remove(&#39;playing&#39;);
    });
  });
}

// Initialize when page loads
document.addEventListener(&#39;DOMContentLoaded&#39;, function() {
  const sets = [&#39;setA&#39;, &#39;setB&#39;, &#39;setC&#39;, &#39;setD&#39;, &#39;setE&#39;, &#39;setF&#39;, &#39;setG&#39;, &#39;setH&#39;];
  const allVideos = document.querySelectorAll(&#39;video&#39;);
  totalVideos = allVideos.length;
  
  // Show loading indicator
  const indicator = document.getElementById(&#39;loadingIndicator&#39;);
  if (indicator &amp;&amp; totalVideos &gt; 0) {
    indicator.classList.add(&#39;show&#39;);
  }
  
  // Disable all play buttons initially
  document.querySelectorAll(&#39;.play-button&#39;).forEach(button =&gt; {
    button.disabled = true;
    button.textContent = &#39;⏳ Loading...&#39;;
  });
  
  sets.forEach(setId =&gt; {
    if (document.getElementById(setId)) {
      syncVideos(setId);
      
      // Preload all videos in the set
      const videos = document.querySelectorAll(`#${setId} video`);
      videos.forEach(video =&gt; {
        preloadVideo(video, setId);
      });
    }
  });
});
&lt;/script&gt;

&lt;h3 id=&#34;stimulus-set-a&#34;&gt;Stimulus Set A&lt;/h3&gt;

&lt;div class=&#34;video-set&#34; id=&#34;setA&#34;&gt;
  &lt;div class=&#34;video-grid&#34;&gt;
    &lt;div class=&#34;video-container&#34;&gt;
      &lt;h4&gt;RLAnimate A4&lt;/h4&gt;
      &lt;video controls width=&#34;100%&#34;&gt;
        &lt;source src=&#34;../../videos/chapter7/02_rlanimate_malcolm.mp4&#34; type=&#34;video/mp4&#34;&gt;
      &lt;/video&gt;
    &lt;/div&gt;
    &lt;div class=&#34;video-container&#34;&gt;
      &lt;h4&gt;Gesticulator&lt;/h4&gt;
      &lt;video controls width=&#34;100%&#34;&gt;
        &lt;source src=&#34;../../videos/chapter7/02_gesticulator_malcolm.mp4&#34; type=&#34;video/mp4&#34;&gt;
      &lt;/video&gt;
    &lt;/div&gt;
    &lt;div class=&#34;video-container&#34;&gt;
      &lt;h4&gt;Motion Capture&lt;/h4&gt;
      &lt;video controls width=&#34;100%&#34;&gt;
        &lt;source src=&#34;../../videos/chapter7/02_mocap_malcolm.mp4&#34; type=&#34;video/mp4&#34;&gt;
      &lt;/video&gt;
    &lt;/div&gt;
  &lt;/div&gt;
  &lt;div class=&#34;play-controls&#34;&gt;
    &lt;button class=&#34;play-button&#34; onclick=&#34;toggleVideos(&#39;setA&#39;)&#34;&gt;▶ Play All&lt;/button&gt;
  &lt;/div&gt;
&lt;/div&gt;

&lt;h3 id=&#34;stimulus-set-b&#34;&gt;Stimulus Set B&lt;/h3&gt;

&lt;div class=&#34;video-set&#34; id=&#34;setB&#34;&gt;
  &lt;div class=&#34;video-grid&#34;&gt;
    &lt;div class=&#34;video-container&#34;&gt;
      &lt;h4&gt;RLAnimate A4&lt;/h4&gt;
      &lt;video controls width=&#34;100%&#34;&gt;
        &lt;source src=&#34;../../videos/chapter7/03_rlanimate_malcolm.mp4&#34; type=&#34;video/mp4&#34;&gt;
      &lt;/video&gt;
    &lt;/div&gt;
    &lt;div class=&#34;video-container&#34;&gt;
      &lt;h4&gt;Gesticulator&lt;/h4&gt;
      &lt;video controls width=&#34;100%&#34;&gt;
        &lt;source src=&#34;../../videos/chapter7/03_gesticulator_malcolm.mp4&#34; type=&#34;video/mp4&#34;&gt;
      &lt;/video&gt;
    &lt;/div&gt;
    &lt;div class=&#34;video-container&#34;&gt;
      &lt;h4&gt;Motion Capture&lt;/h4&gt;
      &lt;video controls width=&#34;100%&#34;&gt;
        &lt;source src=&#34;../../videos/chapter7/03_mocap_malcolm.mp4&#34; type=&#34;video/mp4&#34;&gt;
      &lt;/video&gt;
    &lt;/div&gt;
  &lt;/div&gt;
  &lt;div class=&#34;play-controls&#34;&gt;
    &lt;button class=&#34;play-button&#34; onclick=&#34;toggleVideos(&#39;setB&#39;)&#34;&gt;▶ Play All&lt;/button&gt;
  &lt;/div&gt;
&lt;/div&gt;

&lt;h3 id=&#34;stimulus-set-c&#34;&gt;Stimulus Set C&lt;/h3&gt;

&lt;div class=&#34;video-set&#34; id=&#34;setC&#34;&gt;
  &lt;div class=&#34;video-grid&#34;&gt;
    &lt;div class=&#34;video-container&#34;&gt;
      &lt;h4&gt;RLAnimate A4&lt;/h4&gt;
      &lt;video controls width=&#34;100%&#34;&gt;
        &lt;source src=&#34;../../videos/chapter7/05_rlanimate_malcolm.mp4&#34; type=&#34;video/mp4&#34;&gt;
      &lt;/video&gt;
    &lt;/div&gt;
    &lt;div class=&#34;video-container&#34;&gt;
      &lt;h4&gt;Gesticulator&lt;/h4&gt;
      &lt;video controls width=&#34;100%&#34;&gt;
        &lt;source src=&#34;../../videos/chapter7/05_gesticulator_malcolm.mp4&#34; type=&#34;video/mp4&#34;&gt;
      &lt;/video&gt;
    &lt;/div&gt;
    &lt;div class=&#34;video-container&#34;&gt;
      &lt;h4&gt;Motion Capture&lt;/h4&gt;
      &lt;video controls width=&#34;100%&#34;&gt;
        &lt;source src=&#34;../../videos/chapter7/05_mocap_malcolm.mp4&#34; type=&#34;video/mp4&#34;&gt;
      &lt;/video&gt;
    &lt;/div&gt;
  &lt;/div&gt;
  &lt;div class=&#34;play-controls&#34;&gt;
    &lt;button class=&#34;play-button&#34; onclick=&#34;toggleVideos(&#39;setC&#39;)&#34;&gt;▶ Play All&lt;/button&gt;
  &lt;/div&gt;
&lt;/div&gt;

&lt;h3 id=&#34;stimulus-set-d&#34;&gt;Stimulus Set D&lt;/h3&gt;

&lt;div class=&#34;video-set&#34; id=&#34;setD&#34;&gt;
  &lt;div class=&#34;video-grid&#34;&gt;
    &lt;div class=&#34;video-container&#34;&gt;
      &lt;h4&gt;RLAnimate A4&lt;/h4&gt;
      &lt;video controls width=&#34;100%&#34;&gt;
        &lt;source src=&#34;../../videos/chapter7/06_rlanimate_malcolm.mp4&#34; type=&#34;video/mp4&#34;&gt;
      &lt;/video&gt;
    &lt;/div&gt;
    &lt;div class=&#34;video-container&#34;&gt;
      &lt;h4&gt;Gesticulator&lt;/h4&gt;
      &lt;video controls width=&#34;100%&#34;&gt;
        &lt;source src=&#34;../../videos/chapter7/06_gesticulator_malcolm.mp4&#34; type=&#34;video/mp4&#34;&gt;
      &lt;/video&gt;
    &lt;/div&gt;
    &lt;div class=&#34;video-container&#34;&gt;
      &lt;h4&gt;Motion Capture&lt;/h4&gt;
      &lt;video controls width=&#34;100%&#34;&gt;
        &lt;source src=&#34;../../videos/chapter7/06_mocap_malcolm.mp4&#34; type=&#34;video/mp4&#34;&gt;
      &lt;/video&gt;
    &lt;/div&gt;
  &lt;/div&gt;
  &lt;div class=&#34;play-controls&#34;&gt;
    &lt;button class=&#34;play-button&#34; onclick=&#34;toggleVideos(&#39;setD&#39;)&#34;&gt;▶ Play All&lt;/button&gt;
  &lt;/div&gt;
&lt;/div&gt;

&lt;h3 id=&#34;stimulus-set-e&#34;&gt;Stimulus Set E&lt;/h3&gt;

&lt;div class=&#34;video-set&#34; id=&#34;setE&#34;&gt;
  &lt;div class=&#34;video-grid&#34;&gt;
    &lt;div class=&#34;video-container&#34;&gt;
      &lt;h4&gt;RLAnimate A4&lt;/h4&gt;
      &lt;video controls width=&#34;100%&#34;&gt;
        &lt;source src=&#34;../../videos/chapter7/07_rlanimate_malcolm.mp4&#34; type=&#34;video/mp4&#34;&gt;
      &lt;/video&gt;
    &lt;/div&gt;
    &lt;div class=&#34;video-container&#34;&gt;
      &lt;h4&gt;Gesticulator&lt;/h4&gt;
      &lt;video controls width=&#34;100%&#34;&gt;
        &lt;source src=&#34;../../videos/chapter7/07_gesticulator_malcolm.mp4&#34; type=&#34;video/mp4&#34;&gt;
      &lt;/video&gt;
    &lt;/div&gt;
    &lt;div class=&#34;video-container&#34;&gt;
      &lt;h4&gt;Motion Capture&lt;/h4&gt;
      &lt;video controls width=&#34;100%&#34;&gt;
        &lt;source src=&#34;../../videos/chapter7/07_mocap_malcolm.mp4&#34; type=&#34;video/mp4&#34;&gt;
      &lt;/video&gt;
    &lt;/div&gt;
  &lt;/div&gt;
  &lt;div class=&#34;play-controls&#34;&gt;
    &lt;button class=&#34;play-button&#34; onclick=&#34;toggleVideos(&#39;setE&#39;)&#34;&gt;▶ Play All&lt;/button&gt;
  &lt;/div&gt;
&lt;/div&gt;

&lt;h3 id=&#34;stimulus-set-f&#34;&gt;Stimulus Set F&lt;/h3&gt;

&lt;div class=&#34;video-set&#34; id=&#34;setF&#34;&gt;
  &lt;div class=&#34;video-grid&#34;&gt;
    &lt;div class=&#34;video-container&#34;&gt;
      &lt;h4&gt;RLAnimate A4&lt;/h4&gt;
      &lt;video controls width=&#34;100%&#34;&gt;
        &lt;source src=&#34;../../videos/chapter7/08_rlanimate_malcolm.mp4&#34; type=&#34;video/mp4&#34;&gt;
      &lt;/video&gt;
    &lt;/div&gt;
    &lt;div class=&#34;video-container&#34;&gt;
      &lt;h4&gt;Gesticulator&lt;/h4&gt;
      &lt;video controls width=&#34;100%&#34;&gt;
        &lt;source src=&#34;../../videos/chapter7/08_gesticulator_malcolm.mp4&#34; type=&#34;video/mp4&#34;&gt;
      &lt;/video&gt;
    &lt;/div&gt;
    &lt;div class=&#34;video-container&#34;&gt;
      &lt;h4&gt;Motion Capture&lt;/h4&gt;
      &lt;video controls width=&#34;100%&#34;&gt;
        &lt;source src=&#34;../../videos/chapter7/08_mocap_malcolm.mp4&#34; type=&#34;video/mp4&#34;&gt;
      &lt;/video&gt;
    &lt;/div&gt;
  &lt;/div&gt;
  &lt;div class=&#34;play-controls&#34;&gt;
    &lt;button class=&#34;play-button&#34; onclick=&#34;toggleVideos(&#39;setF&#39;)&#34;&gt;▶ Play All&lt;/button&gt;
  &lt;/div&gt;
&lt;/div&gt;

&lt;h3 id=&#34;stimulus-set-g&#34;&gt;Stimulus Set G&lt;/h3&gt;

&lt;div class=&#34;video-set&#34; id=&#34;setG&#34;&gt;
  &lt;div class=&#34;video-grid&#34;&gt;
    &lt;div class=&#34;video-container&#34;&gt;
      &lt;h4&gt;RLAnimate A4&lt;/h4&gt;
      &lt;video controls width=&#34;100%&#34;&gt;
        &lt;source src=&#34;../../videos/chapter7/09_rlanimate_malcolm.mp4&#34; type=&#34;video/mp4&#34;&gt;
      &lt;/video&gt;
    &lt;/div&gt;
    &lt;div class=&#34;video-container&#34;&gt;
      &lt;h4&gt;Gesticulator&lt;/h4&gt;
      &lt;video controls width=&#34;100%&#34;&gt;
        &lt;source src=&#34;../../videos/chapter7/09_gesticulator_malcolm.mp4&#34; type=&#34;video/mp4&#34;&gt;
      &lt;/video&gt;
    &lt;/div&gt;
    &lt;div class=&#34;video-container&#34;&gt;
      &lt;h4&gt;Motion Capture&lt;/h4&gt;
      &lt;video controls width=&#34;100%&#34;&gt;
        &lt;source src=&#34;../../videos/chapter7/09_mocap_malcolm.mp4&#34; type=&#34;video/mp4&#34;&gt;
      &lt;/video&gt;
    &lt;/div&gt;
  &lt;/div&gt;
  &lt;div class=&#34;play-controls&#34;&gt;
    &lt;button class=&#34;play-button&#34; onclick=&#34;toggleVideos(&#39;setG&#39;)&#34;&gt;▶ Play All&lt;/button&gt;
  &lt;/div&gt;
&lt;/div&gt;

&lt;h3 id=&#34;stimulus-set-h&#34;&gt;Stimulus Set H&lt;/h3&gt;

&lt;div class=&#34;video-set&#34; id=&#34;setH&#34;&gt;
  &lt;div class=&#34;video-grid&#34;&gt;
    &lt;div class=&#34;video-container&#34;&gt;
      &lt;h4&gt;RLAnimate A4&lt;/h4&gt;
      &lt;video controls width=&#34;100%&#34;&gt;
        &lt;source src=&#34;../../videos/chapter7/10_rlanimate_malcolm.mp4&#34; type=&#34;video/mp4&#34;&gt;
      &lt;/video&gt;
    &lt;/div&gt;
    &lt;div class=&#34;video-container&#34;&gt;
      &lt;h4&gt;Gesticulator&lt;/h4&gt;
      &lt;video controls width=&#34;100%&#34;&gt;
        &lt;source src=&#34;../../videos/chapter7/10_gesticulator_malcolm.mp4&#34; type=&#34;video/mp4&#34;&gt;
      &lt;/video&gt;
    &lt;/div&gt;
    &lt;div class=&#34;video-container&#34;&gt;
      &lt;h4&gt;Motion Capture&lt;/h4&gt;
      &lt;video controls width=&#34;100%&#34;&gt;
        &lt;source src=&#34;../../videos/chapter7/10_mocap_malcolm.mp4&#34; type=&#34;video/mp4&#34;&gt;
      &lt;/video&gt;
    &lt;/div&gt;
  &lt;/div&gt;
  &lt;div class=&#34;play-controls&#34;&gt;
    &lt;button class=&#34;play-button&#34; onclick=&#34;toggleVideos(&#39;setH&#39;)&#34;&gt;▶ Play All&lt;/button&gt;
  &lt;/div&gt;
&lt;/div&gt;

&lt;h2 id=&#34;perceptual-study-results&#34;&gt;Perceptual Study Results&lt;/h2&gt;

&lt;h3 id=&#34;statistical-results&#34;&gt;Statistical Results&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;RLAnimate A4 vs Gesticulator&lt;/strong&gt; (one-sided t-tests, α = 0.05):
- Human-likeness: 4.33 vs 3.19 (p &amp;lt; 0.001) ✓
- Speech reflection: 4.12 vs 3.42 (p &amp;lt; 0.001) ✓
- Synchronization: 4.48 vs 3.60 (p &amp;lt; 0.001) ✓&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;RLAnimate A4 vs Motion Capture&lt;/strong&gt; (two-sided t-tests, α = 0.05):
- Human-likeness: 4.33 vs 4.65 (p = 0.083, NS)
- Speech reflection: 4.12 vs 4.38 (p = 0.174, NS)
- Synchronization: 4.48 vs 4.67 (p = 0.268, NS)&lt;/p&gt;

&lt;h3 id=&#34;key-finding&#34;&gt;Key Finding&lt;/h3&gt;

&lt;p&gt;RLAnimate A4 achieves &lt;strong&gt;statistical parity with human motion capture&lt;/strong&gt; while significantly outperforming the state-of-the-art Gesticulator system across all evaluation criteria.&lt;/p&gt;

&lt;h2 id=&#34;navigation&#34;&gt;Navigation&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;../c6/&#34;&gt;← Chapter 6 Supplementary Material&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://vihanga.github.io/thesis/&#34; target=&#34;_blank&#34;&gt;← Back to thesis main page&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://vihanga.github.io/thesis/chapters/conversational-gestures/&#34; target=&#34;_blank&#34;&gt;Chapter 7 in main thesis&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>MIG&#39;18 Paper: Virtual Characters in Serious Games</title>
      <link>https://vihanga.github.io/thesis/publications/MIG18/</link>
      <pubDate>Wed, 22 Jan 2025 00:00:00 +0000</pubDate>
      
      <guid>https://vihanga.github.io/thesis/publications/MIG18/</guid>
      <description>

&lt;h1 id=&#34;examining-the-effects-of-a-virtual-character-on-learning-and-engagement-in-serious-games&#34;&gt;Examining the effects of a virtual character on learning and engagement in serious games&lt;/h1&gt;

&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Vihanga Gamage, Cathy Ennis&lt;br /&gt;
&lt;strong&gt;Conference:&lt;/strong&gt; MIG &amp;lsquo;18: Motion, Interaction and Games, November 8–10, 2018, Limassol, Cyprus&lt;br /&gt;
&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href=&#34;https://doi.org/10.1145/3274247.3274499&#34; target=&#34;_blank&#34;&gt;10.&lt;sup&gt;1145&lt;/sup&gt;&amp;frasl;&lt;sub&gt;3274247&lt;/sub&gt;.3274499&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&#34;abstract&#34;&gt;Abstract&lt;/h2&gt;

&lt;p&gt;Virtual characters have been employed for many purposes including interacting with players of serious games, with a purpose to increase engagement. These characters are often embodied conversational agents playing diverse roles, such as demonstrators, guides, teachers or interviewers. Recently, much research has been conducted into properties that affect the realism and plausibility of virtual characters, but it is less clear whether the inclusion of interactive agents in serious applications can enhance a user&amp;rsquo;s engagement with the application, or indeed increase efficacy. In a first step towards answering these questions, we conducted a study where a Virtual Learning Environment was used to examine the effect of employing a virtual character to deliver a lesson. In order to investigate whether increased familiarity between the player and the character would help achieve learning outcomes, we allowed participants to customize the physical appearance of the character. We used direct and indirect measures to assess engagement and learning; we measured knowledge retention to ascertain learning via a test at the end of the lesson, and also measured participants&amp;rsquo; perceived engagement with the lesson. Our findings show that a virtual character can be an effective learning aid, causing heightened engagement and retention of knowledge. However, allowing participants to customize character appearance resulted in inhibited engagement, which was contrary to what we expected.&lt;/p&gt;

&lt;h2 id=&#34;key-findings&#34;&gt;Key Findings&lt;/h2&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Enhanced Engagement&lt;/strong&gt;: Users showed significantly higher engagement when lessons were delivered through a virtual character compared to non-character controls.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;strong&gt;Improved Knowledge Retention&lt;/strong&gt;: Virtual character delivery resulted in better knowledge retention compared to traditional presentation methods.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;strong&gt;Attention Focus&lt;/strong&gt;: User attention was predominantly directed at the virtual character, with limited attention to other environmental elements.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;strong&gt;Customization Paradox&lt;/strong&gt;: Contrary to expectations, character appearance personalization led to significantly lower engagement compared to default appearances.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&#34;relevance-to-thesis&#34;&gt;Relevance to Thesis&lt;/h2&gt;

&lt;p&gt;This work laid important groundwork for understanding how virtual characters affect user engagement and learning. While this paper focused on static virtual characters in educational contexts, it motivated the later development of RLAnimate for creating more dynamic, responsive virtual characters that can adapt their behaviors in real-time - addressing some of the limitations identified in this study.&lt;/p&gt;

&lt;h2 id=&#34;links&#34;&gt;Links&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://dl.acm.org/doi/10.1145/3274247.3274499&#34; target=&#34;_blank&#34;&gt;Paper PDF (ACM Digital Library)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://vihanga.github.io/post/research/MIG18PaperStudy/&#34; target=&#34;_blank&#34;&gt;Conference presentation thoughts and feedback&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://vihanga.github.io/post/blog/MIG18blog/&#34; target=&#34;_blank&#34;&gt;MIG&amp;rsquo;18 conference blog post&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://vihanga.github.io/thesis/&#34; target=&#34;_blank&#34;&gt;Back to thesis&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Why AI is an opportunity rather than a danger</title>
      <link>https://vihanga.github.io/post/blog/AI-Oppurtunity-not-danger/</link>
      <pubDate>Sat, 17 Aug 2019 17:02:59 +0000</pubDate>
      
      <guid>https://vihanga.github.io/post/blog/AI-Oppurtunity-not-danger/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;https://www.rte.ie/brainstorm/2019/0801/1066421-why-ai-is-an-opportunity-rather-than-a-danger/&#34; target=&#34;_blank&#34;&gt;Originally published on RTÉ Brainstorm&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;The solar system is full of debris and rocks floating around. A meteor shower or a shooting star is debris burning up upon entry to the atmosphere. On occasion, pieces of rock make it all the way through. A much larger rock that could make its way through is an asteroid. About 66 million years ago, the impact from an asteroid brought about the Cretaceous extinction. A repetition of this event is theoretically possible - and the same could be said about all-powerful Artificial Intelligence Overlords marginalising the human race.&lt;/p&gt;

&lt;p&gt;Since its inception in the mid 20th century, the field of artificial intelligence has had an interesting ride. Much like the financial markets or Hollywood, there’s been breakthroughs and failures and booms and busts. Driven by advances in deep learning, a great deal of success has been achieved this decade, ushering in the newest AI golden age, along with a frenzy of interest and investment.&lt;/p&gt;

&lt;p&gt;The current popular views of AI are being shaped primarily by several vocal figures in technology. Microsoft founder Bill Gates has said that AI carries the potential to change society deeply and is both exciting and dangerous. IBM CEO Ginni Rometty has stated that she believes advances in AI will create different jobs rather than no jobs. Facebook founder Mark Zuckerberg has expressed the view that AI will deliver various improvements to human quality of life, while Tesla founder Elon Musk has called AI &amp;ldquo;a fundamental risk to the existence of human civilisation&amp;rdquo;.&lt;/p&gt;

&lt;p&gt;The future dangers AI could pose to humanity is a frequently occurring subject in conversation, be it to break the ice on a first date, to fill the time before a meeting starts or while waiting for a pint at the pub on a Friday evening. And arguably, Musk&amp;rsquo;s statement carries the most bite. It certainly is a very catchy line that makes for a more topical conversation starter than asteroids or the Cretaceous extinction.&lt;/p&gt;

&lt;p&gt;As a result, these hypothetical yet sensationalist doomsday opinions are at a point where they dominate the conversation about AI, so much so that the bigger picture on the subject may at the point of being overshadowed and given very little consideration. And that could pose an even greater threat, especially in an age where hype and rhetoric without regard for fact has had a great effect on the fate of the world.&lt;/p&gt;

&lt;p&gt;Advancements in AI could lead to a superintelligence that could pose a fundamental risk to human existence - and so could climate change or a 5 km-wide space rock colliding with the Earth. But there are many points of difference between these three risks. AI is a long way away advancing to the point of superintelligence and it’s not a certainty that superintelligence would lead to the end of humanity. Climate change is a very real and present danger, with the time to take action to prevent irreversible consequences continuing to decrease. And then there are the chances of an asteroid striking the earth.&lt;/p&gt;

&lt;p&gt;Of course, there is no good that can come from an asteroid strike or global warming, but AI can literally be one of the best things to happen to humans. This is not said enough - and it can’t be said enough. AI can and is being used to as a tool in many incredible ways. For example, it is being used to help improve prevention of cancer, provide effective treatment, and could one day even be part of the solution.&lt;/p&gt;

&lt;p&gt;AI is also being leveraged as a promising way to combat climate change. AI-powered simulations and analysis helps make for sustainable urban planning and ocean preservation. It is possible to make more accurate predictions regarding weather events with the help of AI leading to reduced damage to life and property. It can help power grids to be more energy efficient by predicting peak over of usage allowing for better preparation and enhances the benefits of clean energy methods by deploying in ways such as to incorporate weather and other relevant data to make wind turbines more efficient.&lt;/p&gt;

&lt;p&gt;And yet arguments on how AI could hypothetically end humanity as we know it at some point in the future, seem to drown out how AI is already helping humanity’s cause. Such contributions pale in comparison to the very real possibility that AI could be the key to finding the answers for some of humanity’s biggest problems.&lt;/p&gt;

&lt;p&gt;At a 2018 hearing at the US House of Representatives Committee on Science, Space and Technology, Stanford Professor Fei-Fei Li said that AI is bound to alter the human experience, and not necessarily for the better. After acknowledging this, Li pointed out that ensuring AI will transform the world for the better is very much in our hands. &amp;ldquo;There&amp;rsquo;s nothing artificial about AI&amp;rdquo;, she said.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;&amp;ldquo;It’s inspired by people; it’s created by people, and—most importantly—it impacts people. It is a powerful tool we are only just beginning to understand, and that is a profound responsibility.&amp;rdquo;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Concern that AI could pose a threat is very much reasonable. But this possibility is just that - a possibility. Let’s not forget AI is a fundamental opportunity to further the cause of the human civilisation, and that it is in our hands to ensure that it impacts us positively, and be optimistic about what it can mean for us. The world can only be the better place for it.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
