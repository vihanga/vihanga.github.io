<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Vihanga Gamage&#39;s Corner of the World Wide Web on Vihanga Gamage&#39;s Corner of the World Wide Web</title>
    <link>https://vihanga.github.io/</link>
    <description>Recent content in Vihanga Gamage&#39;s Corner of the World Wide Web on Vihanga Gamage&#39;s Corner of the World Wide Web</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>&amp;copy; 2018</copyright>
    <lastBuildDate>Wed, 22 Jan 2025 00:00:00 +0000</lastBuildDate>
    <atom:link href="/" rel="self" type="application/rss+xml" />
    
    <item>
      <title>Chapter 1: Introduction</title>
      <link>https://vihanga.github.io/thesis/chapters/introduction/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://vihanga.github.io/thesis/chapters/introduction/</guid>
      <description>

&lt;h1 id=&#34;introduction&#34;&gt;Introduction&lt;/h1&gt;

&lt;p&gt;This thesis tackles a fundamental challenge in computer animation: how can we create virtual characters that move naturally and responsively without the prohibitive costs of motion capture?&lt;/p&gt;

&lt;h2 id=&#34;the-problem&#34;&gt;The Problem&lt;/h2&gt;

&lt;p&gt;Traditional character animation relies heavily on motion capture - an expensive, time-consuming process that requires recording human actors for every possible behavior variation. While this produces high-quality results, it&amp;rsquo;s simply not scalable for interactive applications where characters need to respond dynamically to user input or changing scenarios.&lt;/p&gt;

&lt;p&gt;Meanwhile, existing alternatives have significant limitations. Supervised learning methods can generate animations but lack flexibility. Physics-based reinforcement learning works brilliantly for athletic movements but fails for social behaviors like gestures - there&amp;rsquo;s no physics simulation to tell us if a wave looks &amp;ldquo;friendly&amp;rdquo; or a gesture feels &amp;ldquo;natural.&amp;rdquo;&lt;/p&gt;

&lt;h2 id=&#34;my-approach&#34;&gt;My Approach&lt;/h2&gt;

&lt;p&gt;I developed RLAnimate, a framework that fundamentally rethinks how we approach character animation. Instead of using reward functions (the traditional RL approach), I use motion capture data as learning objectives. This allows agents to learn what &amp;ldquo;human-like&amp;rdquo; means directly from examples, while maintaining the flexibility and responsiveness that makes RL powerful.&lt;/p&gt;

&lt;p&gt;The key insight is treating animation as a model-based reinforcement learning problem where agents learn the dynamics of human movement. By understanding how joint rotations create behaviors, agents can generate novel animations that look natural while adapting to changing requirements in real-time.&lt;/p&gt;

&lt;h2 id=&#34;key-contributions&#34;&gt;Key Contributions&lt;/h2&gt;

&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;A new paradigm for animation RL&lt;/strong&gt; - Using motion capture data as objectives rather than rewards&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Multi-behavior agents&lt;/strong&gt; - Single agents that can wave, point, and generate conversational gestures&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Technical innovations&lt;/strong&gt; - Quaternion neural networks, realism regularization, and hierarchical dynamics models&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Proven results&lt;/strong&gt; - Animations that are statistically indistinguishable from human motion capture&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&#34;research-questions&#34;&gt;Research Questions&lt;/h2&gt;

&lt;p&gt;The thesis systematically addresses four key questions:
- Can RL work for animation without physics? (Yes - through latent dynamics learning)
- Can we make it human-like? (Yes - using motion capture as objectives)
- How do we handle complexity like fingers? (Quaternions + dedicated animation dynamics)
- Can it work for conversational gestures? (Yes - with realism regularization)&lt;/p&gt;

&lt;h2 id=&#34;thesis-journey&#34;&gt;Thesis Journey&lt;/h2&gt;

&lt;p&gt;The work progresses from establishing basic feasibility (Chapter 4) through developing the core RLAnimate methodology (Chapter 5), extending to complex movements with fingers (Chapter 6), and culminating in conversational beat gestures that match human quality (Chapter 7).&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Chapter 2: Virtual Character Animation and Perception</title>
      <link>https://vihanga.github.io/thesis/chapters/data-driven-animation/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://vihanga.github.io/thesis/chapters/data-driven-animation/</guid>
      <description>

&lt;h1 id=&#34;virtual-character-animation-and-perception&#34;&gt;Virtual Character Animation and Perception&lt;/h1&gt;

&lt;p&gt;This chapter establishes the foundation for understanding virtual character animation - both the technical challenges and the perceptual requirements that make animation &amp;ldquo;feel right&amp;rdquo; to human viewers.&lt;/p&gt;

&lt;h2 id=&#34;the-animation-pipeline&#34;&gt;The Animation Pipeline&lt;/h2&gt;

&lt;p&gt;Creating believable virtual characters isn&amp;rsquo;t just about making them move - it&amp;rsquo;s a complex pipeline that starts with 3D modeling and ends with real-time rendering. I explore how animation data flows through this pipeline, from motion capture or procedural generation to the final pixels on screen. The key insight? Each step introduces opportunities for both quality improvements and computational optimizations.&lt;/p&gt;

&lt;h2 id=&#34;why-perception-matters&#34;&gt;Why Perception Matters&lt;/h2&gt;

&lt;p&gt;Here&amp;rsquo;s what most animation systems miss: humans are incredibly sensitive to movement patterns. We can spot &amp;ldquo;unnatural&amp;rdquo; motion in milliseconds, even if we can&amp;rsquo;t articulate why. I dive into the perceptual studies that reveal what makes animation feel human-like - from timing and smoothness to the subtle coordination between body parts.&lt;/p&gt;

&lt;p&gt;This understanding drives a crucial design decision in my work: animation quality isn&amp;rsquo;t just about technical metrics like joint angles or velocities. It&amp;rsquo;s about matching the statistical patterns of human movement that our brains have evolved to recognize.&lt;/p&gt;

&lt;h2 id=&#34;current-approaches-and-their-limits&#34;&gt;Current Approaches and Their Limits&lt;/h2&gt;

&lt;p&gt;The field has tried three main approaches:&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Motion Graphs and Blending&lt;/strong&gt;: Works well for predetermined scenarios but lacks flexibility. You need exponentially more data as behaviors become more complex.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Neural Networks&lt;/strong&gt;: Can generate smooth animations but often produce &amp;ldquo;average&amp;rdquo; motions that lack character and specificity. They struggle with rare behaviors and edge cases.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Physics-based RL&lt;/strong&gt;: Produces incredibly athletic movements - backflips, martial arts, parkour. But try to make a physics-based agent wave &amp;ldquo;friendly&amp;rdquo; and you&amp;rsquo;ll see the problem. There&amp;rsquo;s no physics equation for social appropriateness.&lt;/p&gt;

&lt;h2 id=&#34;the-gap-this-thesis-fills&#34;&gt;The Gap This Thesis Fills&lt;/h2&gt;

&lt;p&gt;What&amp;rsquo;s missing is a method that combines the flexibility of RL with the quality of motion capture data, without requiring physics simulation. That&amp;rsquo;s exactly what RLAnimate provides - and understanding this background shows why that combination is so powerful.&lt;/p&gt;

&lt;p&gt;The chapter sets up the key tension: we want animation that&amp;rsquo;s both high-quality (matching human perception) and flexible (responding to dynamic scenarios). Traditional methods excel at one or the other, but not both. This creates the perfect motivation for the model-based RL approach I develop in the following chapters.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Chapter 3: Model-based Reinforcement Learning</title>
      <link>https://vihanga.github.io/thesis/chapters/model-based-rl/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://vihanga.github.io/thesis/chapters/model-based-rl/</guid>
      <description>

&lt;h1 id=&#34;model-based-reinforcement-learning&#34;&gt;Model-based Reinforcement Learning&lt;/h1&gt;

&lt;p&gt;This chapter provides the theoretical foundation for understanding how model-based RL can revolutionize character animation. I explain why learning a model of the world enables more sample-efficient and generalizable agents compared to traditional approaches.&lt;/p&gt;

&lt;h2 id=&#34;the-power-of-world-models&#34;&gt;The Power of World Models&lt;/h2&gt;

&lt;p&gt;Most RL systems are model-free - they learn through trial and error what actions lead to good outcomes. That works great if you have millions of attempts to practice. But what if you need to learn from limited data? That&amp;rsquo;s where model-based RL shines.&lt;/p&gt;

&lt;p&gt;By learning a model of how the world works - in our case, how human bodies move - agents can plan ahead, imagine consequences, and generalize from limited examples. It&amp;rsquo;s the difference between memorizing dance moves and understanding the principles of movement.&lt;/p&gt;

&lt;h2 id=&#34;latent-dynamics-the-secret-sauce&#34;&gt;Latent Dynamics: The Secret Sauce&lt;/h2&gt;

&lt;p&gt;Here&amp;rsquo;s the key insight that makes RLAnimate work: we don&amp;rsquo;t need to model every muscle and bone. Instead, we learn compact &amp;ldquo;latent&amp;rdquo; representations that capture the essence of movement patterns.&lt;/p&gt;

&lt;p&gt;I introduce the mathematical framework of latent dynamics models, showing how they compress high-dimensional movement data into manageable representations while preserving the critical information for generating natural motion. This is what allows our agents to run at 5ms per frame - we&amp;rsquo;re working in an efficient latent space, not raw joint angles.&lt;/p&gt;

&lt;h2 id=&#34;representation-learning-and-realism&#34;&gt;Representation Learning and Realism&lt;/h2&gt;

&lt;p&gt;The chapter explores how to learn good representations for animation. Not all latent spaces are created equal - some preserve the nuances that make motion feel human, others lose them. I examine techniques from variational autoencoders to adversarial learning, setting up the theoretical foundation for the realism regularization used in later chapters.&lt;/p&gt;

&lt;h2 id=&#34;why-model-based-for-animation&#34;&gt;Why Model-based for Animation?&lt;/h2&gt;

&lt;p&gt;The chapter concludes by connecting these abstract concepts to character animation:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Sample Efficiency&lt;/strong&gt;: Learn new behaviors from just a few motion capture examples&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Generalization&lt;/strong&gt;: Interpolate and extrapolate beyond the training data&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Planning&lt;/strong&gt;: Generate coherent long-term motion sequences&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Speed&lt;/strong&gt;: Efficient latent representations enable real-time performance&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;This theoretical grounding is essential for understanding why RLAnimate succeeds where other methods fail. Model-based RL isn&amp;rsquo;t just another technique - it&amp;rsquo;s fundamentally the right abstraction for the animation problem.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Chapter 4: Model-based Character Animation</title>
      <link>https://vihanga.github.io/thesis/chapters/model-based-animation/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://vihanga.github.io/thesis/chapters/model-based-animation/</guid>
      <description>

&lt;h1 id=&#34;model-based-character-animation&#34;&gt;Model-based Character Animation&lt;/h1&gt;

&lt;p&gt;This chapter marks the beginning of my core contributions - demonstrating that model-based RL can work for character animation without physics simulation. It&amp;rsquo;s the proof-of-concept that sets the stage for everything that follows.&lt;/p&gt;

&lt;h2 id=&#34;the-experiment-that-started-it-all&#34;&gt;The Experiment That Started It All&lt;/h2&gt;

&lt;p&gt;I started with a simple question: can an RL agent learn to animate a character using only a learned model of movement dynamics? No physics engine, no hand-crafted rewards - just data and learning.&lt;/p&gt;

&lt;p&gt;The answer turned out to be a resounding yes, but with important caveats that shaped the rest of my research.&lt;/p&gt;

&lt;h2 id=&#34;star-jumps-a-perfect-test-case&#34;&gt;Star Jumps: A Perfect Test Case&lt;/h2&gt;

&lt;p&gt;Why star jumps? They&amp;rsquo;re deceptively complex - requiring coordination of arms and legs, maintaining balance, and producing a recognizable pattern. Yet they&amp;rsquo;re simple enough to analyze rigorously. This choice let me focus on the core challenge: can model-based RL work for animation at all?&lt;/p&gt;

&lt;p&gt;Using just 5 minutes of motion capture data, I trained agents that could:
- Generate continuous star jump animations
- Maintain synchronization between limbs
- Adapt to different speeds and styles
- Run in real-time (under 5ms per frame)&lt;/p&gt;

&lt;h2 id=&#34;model-free-vs-model-based-the-showdown&#34;&gt;Model-free vs Model-based: The Showdown&lt;/h2&gt;

&lt;p&gt;The chapter presents a direct comparison between model-free (PPO) and model-based (Dreamer) approaches. The results were eye-opening:&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Model-free&lt;/strong&gt;: Achieved the objective but produced unnatural, robotic movements. The agent found shortcuts that technically worked but looked wrong to human eyes.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Model-based&lt;/strong&gt;: Produced natural-looking animations that closely matched the motion capture data. The learned dynamics model acted as an implicit regularizer, keeping movements within the manifold of human motion.&lt;/p&gt;

&lt;h2 id=&#34;key-insights&#34;&gt;Key Insights&lt;/h2&gt;

&lt;p&gt;This early work revealed three critical insights that guided the rest of the thesis:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Dynamics models are regularizers&lt;/strong&gt;: By learning how humans actually move, the model prevents unnatural animations&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Latent planning works&lt;/strong&gt;: Planning in a learned latent space is both efficient and effective&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Physics isn&amp;rsquo;t necessary&lt;/strong&gt;: For many animation tasks, learned dynamics are sufficient&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&#34;setting-the-stage&#34;&gt;Setting the Stage&lt;/h2&gt;

&lt;p&gt;While successful, this chapter also exposed limitations - single-behavior agents, no interaction with physics, limited complexity. These limitations motivated the innovations in subsequent chapters, but the core finding stands: model-based RL is a viable paradigm for character animation.&lt;/p&gt;

&lt;p&gt;The supplementary material shows these agents in action, demonstrating that even this early prototype could produce animations that feel genuinely human-like.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Chapter 5: RLAnimate - Data-driven RL for Character Animation</title>
      <link>https://vihanga.github.io/thesis/chapters/data-driven-rl/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://vihanga.github.io/thesis/chapters/data-driven-rl/</guid>
      <description>

&lt;h1 id=&#34;rlanimate-data-driven-rl-for-character-animation&#34;&gt;RLAnimate: Data-driven RL for Character Animation&lt;/h1&gt;

&lt;p&gt;This is where everything comes together. Chapter 5 introduces RLAnimate - my core framework that revolutionizes how we think about RL for animation. Instead of rewards, we use motion capture data as objectives. It&amp;rsquo;s a simple idea with profound implications.&lt;/p&gt;

&lt;h2 id=&#34;the-breakthrough-objectives-not-rewards&#34;&gt;The Breakthrough: Objectives, Not Rewards&lt;/h2&gt;

&lt;p&gt;Traditional RL for animation struggles with a fundamental problem: how do you write a reward function for &amp;ldquo;wave friendly&amp;rdquo; or &amp;ldquo;point naturally&amp;rdquo;? You can&amp;rsquo;t. Previous work tried complex reward engineering, but it always felt hacky.&lt;/p&gt;

&lt;p&gt;My insight: we don&amp;rsquo;t need rewards at all. We have motion capture data showing humans performing these behaviors. So instead of rewarding specific outcomes, I train agents to match the dynamics of human movement. The &amp;ldquo;ideal action&amp;rdquo; at each timestep is simply what a human would do.&lt;/p&gt;

&lt;h2 id=&#34;the-a1-architecture&#34;&gt;The A1 Architecture&lt;/h2&gt;

&lt;p&gt;RLAnimate&amp;rsquo;s first architecture (A1) demonstrates the concept:
- &lt;strong&gt;Behavior encoders&lt;/strong&gt; that understand what motion the agent should perform
- &lt;strong&gt;Perceptual encoders&lt;/strong&gt; that process the current body state
- &lt;strong&gt;Latent dynamics&lt;/strong&gt; that model how movements unfold over time
- &lt;strong&gt;Multi-behavior support&lt;/strong&gt; - one agent, multiple skills&lt;/p&gt;

&lt;p&gt;The results speak for themselves: agents that can wave, point, and transition between behaviors smoothly, all while maintaining human-like quality.&lt;/p&gt;

&lt;h2 id=&#34;multi-behavior-magic&#34;&gt;Multi-Behavior Magic&lt;/h2&gt;

&lt;p&gt;Previous systems needed separate agents for each behavior. RLAnimate changes that. By learning shared representations of movement dynamics, a single agent can:
- Wave with different styles and speeds
- Point at moving targets accurately
- Blend behaviors naturally
- Generalize to new variations&lt;/p&gt;

&lt;p&gt;This isn&amp;rsquo;t just convenient - it&amp;rsquo;s essential for interactive applications where characters need to respond dynamically.&lt;/p&gt;

&lt;h2 id=&#34;the-numbers-that-matter&#34;&gt;The Numbers That Matter&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;5ms inference time&lt;/strong&gt;: Fast enough for any real-time application&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;93% behavior accuracy&lt;/strong&gt;: Correctly performs requested behaviors&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Statistical parity&lt;/strong&gt;: Movement statistics match human motion capture&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Efficient training&lt;/strong&gt;: Learn from minutes, not hours, of mocap data&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;a2-scaling-up&#34;&gt;A2: Scaling Up&lt;/h2&gt;

&lt;p&gt;The chapter also introduces A2, which adds physical grounding:
- Foot contacts that respect physics
- Momentum preservation during movement
- Interaction with virtual objects
- All while maintaining the ease of training&lt;/p&gt;

&lt;h2 id=&#34;why-this-changes-everything&#34;&gt;Why This Changes Everything&lt;/h2&gt;

&lt;p&gt;RLAnimate isn&amp;rsquo;t just another animation method - it&amp;rsquo;s a new paradigm. By treating animation as matching human movement dynamics rather than optimizing rewards, we get:
1. Natural quality without reward engineering
2. Multi-behavior flexibility
3. Practical training times
4. Real-time performance&lt;/p&gt;

&lt;p&gt;The supplementary materials showcase these agents in action, demonstrating capabilities that simply weren&amp;rsquo;t possible before. This is the foundation that enables everything in the following chapters - from finger animation to conversational gestures.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Chapter 6: Quaternions and Finger Animation</title>
      <link>https://vihanga.github.io/thesis/chapters/latent-dynamics/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://vihanga.github.io/thesis/chapters/latent-dynamics/</guid>
      <description>

&lt;h1 id=&#34;quaternions-and-finger-animation&#34;&gt;Quaternions and Finger Animation&lt;/h1&gt;

&lt;p&gt;Chapter 6 tackles one of animation&amp;rsquo;s nastiest problems: fingers. With 30+ joints that move in complex, coordinated patterns, finger animation has long been the bane of procedural methods. My solution? Quaternions and a dedicated animation dynamics model.&lt;/p&gt;

&lt;h2 id=&#34;why-fingers-matter-and-why-they-re-hard&#34;&gt;Why Fingers Matter (And Why They&amp;rsquo;re Hard)&lt;/h2&gt;

&lt;p&gt;Watch someone wave - their fingers aren&amp;rsquo;t static. They curl, extend, and move with subtle patterns that make the gesture feel alive. Miss these details and even perfect arm movements feel robotic.&lt;/p&gt;

&lt;p&gt;The challenge is combinatorial explosion. Each finger has multiple joints, each with constraints, and they all need to coordinate. Traditional methods either ignore fingers entirely or produce uncanny &amp;ldquo;spider hands&amp;rdquo; that distract from otherwise good animation.&lt;/p&gt;

&lt;h2 id=&#34;the-quaternion-revolution&#34;&gt;The Quaternion Revolution&lt;/h2&gt;

&lt;p&gt;The first breakthrough was switching from Euler angles to quaternions. This isn&amp;rsquo;t just a mathematical nicety - it fundamentally changes what the neural networks can learn:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;No gimbal lock&lt;/strong&gt;: Fingers can rotate freely without singularities&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Smooth interpolation&lt;/strong&gt;: Natural transitions between poses&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Efficient computation&lt;/strong&gt;: 4 parameters instead of 9 for rotation matrices&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Better gradients&lt;/strong&gt;: Networks train more stably&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;I developed a custom quaternion activation function that guarantees valid unit quaternions, eliminating a whole class of animation artifacts.&lt;/p&gt;

&lt;h2 id=&#34;a3-the-animation-dynamics-architecture&#34;&gt;A3: The Animation Dynamics Architecture&lt;/h2&gt;

&lt;p&gt;The real innovation is the A3 architecture&amp;rsquo;s animation dynamics model. Instead of trying to force finger movements into the same model as body dynamics, A3 uses:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Dedicated animation dynamics&lt;/strong&gt;: A separate latent model just for animation generation&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Hierarchical dependencies&lt;/strong&gt;: Fingers know what the wrist is doing&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Stochastic variation&lt;/strong&gt;: Natural movement isn&amp;rsquo;t perfectly deterministic&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;This separation is key. Physical dynamics (for the body) and animation dynamics (for detailed movements) have different requirements. By acknowledging this, A3 achieves both physical plausibility and expressive detail.&lt;/p&gt;

&lt;h2 id=&#34;the-results-speak-or-rather-wave&#34;&gt;The Results Speak (Or Rather, Wave)&lt;/h2&gt;

&lt;p&gt;The numbers tell the story:
- &lt;strong&gt;5.34 point improvement&lt;/strong&gt; in animation quality over A2
- &lt;strong&gt;All velocity errors under 0.4&lt;/strong&gt; - below human perceptual threshold
- &lt;strong&gt;30+ additional joints&lt;/strong&gt; handled without performance degradation
- &lt;strong&gt;Same 5ms inference time&lt;/strong&gt; despite the added complexity&lt;/p&gt;

&lt;p&gt;But watching the animations tells it better. A3 agents wave with natural finger curls, point with appropriate hand shapes, and maintain hand poses that feel human.&lt;/p&gt;

&lt;h2 id=&#34;technical-contributions&#34;&gt;Technical Contributions&lt;/h2&gt;

&lt;p&gt;Beyond the practical results, this chapter contributes:
1. &lt;strong&gt;Quaternion neural networks&lt;/strong&gt; for animation
2. &lt;strong&gt;Hierarchical latent dynamics&lt;/strong&gt; modeling
3. &lt;strong&gt;Animation-specific dynamics&lt;/strong&gt; separate from physics
4. &lt;strong&gt;Proof that detail doesn&amp;rsquo;t require sacrificing speed&lt;/strong&gt;&lt;/p&gt;

&lt;h2 id=&#34;why-this-matters&#34;&gt;Why This Matters&lt;/h2&gt;

&lt;p&gt;Fingers might seem like a detail, but they&amp;rsquo;re symptomatic of a larger challenge: how do we handle the full complexity of human movement? A3 shows that with the right representations and architecture, we can capture fine details while maintaining real-time performance.&lt;/p&gt;

&lt;p&gt;This sets the stage for Chapter 7&amp;rsquo;s ultimate test - conversational gestures where every detail matters for believability.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Chapter 7: Beat Gestures - The Ultimate Test</title>
      <link>https://vihanga.github.io/thesis/chapters/conversational-gestures/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://vihanga.github.io/thesis/chapters/conversational-gestures/</guid>
      <description>

&lt;h1 id=&#34;beat-gestures-the-ultimate-test&#34;&gt;Beat Gestures - The Ultimate Test&lt;/h1&gt;

&lt;p&gt;This chapter represents the culmination of everything - applying RLAnimate to conversational beat gestures. It&amp;rsquo;s the hardest test possible: can RL-generated animations be indistinguishable from real human gestures? The answer, validated through rigorous perceptual studies, is yes.&lt;/p&gt;

&lt;h2 id=&#34;why-beat-gestures-are-different&#34;&gt;Why Beat Gestures Are Different&lt;/h2&gt;

&lt;p&gt;Beat gestures - those rhythmic hand movements we make while speaking - are uniquely challenging:
- They&amp;rsquo;re not goal-directed (unlike pointing or waving)
- They must synchronize with speech prosody
- They require subtle variety to feel natural
- There&amp;rsquo;s no &amp;ldquo;correct&amp;rdquo; gesture - only human-like ones&lt;/p&gt;

&lt;p&gt;This is where physics-based RL completely fails. There&amp;rsquo;s no physics objective for &amp;ldquo;emphasize this word appropriately.&amp;rdquo; Yet these gestures are crucial for believable virtual humans.&lt;/p&gt;

&lt;h2 id=&#34;the-a4-architecture-realism-first&#34;&gt;The A4 Architecture: Realism First&lt;/h2&gt;

&lt;p&gt;A4 builds on A3 but adds my most important innovation: realism regularization. Four components work together:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Rotation regularization&lt;/strong&gt;: Prevents unnatural joint configurations&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Physics regularization&lt;/strong&gt;: Respects momentum and energy conservation&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Smoothness regularization&lt;/strong&gt;: Eliminates jittery movements&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Adversarial regularization&lt;/strong&gt;: Learned realism from data&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;The key insight: by combining multiple forms of regularization, we achieve robustness. Any single method might fail, but together they create a strong prior for human-like movement.&lt;/p&gt;

&lt;h2 id=&#34;the-perceptual-study-proof-in-the-pudding&#34;&gt;The Perceptual Study: Proof in the Pudding&lt;/h2&gt;

&lt;p&gt;I didn&amp;rsquo;t just claim success - I proved it. 24 participants watched 480 video clips comparing:
- RLAnimate (A4) gestures
- Gesticulator (state-of-the-art supervised learning)
- Real motion capture&lt;/p&gt;

&lt;p&gt;The results were stunning:
- &lt;strong&gt;No significant difference&lt;/strong&gt; between RLAnimate and motion capture (p &amp;gt; 0.31)
- &lt;strong&gt;Significant improvement&lt;/strong&gt; over Gesticulator (p &amp;lt; 0.001)
- Participants literally couldn&amp;rsquo;t tell our animations from real humans&lt;/p&gt;

&lt;h2 id=&#34;technical-innovations&#34;&gt;Technical Innovations&lt;/h2&gt;

&lt;p&gt;Beyond the perceptual success, A4 contributes several technical advances:&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Speech-motion synchronization&lt;/strong&gt;: A hierarchical encoder that processes speech at multiple timescales, capturing both phoneme-level detail and phrase-level rhythm.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Stochastic generation&lt;/strong&gt;: The same speech produces varied but appropriate gestures, just like humans.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Style control&lt;/strong&gt;: Implicit style learning allows generating gestures in different &amp;ldquo;personalities.&amp;rdquo;&lt;/p&gt;

&lt;h2 id=&#34;the-bigger-picture&#34;&gt;The Bigger Picture&lt;/h2&gt;

&lt;p&gt;This chapter proves something profound: RL can match human quality for the most subjective, culturally-dependent aspects of movement. We&amp;rsquo;re not just animating characters - we&amp;rsquo;re capturing the essence of human nonverbal communication.&lt;/p&gt;

&lt;p&gt;The implications extend beyond animation:
- Virtual assistants that gesture naturally
- More engaging educational avatars
- Believable characters for therapy and training
- A new baseline for what &amp;ldquo;human-like&amp;rdquo; means in AI&lt;/p&gt;

&lt;h2 id=&#34;why-this-matters&#34;&gt;Why This Matters&lt;/h2&gt;

&lt;p&gt;Beat gestures are the Turing test of character animation. They&amp;rsquo;re purely about human perception - there&amp;rsquo;s no objective metric, no physics to simulate, just the question: does this feel right?&lt;/p&gt;

&lt;p&gt;That RLAnimate passes this test validates the entire approach. Motion capture as objectives, model-based learning, realism regularization - it all comes together to create something indistinguishable from human movement.&lt;/p&gt;

&lt;p&gt;The supplementary material contains all 24 video stimuli from the perceptual study. Watch them yourself - can you tell which ones are generated?&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Chapter 8: Conclusions and Future Impact</title>
      <link>https://vihanga.github.io/thesis/chapters/conclusion/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://vihanga.github.io/thesis/chapters/conclusion/</guid>
      <description>

&lt;h1 id=&#34;conclusions-and-future-impact&#34;&gt;Conclusions and Future Impact&lt;/h1&gt;

&lt;p&gt;This thesis fundamentally changes how we think about character animation. By treating animation as a model-based RL problem with motion capture objectives, I&amp;rsquo;ve created a system that produces human-quality movement at real-time speeds. But this is just the beginning.&lt;/p&gt;

&lt;h2 id=&#34;what-we-ve-achieved&#34;&gt;What We&amp;rsquo;ve Achieved&lt;/h2&gt;

&lt;p&gt;RLAnimate represents several breakthrough achievements:&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Human-quality animation from RL&lt;/strong&gt;: For the first time, RL agents generate movement that humans can&amp;rsquo;t distinguish from motion capture. This isn&amp;rsquo;t incremental improvement - it&amp;rsquo;s a paradigm shift.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Real-time performance&lt;/strong&gt;: 5ms per frame means these aren&amp;rsquo;t just research demos. They&amp;rsquo;re ready for games, VR, and interactive applications today.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Multi-behavior flexibility&lt;/strong&gt;: Single agents that wave, point, and gesture naturally - no behavior-specific engineering required.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Scalability to complexity&lt;/strong&gt;: From simple waves to 30+ finger joints to speech-synchronized gestures, the approach scales.&lt;/p&gt;

&lt;h2 id=&#34;the-technical-revolution&#34;&gt;The Technical Revolution&lt;/h2&gt;

&lt;p&gt;Three key innovations make this possible:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Motion capture as objectives&lt;/strong&gt;: Eliminating reward engineering by learning directly from human examples&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Latent dynamics models&lt;/strong&gt;: Efficient representations that capture movement essence&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Realism regularization&lt;/strong&gt;: Multiple complementary methods ensuring human-like quality&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Together, these create a framework that&amp;rsquo;s both theoretically principled and practically effective.&lt;/p&gt;

&lt;h2 id=&#34;why-this-matters-beyond-animation&#34;&gt;Why This Matters Beyond Animation&lt;/h2&gt;

&lt;p&gt;The implications extend far beyond making pretty animations:&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;For AI&lt;/strong&gt;: We&amp;rsquo;ve shown that complex, subjective human behaviors can be learned without explicit programming. The approach could apply to any domain where we have examples but can&amp;rsquo;t write rules.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;For HCI&lt;/strong&gt;: Natural movement is crucial for acceptance of virtual agents. This work enables a new generation of interfaces that communicate through body language.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;For Science&lt;/strong&gt;: By learning what makes movement &amp;ldquo;human-like,&amp;rdquo; we&amp;rsquo;re gaining insights into human motor control and social signaling.&lt;/p&gt;

&lt;h2 id=&#34;limitations-and-honesty&#34;&gt;Limitations and Honesty&lt;/h2&gt;

&lt;p&gt;No system is perfect. Current limitations include:
- Training requires motion capture data (though much less than alternatives)
- Style control is implicit rather than parametric
- Physical interactions remain challenging
- Cultural gesture variations need more exploration&lt;/p&gt;

&lt;p&gt;These aren&amp;rsquo;t fundamental barriers - they&amp;rsquo;re the next research challenges.&lt;/p&gt;

&lt;h2 id=&#34;the-road-ahead&#34;&gt;The Road Ahead&lt;/h2&gt;

&lt;p&gt;This thesis opens several exciting directions:&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Behavioral complexity&lt;/strong&gt;: Extending to full-body social interactions, emotional expressions, and context-aware responses.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Zero-shot generalization&lt;/strong&gt;: Learning movement principles that transfer across characters and scenarios without retraining.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Integration with LLMs&lt;/strong&gt;: Imagine language models that don&amp;rsquo;t just speak but move naturally as they communicate.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Real-world robotics&lt;/strong&gt;: The principles could enable robots that move in ways humans find natural and non-threatening.&lt;/p&gt;

&lt;h2 id=&#34;a-personal-vision&#34;&gt;A Personal Vision&lt;/h2&gt;

&lt;p&gt;I believe we&amp;rsquo;re at an inflection point. Just as deep learning revolutionized computer vision, model-based RL with human objectives will revolutionize character animation. We&amp;rsquo;re moving from &amp;ldquo;making characters move&amp;rdquo; to &amp;ldquo;bringing characters to life.&amp;rdquo;&lt;/p&gt;

&lt;p&gt;The tools are here. The methods work. What we create with them - more engaging games, more effective education, more natural human-computer interaction - is limited only by imagination.&lt;/p&gt;

&lt;h2 id=&#34;final-thoughts&#34;&gt;Final Thoughts&lt;/h2&gt;

&lt;p&gt;This thesis asked whether RL could create human-like animation without physics simulation or reward engineering. The answer is definitively yes. But more importantly, it&amp;rsquo;s shown a path forward for creating AI systems that capture the subtlety and beauty of human movement.&lt;/p&gt;

&lt;p&gt;Virtual characters that move like us aren&amp;rsquo;t just technically impressive - they&amp;rsquo;re emotionally resonant. They make technology feel more human. In a world increasingly mediated by screens and virtual interactions, that&amp;rsquo;s not just an academic achievement. It&amp;rsquo;s a step toward technology that truly understands and reflects our humanity.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Chapter 4: Supplementary Material</title>
      <link>https://vihanga.github.io/thesis/supplementary/c4/</link>
      <pubDate>Wed, 22 Jan 2025 00:00:00 +0000</pubDate>
      
      <guid>https://vihanga.github.io/thesis/supplementary/c4/</guid>
      <description>

&lt;h1 id=&#34;chapter-4-model-based-character-animation&#34;&gt;Chapter 4: Model-Based Character Animation&lt;/h1&gt;

&lt;p&gt;This page contains supplementary material referenced in Chapter 4 of the thesis, demonstrating the model-based reinforcement learning agents trained to portray gazing and pointing behaviors.&lt;/p&gt;

&lt;h2 id=&#34;video-demonstrations&#34;&gt;Video Demonstrations&lt;/h2&gt;

&lt;h3 id=&#34;model-based-animation-agents-amsta-2021&#34;&gt;Model-Based Animation Agents (AMSTA 2021)&lt;/h3&gt;

&lt;p&gt;The following video demonstrates the agents trained using learned dynamics models and online planning for target-driven behaviors:&lt;/p&gt;


&lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
  &lt;iframe src=&#34;//www.youtube.com/embed/XBNY2tPK7JE&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; allowfullscreen title=&#34;YouTube Video&#34;&gt;&lt;/iframe&gt;
&lt;/div&gt;


&lt;p&gt;This video shows:
- Gazing behavior with dynamic target tracking
- Pointing behavior with both left and right arms
- Combined gaze and point behaviors
- Real-time responsiveness to changing targets
- Comparison with inverse kinematics baseline&lt;/p&gt;

&lt;h3 id=&#34;key-results-demonstrated&#34;&gt;Key Results Demonstrated&lt;/h3&gt;

&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Computational Efficiency&lt;/strong&gt;: 5ms per frame (40x faster than IK baseline)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Dynamic Flexibility&lt;/strong&gt;: Agents adapt to moving targets without computational overhead&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Multi-behavior Support&lt;/strong&gt;: Single agent architecture supports gaze, point, and combined behaviors&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Beta Distribution Planning&lt;/strong&gt;: Novel approach to smooth animation generation&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&#34;navigation&#34;&gt;Navigation&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;../c5a/&#34;&gt;Chapter 5 Part A: Model-free RL →&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://vihanga.github.io/thesis/&#34; target=&#34;_blank&#34;&gt;← Back to thesis main page&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://vihanga.github.io/thesis/chapters/model-based-animation/&#34; target=&#34;_blank&#34;&gt;Chapter 4 in main thesis&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Chapter 5 Supplementary Material - Part A: RLAnimate Output Sequences</title>
      <link>https://vihanga.github.io/thesis/supplementary/c5a/</link>
      <pubDate>Wed, 22 Jan 2025 00:00:00 +0000</pubDate>
      
      <guid>https://vihanga.github.io/thesis/supplementary/c5a/</guid>
      <description>

&lt;h1 id=&#34;chapter-5-part-a-rlanimate-output-sequences&#34;&gt;Chapter 5 Part A: RLAnimate Output Sequences&lt;/h1&gt;

&lt;p&gt;This page contains supplementary material for Chapter 5, demonstrating the RLAnimate A1 agents trained to portray waving and pointing behaviors.&lt;/p&gt;

&lt;h2 id=&#34;video-demonstrations&#34;&gt;Video Demonstrations&lt;/h2&gt;

&lt;h3 id=&#34;rlanimate-output-sequences-and-comparison-to-control-agents-ala-2021&#34;&gt;RLAnimate Output Sequences and Comparison to Control Agents (ALA 2021)&lt;/h3&gt;

&lt;p&gt;The following video shows RLAnimate agents performing waving and pointing behaviors, with comparisons to baseline control agents:&lt;/p&gt;


&lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
  &lt;iframe src=&#34;//www.youtube.com/embed/XyxD86Jz4Z0&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; allowfullscreen title=&#34;YouTube Video&#34;&gt;&lt;/iframe&gt;
&lt;/div&gt;


&lt;p&gt;This video demonstrates:
- A1 agent performing waving behaviors with varying exaggeration levels
- A1 agent performing pointing behaviors to different targets
- Comparison with single dynamics baseline
- Comparison with supervised learning baseline
- Natural, human-like motion generation&lt;/p&gt;

&lt;h3 id=&#34;key-results-shown&#34;&gt;Key Results Shown&lt;/h3&gt;

&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Multi-behavior Support&lt;/strong&gt;: Single agent portraying both waving and pointing&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Dynamic Control&lt;/strong&gt;: Real-time adaptation to behavior parameters&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Human-like Quality&lt;/strong&gt;: Smooth, natural movements without artifacts&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Superior Performance&lt;/strong&gt;: Outperforms baselines on test set (87.23 vs 70.94)&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&#34;navigation&#34;&gt;Navigation&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;../c5b/&#34;&gt;Chapter 5 Part B: Behavior Flexibility →&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://vihanga.github.io/thesis/&#34; target=&#34;_blank&#34;&gt;← Back to thesis main page&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://vihanga.github.io/thesis/chapters/data-driven-rl/&#34; target=&#34;_blank&#34;&gt;Chapter 5 in main thesis&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Chapter 5 Supplementary Material - Part B: Behavior Flexibility</title>
      <link>https://vihanga.github.io/thesis/supplementary/c5b/</link>
      <pubDate>Wed, 22 Jan 2025 00:00:00 +0000</pubDate>
      
      <guid>https://vihanga.github.io/thesis/supplementary/c5b/</guid>
      <description>

&lt;h1 id=&#34;chapter-5-part-b-rlanimate-behavior-portrayal-flexibility&#34;&gt;Chapter 5 Part B: RLAnimate Behavior Portrayal Flexibility&lt;/h1&gt;

&lt;p&gt;This page contains supplementary material for Chapter 5, demonstrating the flexibility and dynamic control capabilities of RLAnimate agents.&lt;/p&gt;

&lt;h2 id=&#34;video-demonstrations&#34;&gt;Video Demonstrations&lt;/h2&gt;

&lt;h3 id=&#34;rlanimate-behavior-portrayal-flexibility-ala-2021&#34;&gt;RLAnimate Behavior Portrayal Flexibility (ALA 2021)&lt;/h3&gt;

&lt;p&gt;The following video demonstrates the dynamic flexibility of RLAnimate agents in portraying varied behaviors:&lt;/p&gt;


&lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
  &lt;iframe src=&#34;//www.youtube.com/embed/cdIM7vMi1XM&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; allowfullscreen title=&#34;YouTube Video&#34;&gt;&lt;/iframe&gt;
&lt;/div&gt;


&lt;p&gt;This video shows:
- Waving with continuously variable exaggeration levels
- Pointing to dynamically changing targets
- Smooth transitions between behaviors
- Real-time responsiveness to parameter changes
- Natural variation in movement execution&lt;/p&gt;

&lt;h3 id=&#34;key-capabilities-demonstrated&#34;&gt;Key Capabilities Demonstrated&lt;/h3&gt;

&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Continuous Control&lt;/strong&gt;: Smooth interpolation between behavior parameters&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Dynamic Adaptation&lt;/strong&gt;: Real-time response to changing objectives&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Natural Variation&lt;/strong&gt;: Human-like variability in repeated behaviors&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Behavior Transitions&lt;/strong&gt;: Seamless switching between waving and pointing&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&#34;navigation&#34;&gt;Navigation&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;../c5a/&#34;&gt;← Chapter 5 Part A: Output Sequences&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;../c6/&#34;&gt;Chapter 6 Supplementary Material →&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://vihanga.github.io/thesis/&#34; target=&#34;_blank&#34;&gt;← Back to thesis main page&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://vihanga.github.io/thesis/chapters/data-driven-rl/&#34; target=&#34;_blank&#34;&gt;Chapter 5 in main thesis&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Chapter 6 Supplementary Material: Quaternions and Finger Animation</title>
      <link>https://vihanga.github.io/thesis/supplementary/c6/</link>
      <pubDate>Wed, 22 Jan 2025 00:00:00 +0000</pubDate>
      
      <guid>https://vihanga.github.io/thesis/supplementary/c6/</guid>
      <description>

&lt;h1 id=&#34;chapter-6-latent-dynamic-augmented-animation-output&#34;&gt;Chapter 6: Latent Dynamic-augmented Animation Output&lt;/h1&gt;

&lt;p&gt;This page contains supplementary material referenced in Chapter 6 of the thesis, demonstrating the A3 architecture with quaternion rotations and animation dynamics for finger animation.&lt;/p&gt;

&lt;h2 id=&#34;a3-architecture-results&#34;&gt;A3 Architecture Results&lt;/h2&gt;

&lt;p&gt;The A3 architecture introduced in Chapter 6 addresses the challenge of incorporating finger animation through:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Quaternion Representations&lt;/strong&gt;: Eliminating gimbal lock and improving computational efficiency&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Animation Dynamics Model&lt;/strong&gt;: Dedicated latent dynamics for animation generation&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Hierarchical Learning&lt;/strong&gt;: Better capture of joint dependencies&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&#34;performance-improvements&#34;&gt;Performance Improvements&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;A2 + Quaternion baseline&lt;/strong&gt;: 81.23 similarity score&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;A3 Standard&lt;/strong&gt;: 86.57 similarity score (5.34 point improvement)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;A3 with all latents&lt;/strong&gt;: 86.19 similarity score&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;All velocity errors remain under the 0.4 perceptual threshold, confirming that the A3 architecture successfully maintains motion quality while handling the additional complexity of finger animation. As noted in the thesis (Section 6.3), visual analysis confirms that the A3 architecture maintains animation quality despite the increased complexity of animating 30+ finger joints.&lt;/p&gt;

&lt;h2 id=&#34;technical-contributions&#34;&gt;Technical Contributions&lt;/h2&gt;

&lt;h3 id=&#34;quaternion-activation-function&#34;&gt;Quaternion Activation Function&lt;/h3&gt;

&lt;p&gt;The novel quaternion activation function ensures all neural network outputs are valid unit quaternions:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;f(w,x,y,z) = [w,x,y,z] / sqrt(w² + x² + y² + z²)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This eliminates singularities (except at origin) and provides smooth gradients for training.&lt;/p&gt;

&lt;h3 id=&#34;animation-dynamics-model&#34;&gt;Animation Dynamics Model&lt;/h3&gt;

&lt;p&gt;The dedicated animation dynamics model learns hierarchical relationships between joints through:
- Deterministic state: h&lt;em&gt;t^a = f(h&lt;/em&gt;{t-1}^a, s_{t-1}, h_t^p, p_t, h_t^b, b_t)
- Stochastic component for natural variation
- Temporal consistency through recurrent processing&lt;/p&gt;

&lt;h3 id=&#34;a3-agent-capabilities&#34;&gt;A3 Agent Capabilities&lt;/h3&gt;

&lt;p&gt;The A3 agents demonstrate:
- Successful incorporation of finger movements in waving behaviors
- Natural finger positioning during pointing tasks
- Smooth transitions without artifacts
- Maintained performance under the 0.4 velocity error threshold&lt;/p&gt;

&lt;h2 id=&#34;navigation&#34;&gt;Navigation&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;../c5b/&#34;&gt;← Chapter 5 Part B: Behavior Flexibility&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;../c7/&#34;&gt;Chapter 7 Supplementary Material →&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://vihanga.github.io/thesis/&#34; target=&#34;_blank&#34;&gt;← Back to thesis main page&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://vihanga.github.io/thesis/chapters/latent-dynamics/&#34; target=&#34;_blank&#34;&gt;Chapter 6 in main thesis&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Chapter 7 Supplementary Material: Beat Gestures</title>
      <link>https://vihanga.github.io/thesis/supplementary/c7/</link>
      <pubDate>Wed, 22 Jan 2025 00:00:00 +0000</pubDate>
      
      <guid>https://vihanga.github.io/thesis/supplementary/c7/</guid>
      <description>

&lt;h1 id=&#34;chapter-7-portraying-conversational-gestures-via-realism-regularisation&#34;&gt;Chapter 7: Portraying Conversational Gestures via Realism Regularisation&lt;/h1&gt;

&lt;p&gt;This page contains supplementary material referenced in Chapter 7, including the video stimuli used in the perceptual evaluation study comparing RLAnimate A4, Gesticulator, and motion capture.&lt;/p&gt;

&lt;h2 id=&#34;perceptual-study-video-stimuli&#34;&gt;Perceptual Study Video Stimuli&lt;/h2&gt;

&lt;p&gt;The following videos were used in the perceptual evaluation with 28 participants. Each stimulus set was rendered with three methods: RLAnimate A4, Gesticulator, and Motion Capture (ground truth).&lt;/p&gt;

&lt;h3 id=&#34;stimulus-set-a&#34;&gt;Stimulus Set A&lt;/h3&gt;

&lt;div style=&#34;display: grid; grid-template-columns: repeat(3, 1fr); gap: 10px; margin-bottom: 30px;&#34;&gt;
  &lt;div&gt;
    &lt;h4 style=&#34;text-align: center;&#34;&gt;RLAnimate A4&lt;/h4&gt;
    &lt;video controls width=&#34;100%&#34;&gt;
      &lt;source src=&#34;../videos/chapter7/02_rlanimate_malcolm.mp4&#34; type=&#34;video/mp4&#34;&gt;
    &lt;/video&gt;
  &lt;/div&gt;
  &lt;div&gt;
    &lt;h4 style=&#34;text-align: center;&#34;&gt;Gesticulator&lt;/h4&gt;
    &lt;video controls width=&#34;100%&#34;&gt;
      &lt;source src=&#34;../videos/chapter7/02_gesticulator_malcolm.mp4&#34; type=&#34;video/mp4&#34;&gt;
    &lt;/video&gt;
  &lt;/div&gt;
  &lt;div&gt;
    &lt;h4 style=&#34;text-align: center;&#34;&gt;Motion Capture&lt;/h4&gt;
    &lt;video controls width=&#34;100%&#34;&gt;
      &lt;source src=&#34;../videos/chapter7/02_mocap_malcolm.mp4&#34; type=&#34;video/mp4&#34;&gt;
    &lt;/video&gt;
  &lt;/div&gt;
&lt;/div&gt;

&lt;h3 id=&#34;stimulus-set-b&#34;&gt;Stimulus Set B&lt;/h3&gt;

&lt;div style=&#34;display: grid; grid-template-columns: repeat(3, 1fr); gap: 10px; margin-bottom: 30px;&#34;&gt;
  &lt;div&gt;
    &lt;h4 style=&#34;text-align: center;&#34;&gt;RLAnimate A4&lt;/h4&gt;
    &lt;video controls width=&#34;100%&#34;&gt;
      &lt;source src=&#34;../videos/chapter7/03_rlanimate_malcolm.mp4&#34; type=&#34;video/mp4&#34;&gt;
    &lt;/video&gt;
  &lt;/div&gt;
  &lt;div&gt;
    &lt;h4 style=&#34;text-align: center;&#34;&gt;Gesticulator&lt;/h4&gt;
    &lt;video controls width=&#34;100%&#34;&gt;
      &lt;source src=&#34;../videos/chapter7/03_gesticulator_malcolm.mp4&#34; type=&#34;video/mp4&#34;&gt;
    &lt;/video&gt;
  &lt;/div&gt;
  &lt;div&gt;
    &lt;h4 style=&#34;text-align: center;&#34;&gt;Motion Capture&lt;/h4&gt;
    &lt;video controls width=&#34;100%&#34;&gt;
      &lt;source src=&#34;../videos/chapter7/03_mocap_malcolm.mp4&#34; type=&#34;video/mp4&#34;&gt;
    &lt;/video&gt;
  &lt;/div&gt;
&lt;/div&gt;

&lt;h3 id=&#34;stimulus-set-c&#34;&gt;Stimulus Set C&lt;/h3&gt;

&lt;div style=&#34;display: grid; grid-template-columns: repeat(3, 1fr); gap: 10px; margin-bottom: 30px;&#34;&gt;
  &lt;div&gt;
    &lt;h4 style=&#34;text-align: center;&#34;&gt;RLAnimate A4&lt;/h4&gt;
    &lt;video controls width=&#34;100%&#34;&gt;
      &lt;source src=&#34;../videos/chapter7/05_rlanimate_malcolm.mp4&#34; type=&#34;video/mp4&#34;&gt;
    &lt;/video&gt;
  &lt;/div&gt;
  &lt;div&gt;
    &lt;h4 style=&#34;text-align: center;&#34;&gt;Gesticulator&lt;/h4&gt;
    &lt;video controls width=&#34;100%&#34;&gt;
      &lt;source src=&#34;../videos/chapter7/05_gesticulator_malcolm.mp4&#34; type=&#34;video/mp4&#34;&gt;
    &lt;/video&gt;
  &lt;/div&gt;
  &lt;div&gt;
    &lt;h4 style=&#34;text-align: center;&#34;&gt;Motion Capture&lt;/h4&gt;
    &lt;video controls width=&#34;100%&#34;&gt;
      &lt;source src=&#34;../videos/chapter7/05_mocap_malcolm.mp4&#34; type=&#34;video/mp4&#34;&gt;
    &lt;/video&gt;
  &lt;/div&gt;
&lt;/div&gt;

&lt;h3 id=&#34;stimulus-set-d&#34;&gt;Stimulus Set D&lt;/h3&gt;

&lt;div style=&#34;display: grid; grid-template-columns: repeat(3, 1fr); gap: 10px; margin-bottom: 30px;&#34;&gt;
  &lt;div&gt;
    &lt;h4 style=&#34;text-align: center;&#34;&gt;RLAnimate A4&lt;/h4&gt;
    &lt;video controls width=&#34;100%&#34;&gt;
      &lt;source src=&#34;../videos/chapter7/06_rlanimate_malcolm.mp4&#34; type=&#34;video/mp4&#34;&gt;
    &lt;/video&gt;
  &lt;/div&gt;
  &lt;div&gt;
    &lt;h4 style=&#34;text-align: center;&#34;&gt;Gesticulator&lt;/h4&gt;
    &lt;video controls width=&#34;100%&#34;&gt;
      &lt;source src=&#34;../videos/chapter7/06_gesticulator_malcolm.mp4&#34; type=&#34;video/mp4&#34;&gt;
    &lt;/video&gt;
  &lt;/div&gt;
  &lt;div&gt;
    &lt;h4 style=&#34;text-align: center;&#34;&gt;Motion Capture&lt;/h4&gt;
    &lt;video controls width=&#34;100%&#34;&gt;
      &lt;source src=&#34;../videos/chapter7/06_mocap_malcolm.mp4&#34; type=&#34;video/mp4&#34;&gt;
    &lt;/video&gt;
  &lt;/div&gt;
&lt;/div&gt;

&lt;h3 id=&#34;stimulus-set-e&#34;&gt;Stimulus Set E&lt;/h3&gt;

&lt;div style=&#34;display: grid; grid-template-columns: repeat(3, 1fr); gap: 10px; margin-bottom: 30px;&#34;&gt;
  &lt;div&gt;
    &lt;h4 style=&#34;text-align: center;&#34;&gt;RLAnimate A4&lt;/h4&gt;
    &lt;video controls width=&#34;100%&#34;&gt;
      &lt;source src=&#34;../videos/chapter7/07_rlanimate_malcolm.mp4&#34; type=&#34;video/mp4&#34;&gt;
    &lt;/video&gt;
  &lt;/div&gt;
  &lt;div&gt;
    &lt;h4 style=&#34;text-align: center;&#34;&gt;Gesticulator&lt;/h4&gt;
    &lt;video controls width=&#34;100%&#34;&gt;
      &lt;source src=&#34;../videos/chapter7/07_gesticulator_malcolm.mp4&#34; type=&#34;video/mp4&#34;&gt;
    &lt;/video&gt;
  &lt;/div&gt;
  &lt;div&gt;
    &lt;h4 style=&#34;text-align: center;&#34;&gt;Motion Capture&lt;/h4&gt;
    &lt;video controls width=&#34;100%&#34;&gt;
      &lt;source src=&#34;../videos/chapter7/07_mocap_malcolm.mp4&#34; type=&#34;video/mp4&#34;&gt;
    &lt;/video&gt;
  &lt;/div&gt;
&lt;/div&gt;

&lt;h3 id=&#34;stimulus-set-f&#34;&gt;Stimulus Set F&lt;/h3&gt;

&lt;div style=&#34;display: grid; grid-template-columns: repeat(3, 1fr); gap: 10px; margin-bottom: 30px;&#34;&gt;
  &lt;div&gt;
    &lt;h4 style=&#34;text-align: center;&#34;&gt;RLAnimate A4&lt;/h4&gt;
    &lt;video controls width=&#34;100%&#34;&gt;
      &lt;source src=&#34;../videos/chapter7/08_rlanimate_malcolm.mp4&#34; type=&#34;video/mp4&#34;&gt;
    &lt;/video&gt;
  &lt;/div&gt;
  &lt;div&gt;
    &lt;h4 style=&#34;text-align: center;&#34;&gt;Gesticulator&lt;/h4&gt;
    &lt;video controls width=&#34;100%&#34;&gt;
      &lt;source src=&#34;../videos/chapter7/08_gesticulator_malcolm.mp4&#34; type=&#34;video/mp4&#34;&gt;
    &lt;/video&gt;
  &lt;/div&gt;
  &lt;div&gt;
    &lt;h4 style=&#34;text-align: center;&#34;&gt;Motion Capture&lt;/h4&gt;
    &lt;video controls width=&#34;100%&#34;&gt;
      &lt;source src=&#34;../videos/chapter7/08_mocap_malcolm.mp4&#34; type=&#34;video/mp4&#34;&gt;
    &lt;/video&gt;
  &lt;/div&gt;
&lt;/div&gt;

&lt;h3 id=&#34;stimulus-set-g&#34;&gt;Stimulus Set G&lt;/h3&gt;

&lt;div style=&#34;display: grid; grid-template-columns: repeat(3, 1fr); gap: 10px; margin-bottom: 30px;&#34;&gt;
  &lt;div&gt;
    &lt;h4 style=&#34;text-align: center;&#34;&gt;RLAnimate A4&lt;/h4&gt;
    &lt;video controls width=&#34;100%&#34;&gt;
      &lt;source src=&#34;../videos/chapter7/09_rlanimate_malcolm.mp4&#34; type=&#34;video/mp4&#34;&gt;
    &lt;/video&gt;
  &lt;/div&gt;
  &lt;div&gt;
    &lt;h4 style=&#34;text-align: center;&#34;&gt;Gesticulator&lt;/h4&gt;
    &lt;video controls width=&#34;100%&#34;&gt;
      &lt;source src=&#34;../videos/chapter7/09_gesticulator_malcolm.mp4&#34; type=&#34;video/mp4&#34;&gt;
    &lt;/video&gt;
  &lt;/div&gt;
  &lt;div&gt;
    &lt;h4 style=&#34;text-align: center;&#34;&gt;Motion Capture&lt;/h4&gt;
    &lt;video controls width=&#34;100%&#34;&gt;
      &lt;source src=&#34;../videos/chapter7/09_mocap_malcolm.mp4&#34; type=&#34;video/mp4&#34;&gt;
    &lt;/video&gt;
  &lt;/div&gt;
&lt;/div&gt;

&lt;h3 id=&#34;stimulus-set-h&#34;&gt;Stimulus Set H&lt;/h3&gt;

&lt;div style=&#34;display: grid; grid-template-columns: repeat(3, 1fr); gap: 10px; margin-bottom: 30px;&#34;&gt;
  &lt;div&gt;
    &lt;h4 style=&#34;text-align: center;&#34;&gt;RLAnimate A4&lt;/h4&gt;
    &lt;video controls width=&#34;100%&#34;&gt;
      &lt;source src=&#34;../videos/chapter7/10_rlanimate_malcolm.mp4&#34; type=&#34;video/mp4&#34;&gt;
    &lt;/video&gt;
  &lt;/div&gt;
  &lt;div&gt;
    &lt;h4 style=&#34;text-align: center;&#34;&gt;Gesticulator&lt;/h4&gt;
    &lt;video controls width=&#34;100%&#34;&gt;
      &lt;source src=&#34;../videos/chapter7/10_gesticulator_malcolm.mp4&#34; type=&#34;video/mp4&#34;&gt;
    &lt;/video&gt;
  &lt;/div&gt;
  &lt;div&gt;
    &lt;h4 style=&#34;text-align: center;&#34;&gt;Motion Capture&lt;/h4&gt;
    &lt;video controls width=&#34;100%&#34;&gt;
      &lt;source src=&#34;../videos/chapter7/10_mocap_malcolm.mp4&#34; type=&#34;video/mp4&#34;&gt;
    &lt;/video&gt;
  &lt;/div&gt;
&lt;/div&gt;

&lt;h2 id=&#34;perceptual-study-results&#34;&gt;Perceptual Study Results&lt;/h2&gt;

&lt;h3 id=&#34;statistical-results&#34;&gt;Statistical Results&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;RLAnimate A4 vs Gesticulator&lt;/strong&gt; (one-sided t-tests, α = 0.05):
- Human-likeness: 4.33 vs 3.19 (p &amp;lt; 0.001) ✓
- Speech reflection: 4.12 vs 3.42 (p &amp;lt; 0.001) ✓
- Synchronization: 4.48 vs 3.60 (p &amp;lt; 0.001) ✓&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;RLAnimate A4 vs Motion Capture&lt;/strong&gt; (two-sided t-tests, α = 0.05):
- Human-likeness: 4.33 vs 4.65 (p = 0.083, NS)
- Speech reflection: 4.12 vs 4.38 (p = 0.174, NS)
- Synchronization: 4.48 vs 4.67 (p = 0.268, NS)&lt;/p&gt;

&lt;h3 id=&#34;key-finding&#34;&gt;Key Finding&lt;/h3&gt;

&lt;p&gt;RLAnimate A4 achieves &lt;strong&gt;statistical parity with human motion capture&lt;/strong&gt; while significantly outperforming the state-of-the-art Gesticulator system across all evaluation criteria.&lt;/p&gt;

&lt;h2 id=&#34;navigation&#34;&gt;Navigation&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;../c6/&#34;&gt;← Chapter 6 Supplementary Material&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://vihanga.github.io/thesis/&#34; target=&#34;_blank&#34;&gt;← Back to thesis main page&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://vihanga.github.io/thesis/chapters/conversational-gestures/&#34; target=&#34;_blank&#34;&gt;Chapter 7 in main thesis&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>MIG&#39;18 Paper: Virtual Characters in Serious Games</title>
      <link>https://vihanga.github.io/thesis/publications/MIG18/</link>
      <pubDate>Wed, 22 Jan 2025 00:00:00 +0000</pubDate>
      
      <guid>https://vihanga.github.io/thesis/publications/MIG18/</guid>
      <description>

&lt;h1 id=&#34;examining-the-effects-of-a-virtual-character-on-learning-and-engagement-in-serious-games&#34;&gt;Examining the effects of a virtual character on learning and engagement in serious games&lt;/h1&gt;

&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Vihanga Gamage, Cathy Ennis&lt;br /&gt;
&lt;strong&gt;Conference:&lt;/strong&gt; MIG &amp;lsquo;18: Motion, Interaction and Games, November 8–10, 2018, Limassol, Cyprus&lt;br /&gt;
&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href=&#34;https://doi.org/10.1145/3274247.3274499&#34; target=&#34;_blank&#34;&gt;10.&lt;sup&gt;1145&lt;/sup&gt;&amp;frasl;&lt;sub&gt;3274247&lt;/sub&gt;.3274499&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&#34;abstract&#34;&gt;Abstract&lt;/h2&gt;

&lt;p&gt;Virtual characters have been employed for many purposes including interacting with players of serious games, with a purpose to increase engagement. These characters are often embodied conversational agents playing diverse roles, such as demonstrators, guides, teachers or interviewers. Recently, much research has been conducted into properties that affect the realism and plausibility of virtual characters, but it is less clear whether the inclusion of interactive agents in serious applications can enhance a user&amp;rsquo;s engagement with the application, or indeed increase efficacy. In a first step towards answering these questions, we conducted a study where a Virtual Learning Environment was used to examine the effect of employing a virtual character to deliver a lesson. In order to investigate whether increased familiarity between the player and the character would help achieve learning outcomes, we allowed participants to customize the physical appearance of the character. We used direct and indirect measures to assess engagement and learning; we measured knowledge retention to ascertain learning via a test at the end of the lesson, and also measured participants&amp;rsquo; perceived engagement with the lesson. Our findings show that a virtual character can be an effective learning aid, causing heightened engagement and retention of knowledge. However, allowing participants to customize character appearance resulted in inhibited engagement, which was contrary to what we expected.&lt;/p&gt;

&lt;h2 id=&#34;key-findings&#34;&gt;Key Findings&lt;/h2&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Enhanced Engagement&lt;/strong&gt;: Users showed significantly higher engagement when lessons were delivered through a virtual character compared to non-character controls.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;strong&gt;Improved Knowledge Retention&lt;/strong&gt;: Virtual character delivery resulted in better knowledge retention compared to traditional presentation methods.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;strong&gt;Attention Focus&lt;/strong&gt;: User attention was predominantly directed at the virtual character, with limited attention to other environmental elements.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;strong&gt;Customization Paradox&lt;/strong&gt;: Contrary to expectations, character appearance personalization led to significantly lower engagement compared to default appearances.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&#34;relevance-to-thesis&#34;&gt;Relevance to Thesis&lt;/h2&gt;

&lt;p&gt;This work laid important groundwork for understanding how virtual characters affect user engagement and learning. While this paper focused on static virtual characters in educational contexts, it motivated the later development of RLAnimate for creating more dynamic, responsive virtual characters that can adapt their behaviors in real-time - addressing some of the limitations identified in this study.&lt;/p&gt;

&lt;h2 id=&#34;links&#34;&gt;Links&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://dl.acm.org/doi/10.1145/3274247.3274499&#34; target=&#34;_blank&#34;&gt;Paper PDF (ACM Digital Library)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://vihanga.github.io/post/research/MIG18PaperStudy/&#34; target=&#34;_blank&#34;&gt;Conference presentation thoughts and feedback&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://vihanga.github.io/post/blog/MIG18blog/&#34; target=&#34;_blank&#34;&gt;MIG&amp;rsquo;18 conference blog post&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://vihanga.github.io/thesis/&#34; target=&#34;_blank&#34;&gt;Back to thesis&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Why AI is an opportunity rather than a danger</title>
      <link>https://vihanga.github.io/post/blog/AI-Oppurtunity-not-danger/</link>
      <pubDate>Sat, 17 Aug 2019 17:02:59 +0000</pubDate>
      
      <guid>https://vihanga.github.io/post/blog/AI-Oppurtunity-not-danger/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;https://www.rte.ie/brainstorm/2019/0801/1066421-why-ai-is-an-opportunity-rather-than-a-danger/&#34; target=&#34;_blank&#34;&gt;Originally published on RTÉ Brainstorm&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;The solar system is full of debris and rocks floating around. A meteor shower or a shooting star is debris burning up upon entry to the atmosphere. On occasion, pieces of rock make it all the way through. A much larger rock that could make its way through is an asteroid. About 66 million years ago, the impact from an asteroid brought about the Cretaceous extinction. A repetition of this event is theoretically possible - and the same could be said about all-powerful Artificial Intelligence Overlords marginalising the human race.&lt;/p&gt;

&lt;p&gt;Since its inception in the mid 20th century, the field of artificial intelligence has had an interesting ride. Much like the financial markets or Hollywood, there’s been breakthroughs and failures and booms and busts. Driven by advances in deep learning, a great deal of success has been achieved this decade, ushering in the newest AI golden age, along with a frenzy of interest and investment.&lt;/p&gt;

&lt;p&gt;The current popular views of AI are being shaped primarily by several vocal figures in technology. Microsoft founder Bill Gates has said that AI carries the potential to change society deeply and is both exciting and dangerous. IBM CEO Ginni Rometty has stated that she believes advances in AI will create different jobs rather than no jobs. Facebook founder Mark Zuckerberg has expressed the view that AI will deliver various improvements to human quality of life, while Tesla founder Elon Musk has called AI &amp;ldquo;a fundamental risk to the existence of human civilisation&amp;rdquo;.&lt;/p&gt;

&lt;p&gt;The future dangers AI could pose to humanity is a frequently occurring subject in conversation, be it to break the ice on a first date, to fill the time before a meeting starts or while waiting for a pint at the pub on a Friday evening. And arguably, Musk&amp;rsquo;s statement carries the most bite. It certainly is a very catchy line that makes for a more topical conversation starter than asteroids or the Cretaceous extinction.&lt;/p&gt;

&lt;p&gt;As a result, these hypothetical yet sensationalist doomsday opinions are at a point where they dominate the conversation about AI, so much so that the bigger picture on the subject may at the point of being overshadowed and given very little consideration. And that could pose an even greater threat, especially in an age where hype and rhetoric without regard for fact has had a great effect on the fate of the world.&lt;/p&gt;

&lt;p&gt;Advancements in AI could lead to a superintelligence that could pose a fundamental risk to human existence - and so could climate change or a 5 km-wide space rock colliding with the Earth. But there are many points of difference between these three risks. AI is a long way away advancing to the point of superintelligence and it’s not a certainty that superintelligence would lead to the end of humanity. Climate change is a very real and present danger, with the time to take action to prevent irreversible consequences continuing to decrease. And then there are the chances of an asteroid striking the earth.&lt;/p&gt;

&lt;p&gt;Of course, there is no good that can come from an asteroid strike or global warming, but AI can literally be one of the best things to happen to humans. This is not said enough - and it can’t be said enough. AI can and is being used to as a tool in many incredible ways. For example, it is being used to help improve prevention of cancer, provide effective treatment, and could one day even be part of the solution.&lt;/p&gt;

&lt;p&gt;AI is also being leveraged as a promising way to combat climate change. AI-powered simulations and analysis helps make for sustainable urban planning and ocean preservation. It is possible to make more accurate predictions regarding weather events with the help of AI leading to reduced damage to life and property. It can help power grids to be more energy efficient by predicting peak over of usage allowing for better preparation and enhances the benefits of clean energy methods by deploying in ways such as to incorporate weather and other relevant data to make wind turbines more efficient.&lt;/p&gt;

&lt;p&gt;And yet arguments on how AI could hypothetically end humanity as we know it at some point in the future, seem to drown out how AI is already helping humanity’s cause. Such contributions pale in comparison to the very real possibility that AI could be the key to finding the answers for some of humanity’s biggest problems.&lt;/p&gt;

&lt;p&gt;At a 2018 hearing at the US House of Representatives Committee on Science, Space and Technology, Stanford Professor Fei-Fei Li said that AI is bound to alter the human experience, and not necessarily for the better. After acknowledging this, Li pointed out that ensuring AI will transform the world for the better is very much in our hands. &amp;ldquo;There&amp;rsquo;s nothing artificial about AI&amp;rdquo;, she said.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;&amp;ldquo;It’s inspired by people; it’s created by people, and—most importantly—it impacts people. It is a powerful tool we are only just beginning to understand, and that is a profound responsibility.&amp;rdquo;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Concern that AI could pose a threat is very much reasonable. But this possibility is just that - a possibility. Let’s not forget AI is a fundamental opportunity to further the cause of the human civilisation, and that it is in our hands to ensure that it impacts us positively, and be optimistic about what it can mean for us. The world can only be the better place for it.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
